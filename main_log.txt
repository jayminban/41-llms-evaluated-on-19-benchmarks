ðŸ”¥ Starting benchmark for meta-llama_Llama-3.2-1B-Instruct
Passed argument batch_size = auto:1. Detecting largest batch size
Determined largest batch size: 2
Passed argument batch_size = auto. Detecting largest batch size
Determined Largest batch size: 1
hf (pretrained=/home/jaymin/Documents/llm/llm_models/meta-llama_Llama-3.2-1B-Instruct), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto (2)
|                          Tasks                           |Version|     Filter      |n-shot|  Metric   |   | Value |   |Stderr|
|----------------------------------------------------------|------:|-----------------|-----:|-----------|---|------:|---|-----:|
|anli_r1                                                   |      1|none             |     0|acc        |â†‘  | 0.3380|Â±  |0.0150|
|anli_r2                                                   |      1|none             |     0|acc        |â†‘  | 0.3340|Â±  |0.0149|
|anli_r3                                                   |      1|none             |     0|acc        |â†‘  | 0.3725|Â±  |0.0140|
|arc_challenge                                             |      1|none             |     0|acc        |â†‘  | 0.3567|Â±  |0.0140|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3805|Â±  |0.0142|
|bbh                                                       |      3|get-answer       |      |exact_match|â†‘  | 0.3781|Â±  |0.0055|
| - bbh_cot_fewshot_boolean_expressions                    |      4|get-answer       |     3|exact_match|â†‘  | 0.7600|Â±  |0.0271|
| - bbh_cot_fewshot_causal_judgement                       |      4|get-answer       |     3|exact_match|â†‘  | 0.4225|Â±  |0.0362|
| - bbh_cot_fewshot_date_understanding                     |      4|get-answer       |     3|exact_match|â†‘  | 0.5320|Â±  |0.0316|
| - bbh_cot_fewshot_disambiguation_qa                      |      4|get-answer       |     3|exact_match|â†‘  | 0.4760|Â±  |0.0316|
| - bbh_cot_fewshot_dyck_languages                         |      4|get-answer       |     3|exact_match|â†‘  | 0.0280|Â±  |0.0105|
| - bbh_cot_fewshot_formal_fallacies                       |      4|get-answer       |     3|exact_match|â†‘  | 0.5200|Â±  |0.0317|
| - bbh_cot_fewshot_geometric_shapes                       |      4|get-answer       |     3|exact_match|â†‘  | 0.2640|Â±  |0.0279|
| - bbh_cot_fewshot_hyperbaton                             |      4|get-answer       |     3|exact_match|â†‘  | 0.5760|Â±  |0.0313|
| - bbh_cot_fewshot_logical_deduction_five_objects         |      4|get-answer       |     3|exact_match|â†‘  | 0.2320|Â±  |0.0268|
| - bbh_cot_fewshot_logical_deduction_seven_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.1440|Â±  |0.0222|
| - bbh_cot_fewshot_logical_deduction_three_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.3680|Â±  |0.0306|
| - bbh_cot_fewshot_movie_recommendation                   |      4|get-answer       |     3|exact_match|â†‘  | 0.4720|Â±  |0.0316|
| - bbh_cot_fewshot_multistep_arithmetic_two               |      4|get-answer       |     3|exact_match|â†‘  | 0.2400|Â±  |0.0271|
| - bbh_cot_fewshot_navigate                               |      4|get-answer       |     3|exact_match|â†‘  | 0.6080|Â±  |0.0309|
| - bbh_cot_fewshot_object_counting                        |      4|get-answer       |     3|exact_match|â†‘  | 0.5320|Â±  |0.0316|
| - bbh_cot_fewshot_penguins_in_a_table                    |      4|get-answer       |     3|exact_match|â†‘  | 0.4315|Â±  |0.0411|
| - bbh_cot_fewshot_reasoning_about_colored_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.2720|Â±  |0.0282|
| - bbh_cot_fewshot_ruin_names                             |      4|get-answer       |     3|exact_match|â†‘  | 0.4800|Â±  |0.0317|
| - bbh_cot_fewshot_salient_translation_error_detection    |      4|get-answer       |     3|exact_match|â†‘  | 0.2320|Â±  |0.0268|
| - bbh_cot_fewshot_snarks                                 |      4|get-answer       |     3|exact_match|â†‘  | 0.4607|Â±  |0.0375|
| - bbh_cot_fewshot_sports_understanding                   |      4|get-answer       |     3|exact_match|â†‘  | 0.7320|Â±  |0.0281|
| - bbh_cot_fewshot_temporal_sequences                     |      4|get-answer       |     3|exact_match|â†‘  | 0.2160|Â±  |0.0261|
| - bbh_cot_fewshot_tracking_shuffled_objects_five_objects |      4|get-answer       |     3|exact_match|â†‘  | 0.1840|Â±  |0.0246|
| - bbh_cot_fewshot_tracking_shuffled_objects_seven_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.0960|Â±  |0.0187|
| - bbh_cot_fewshot_tracking_shuffled_objects_three_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.3080|Â±  |0.0293|
| - bbh_cot_fewshot_web_of_lies                            |      4|get-answer       |     3|exact_match|â†‘  | 0.6040|Â±  |0.0310|
| - bbh_cot_fewshot_word_sorting                           |      4|get-answer       |     3|exact_match|â†‘  | 0.0760|Â±  |0.0168|
|boolq                                                     |      2|none             |     0|acc        |â†‘  | 0.6948|Â±  |0.0081|
|drop                                                      |      3|none             |     0|em         |â†‘  | 0.0497|Â±  |0.0022|
|                                                          |       |none             |     0|f1         |â†‘  | 0.1635|Â±  |0.0029|
|gpqa_diamond_cot_n_shot                                   |      2|flexible-extract |     0|exact_match|â†‘  | 0.1111|Â±  |0.0224|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_cot_zeroshot                                 |      1|flexible-extract |     0|exact_match|â†‘  | 0.1010|Â±  |0.0215|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_generative_n_shot                            |      2|flexible-extract |     0|exact_match|â†‘  | 0.2727|Â±  |0.0317|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0051|Â±  |0.0051|
|gpqa_diamond_n_shot                                       |      2|none             |     0|acc        |â†‘  | 0.2576|Â±  |0.0312|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2576|Â±  |0.0312|
|gpqa_diamond_zeroshot                                     |      1|none             |     0|acc        |â†‘  | 0.2121|Â±  |0.0291|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2121|Â±  |0.0291|
|gpqa_extended_cot_n_shot                                  |      2|flexible-extract |     0|exact_match|â†‘  | 0.1007|Â±  |0.0129|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_cot_zeroshot                                |      1|flexible-extract |     0|exact_match|â†‘  | 0.0934|Â±  |0.0125|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_generative_n_shot                           |      2|flexible-extract |     0|exact_match|â†‘  | 0.2198|Â±  |0.0177|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_n_shot                                      |      2|none             |     0|acc        |â†‘  | 0.2656|Â±  |0.0189|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2656|Â±  |0.0189|
|gpqa_extended_zeroshot                                    |      1|none             |     0|acc        |â†‘  | 0.2802|Â±  |0.0192|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2802|Â±  |0.0192|
|gpqa_main_cot_n_shot                                      |      2|flexible-extract |     0|exact_match|â†‘  | 0.0982|Â±  |0.0141|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_cot_zeroshot                                    |      1|flexible-extract |     0|exact_match|â†‘  | 0.0982|Â±  |0.0141|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0022|Â±  |0.0022|
|gpqa_main_generative_n_shot                               |      2|flexible-extract |     0|exact_match|â†‘  | 0.2321|Â±  |0.0200|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_n_shot                                          |      2|none             |     0|acc        |â†‘  | 0.2790|Â±  |0.0212|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2790|Â±  |0.0212|
|gpqa_main_zeroshot                                        |      1|none             |     0|acc        |â†‘  | 0.2746|Â±  |0.0211|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2746|Â±  |0.0211|
|gsm8k                                                     |      3|flexible-extract |     5|exact_match|â†‘  | 0.3374|Â±  |0.0130|
|                                                          |       |strict-match     |     5|exact_match|â†‘  | 0.3374|Â±  |0.0130|
|hellaswag                                                 |      1|none             |     0|acc        |â†‘  | 0.4512|Â±  |0.0050|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.6088|Â±  |0.0049|
|mmlu                                                      |      2|none             |      |acc        |â†‘  | 0.4589|Â±  |0.0041|
| - humanities                                             |      2|none             |      |acc        |â†‘  | 0.4389|Â±  |0.0071|
|  - formal_logic                                          |      1|none             |     0|acc        |â†‘  | 0.3095|Â±  |0.0413|
|  - high_school_european_history                          |      1|none             |     0|acc        |â†‘  | 0.6242|Â±  |0.0378|
|  - high_school_us_history                                |      1|none             |     0|acc        |â†‘  | 0.5784|Â±  |0.0347|
|  - high_school_world_history                             |      1|none             |     0|acc        |â†‘  | 0.6540|Â±  |0.0310|
|  - international_law                                     |      1|none             |     0|acc        |â†‘  | 0.5950|Â±  |0.0448|
|  - jurisprudence                                         |      1|none             |     0|acc        |â†‘  | 0.5185|Â±  |0.0483|
|  - logical_fallacies                                     |      1|none             |     0|acc        |â†‘  | 0.4540|Â±  |0.0391|
|  - moral_disputes                                        |      1|none             |     0|acc        |â†‘  | 0.4595|Â±  |0.0268|
|  - moral_scenarios                                       |      1|none             |     0|acc        |â†‘  | 0.3318|Â±  |0.0157|
|  - philosophy                                            |      1|none             |     0|acc        |â†‘  | 0.5177|Â±  |0.0284|
|  - prehistory                                            |      1|none             |     0|acc        |â†‘  | 0.5278|Â±  |0.0278|
|  - professional_law                                      |      1|none             |     0|acc        |â†‘  | 0.3651|Â±  |0.0123|
|  - world_religions                                       |      1|none             |     0|acc        |â†‘  | 0.5848|Â±  |0.0378|
| - other                                                  |      2|none             |      |acc        |â†‘  | 0.5182|Â±  |0.0088|
|  - business_ethics                                       |      1|none             |     0|acc        |â†‘  | 0.4500|Â±  |0.0500|
|  - clinical_knowledge                                    |      1|none             |     0|acc        |â†‘  | 0.4679|Â±  |0.0307|
|  - college_medicine                                      |      1|none             |     0|acc        |â†‘  | 0.3815|Â±  |0.0370|
|  - global_facts                                          |      1|none             |     0|acc        |â†‘  | 0.3200|Â±  |0.0469|
|  - human_aging                                           |      1|none             |     0|acc        |â†‘  | 0.5381|Â±  |0.0335|
|  - management                                            |      1|none             |     0|acc        |â†‘  | 0.5340|Â±  |0.0494|
|  - marketing                                             |      1|none             |     0|acc        |â†‘  | 0.6795|Â±  |0.0306|
|  - medical_genetics                                      |      1|none             |     0|acc        |â†‘  | 0.4700|Â±  |0.0502|
|  - miscellaneous                                         |      1|none             |     0|acc        |â†‘  | 0.6003|Â±  |0.0175|
|  - nutrition                                             |      1|none             |     0|acc        |â†‘  | 0.5588|Â±  |0.0284|
|  - professional_accounting                               |      1|none             |     0|acc        |â†‘  | 0.3546|Â±  |0.0285|
|  - professional_medicine                                 |      1|none             |     0|acc        |â†‘  | 0.5588|Â±  |0.0302|
|  - virology                                              |      1|none             |     0|acc        |â†‘  | 0.4157|Â±  |0.0384|
| - social sciences                                        |      2|none             |      |acc        |â†‘  | 0.5080|Â±  |0.0088|
|  - econometrics                                          |      1|none             |     0|acc        |â†‘  | 0.2281|Â±  |0.0395|
|  - high_school_geography                                 |      1|none             |     0|acc        |â†‘  | 0.5556|Â±  |0.0354|
|  - high_school_government_and_politics                   |      1|none             |     0|acc        |â†‘  | 0.5181|Â±  |0.0361|
|  - high_school_macroeconomics                            |      1|none             |     0|acc        |â†‘  | 0.4103|Â±  |0.0249|
|  - high_school_microeconomics                            |      1|none             |     0|acc        |â†‘  | 0.4538|Â±  |0.0323|
|  - high_school_psychology                                |      1|none             |     0|acc        |â†‘  | 0.6294|Â±  |0.0207|
|  - human_sexuality                                       |      1|none             |     0|acc        |â†‘  | 0.5344|Â±  |0.0437|
|  - professional_psychology                               |      1|none             |     0|acc        |â†‘  | 0.4265|Â±  |0.0200|
|  - public_relations                                      |      1|none             |     0|acc        |â†‘  | 0.4727|Â±  |0.0478|
|  - security_studies                                      |      1|none             |     0|acc        |â†‘  | 0.5388|Â±  |0.0319|
|  - sociology                                             |      1|none             |     0|acc        |â†‘  | 0.6468|Â±  |0.0338|
|  - us_foreign_policy                                     |      1|none             |     0|acc        |â†‘  | 0.7100|Â±  |0.0456|
| - stem                                                   |      2|none             |      |acc        |â†‘  | 0.3825|Â±  |0.0085|
|  - abstract_algebra                                      |      1|none             |     0|acc        |â†‘  | 0.2400|Â±  |0.0429|
|  - anatomy                                               |      1|none             |     0|acc        |â†‘  | 0.4815|Â±  |0.0432|
|  - astronomy                                             |      1|none             |     0|acc        |â†‘  | 0.5395|Â±  |0.0406|
|  - college_biology                                       |      1|none             |     0|acc        |â†‘  | 0.4931|Â±  |0.0418|
|  - college_chemistry                                     |      1|none             |     0|acc        |â†‘  | 0.3700|Â±  |0.0485|
|  - college_computer_science                              |      1|none             |     0|acc        |â†‘  | 0.3600|Â±  |0.0482|
|  - college_mathematics                                   |      1|none             |     0|acc        |â†‘  | 0.2800|Â±  |0.0451|
|  - college_physics                                       |      1|none             |     0|acc        |â†‘  | 0.2549|Â±  |0.0434|
|  - computer_security                                     |      1|none             |     0|acc        |â†‘  | 0.4800|Â±  |0.0502|
|  - conceptual_physics                                    |      1|none             |     0|acc        |â†‘  | 0.4340|Â±  |0.0324|
|  - electrical_engineering                                |      1|none             |     0|acc        |â†‘  | 0.5448|Â±  |0.0415|
|  - elementary_mathematics                                |      1|none             |     0|acc        |â†‘  | 0.2910|Â±  |0.0234|
|  - high_school_biology                                   |      1|none             |     0|acc        |â†‘  | 0.4968|Â±  |0.0284|
|  - high_school_chemistry                                 |      1|none             |     0|acc        |â†‘  | 0.3547|Â±  |0.0337|
|  - high_school_computer_science                          |      1|none             |     0|acc        |â†‘  | 0.4600|Â±  |0.0501|
|  - high_school_mathematics                               |      1|none             |     0|acc        |â†‘  | 0.2630|Â±  |0.0268|
|  - high_school_physics                                   |      1|none             |     0|acc        |â†‘  | 0.2980|Â±  |0.0373|
|  - high_school_statistics                                |      1|none             |     0|acc        |â†‘  | 0.3472|Â±  |0.0325|
|  - machine_learning                                      |      1|none             |     0|acc        |â†‘  | 0.3125|Â±  |0.0440|
|nq_open                                                   |      4|remove_whitespace|     0|exact_match|â†‘  | 0.0565|Â±  |0.0038|
|openbookqa                                                |      1|none             |     0|acc        |â†‘  | 0.2440|Â±  |0.0192|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3460|Â±  |0.0213|
|piqa                                                      |      1|none             |     0|acc        |â†‘  | 0.7437|Â±  |0.0102|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.7421|Â±  |0.0102|
|qnli                                                      |      1|none             |     0|acc        |â†‘  | 0.4946|Â±  |0.0068|
|sciq                                                      |      1|none             |     0|acc        |â†‘  | 0.9390|Â±  |0.0076|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.8970|Â±  |0.0096|
|triviaqa                                                  |      3|remove_whitespace|     0|exact_match|â†‘  | 0.2499|Â±  |0.0032|
|truthfulqa_gen                                            |      3|none             |     0|bleu_acc   |â†‘  | 0.3647|Â±  |0.0169|
|                                                          |       |none             |     0|bleu_diff  |â†‘  |-2.3149|Â±  |0.6149|
|                                                          |       |none             |     0|bleu_max   |â†‘  |17.3007|Â±  |0.6267|
|                                                          |       |none             |     0|rouge1_acc |â†‘  | 0.3550|Â±  |0.0168|
|                                                          |       |none             |     0|rouge1_diff|â†‘  |-4.7074|Â±  |0.9203|
|                                                          |       |none             |     0|rouge1_max |â†‘  |38.8035|Â±  |0.8508|
|                                                          |       |none             |     0|rouge2_acc |â†‘  | 0.2411|Â±  |0.0150|
|                                                          |       |none             |     0|rouge2_diff|â†‘  |-4.5701|Â±  |0.9279|
|                                                          |       |none             |     0|rouge2_max |â†‘  |22.7998|Â±  |0.8999|
|                                                          |       |none             |     0|rougeL_acc |â†‘  | 0.3672|Â±  |0.0169|
|                                                          |       |none             |     0|rougeL_diff|â†‘  |-4.7792|Â±  |0.9300|
|                                                          |       |none             |     0|rougeL_max |â†‘  |36.3004|Â±  |0.8466|
|truthfulqa_mc1                                            |      2|none             |     0|acc        |â†‘  | 0.2717|Â±  |0.0156|
|truthfulqa_mc2                                            |      3|none             |     0|acc        |â†‘  | 0.4383|Â±  |0.0144|
|winogrande                                                |      1|none             |     0|acc        |â†‘  | 0.6014|Â±  |0.0138|

|      Groups      |Version|  Filter  |n-shot|  Metric   |   |Value |   |Stderr|
|------------------|------:|----------|------|-----------|---|-----:|---|-----:|
|bbh               |      3|get-answer|      |exact_match|â†‘  |0.3781|Â±  |0.0055|
|mmlu              |      2|none      |      |acc        |â†‘  |0.4589|Â±  |0.0041|
| - humanities     |      2|none      |      |acc        |â†‘  |0.4389|Â±  |0.0071|
| - other          |      2|none      |      |acc        |â†‘  |0.5182|Â±  |0.0088|
| - social sciences|      2|none      |      |acc        |â†‘  |0.5080|Â±  |0.0088|
| - stem           |      2|none      |      |acc        |â†‘  |0.3825|Â±  |0.0085|

meta-llama_Llama-3.2-1B-Instruct: 3h 31m 2s
âœ… Benchmark completed for meta-llama_Llama-3.2-1B-Instruct

ðŸ”¥ Starting benchmark for meta-llama_Llama-3.2-3B-Instruct
Passed argument batch_size = auto:1. Detecting largest batch size
Determined largest batch size: 2
Passed argument batch_size = auto. Detecting largest batch size
Determined Largest batch size: 1
hf (pretrained=/home/jaymin/Documents/llm/llm_models/meta-llama_Llama-3.2-3B-Instruct), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto (2)
|                          Tasks                           |Version|     Filter      |n-shot|  Metric   |   | Value |   |Stderr|
|----------------------------------------------------------|------:|-----------------|-----:|-----------|---|------:|---|-----:|
|anli_r1                                                   |      1|none             |     0|acc        |â†‘  | 0.4470|Â±  |0.0157|
|anli_r2                                                   |      1|none             |     0|acc        |â†‘  | 0.4180|Â±  |0.0156|
|anli_r3                                                   |      1|none             |     0|acc        |â†‘  | 0.4308|Â±  |0.0143|
|arc_challenge                                             |      1|none             |     0|acc        |â†‘  | 0.4326|Â±  |0.0145|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.4590|Â±  |0.0146|
|bbh                                                       |      3|get-answer       |      |exact_match|â†‘  | 0.5564|Â±  |0.0056|
| - bbh_cot_fewshot_boolean_expressions                    |      4|get-answer       |     3|exact_match|â†‘  | 0.8840|Â±  |0.0203|
| - bbh_cot_fewshot_causal_judgement                       |      4|get-answer       |     3|exact_match|â†‘  | 0.5134|Â±  |0.0366|
| - bbh_cot_fewshot_date_understanding                     |      4|get-answer       |     3|exact_match|â†‘  | 0.7080|Â±  |0.0288|
| - bbh_cot_fewshot_disambiguation_qa                      |      4|get-answer       |     3|exact_match|â†‘  | 0.5560|Â±  |0.0315|
| - bbh_cot_fewshot_dyck_languages                         |      4|get-answer       |     3|exact_match|â†‘  | 0.0600|Â±  |0.0151|
| - bbh_cot_fewshot_formal_fallacies                       |      4|get-answer       |     3|exact_match|â†‘  | 0.3800|Â±  |0.0308|
| - bbh_cot_fewshot_geometric_shapes                       |      4|get-answer       |     3|exact_match|â†‘  | 0.3840|Â±  |0.0308|
| - bbh_cot_fewshot_hyperbaton                             |      4|get-answer       |     3|exact_match|â†‘  | 0.7120|Â±  |0.0287|
| - bbh_cot_fewshot_logical_deduction_five_objects         |      4|get-answer       |     3|exact_match|â†‘  | 0.4640|Â±  |0.0316|
| - bbh_cot_fewshot_logical_deduction_seven_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.3880|Â±  |0.0309|
| - bbh_cot_fewshot_logical_deduction_three_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.7880|Â±  |0.0259|
| - bbh_cot_fewshot_movie_recommendation                   |      4|get-answer       |     3|exact_match|â†‘  | 0.6520|Â±  |0.0302|
| - bbh_cot_fewshot_multistep_arithmetic_two               |      4|get-answer       |     3|exact_match|â†‘  | 0.5640|Â±  |0.0314|
| - bbh_cot_fewshot_navigate                               |      4|get-answer       |     3|exact_match|â†‘  | 0.7720|Â±  |0.0266|
| - bbh_cot_fewshot_object_counting                        |      4|get-answer       |     3|exact_match|â†‘  | 0.8280|Â±  |0.0239|
| - bbh_cot_fewshot_penguins_in_a_table                    |      4|get-answer       |     3|exact_match|â†‘  | 0.5822|Â±  |0.0410|
| - bbh_cot_fewshot_reasoning_about_colored_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.6560|Â±  |0.0301|
| - bbh_cot_fewshot_ruin_names                             |      4|get-answer       |     3|exact_match|â†‘  | 0.5640|Â±  |0.0314|
| - bbh_cot_fewshot_salient_translation_error_detection    |      4|get-answer       |     3|exact_match|â†‘  | 0.4480|Â±  |0.0315|
| - bbh_cot_fewshot_snarks                                 |      4|get-answer       |     3|exact_match|â†‘  | 0.5899|Â±  |0.0370|
| - bbh_cot_fewshot_sports_understanding                   |      4|get-answer       |     3|exact_match|â†‘  | 0.8440|Â±  |0.0230|
| - bbh_cot_fewshot_temporal_sequences                     |      4|get-answer       |     3|exact_match|â†‘  | 0.4960|Â±  |0.0317|
| - bbh_cot_fewshot_tracking_shuffled_objects_five_objects |      4|get-answer       |     3|exact_match|â†‘  | 0.3600|Â±  |0.0304|
| - bbh_cot_fewshot_tracking_shuffled_objects_seven_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.2040|Â±  |0.0255|
| - bbh_cot_fewshot_tracking_shuffled_objects_three_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.5440|Â±  |0.0316|
| - bbh_cot_fewshot_web_of_lies                            |      4|get-answer       |     3|exact_match|â†‘  | 0.8480|Â±  |0.0228|
| - bbh_cot_fewshot_word_sorting                           |      4|get-answer       |     3|exact_match|â†‘  | 0.2440|Â±  |0.0272|
|boolq                                                     |      2|none             |     0|acc        |â†‘  | 0.7847|Â±  |0.0072|
|drop                                                      |      3|none             |     0|em         |â†‘  | 0.0259|Â±  |0.0016|
|                                                          |       |none             |     0|f1         |â†‘  | 0.1554|Â±  |0.0025|
|gpqa_diamond_cot_n_shot                                   |      2|flexible-extract |     0|exact_match|â†‘  | 0.0960|Â±  |0.0210|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_cot_zeroshot                                 |      1|flexible-extract |     0|exact_match|â†‘  | 0.1162|Â±  |0.0228|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_generative_n_shot                            |      2|flexible-extract |     0|exact_match|â†‘  | 0.2172|Â±  |0.0294|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_n_shot                                       |      2|none             |     0|acc        |â†‘  | 0.3030|Â±  |0.0327|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3030|Â±  |0.0327|
|gpqa_diamond_zeroshot                                     |      1|none             |     0|acc        |â†‘  | 0.2828|Â±  |0.0321|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2828|Â±  |0.0321|
|gpqa_extended_cot_n_shot                                  |      2|flexible-extract |     0|exact_match|â†‘  | 0.1136|Â±  |0.0136|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0018|Â±  |0.0018|
|gpqa_extended_cot_zeroshot                                |      1|flexible-extract |     0|exact_match|â†‘  | 0.1374|Â±  |0.0147|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_generative_n_shot                           |      2|flexible-extract |     0|exact_match|â†‘  | 0.2326|Â±  |0.0181|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0018|Â±  |0.0018|
|gpqa_extended_n_shot                                      |      2|none             |     0|acc        |â†‘  | 0.2802|Â±  |0.0192|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2802|Â±  |0.0192|
|gpqa_extended_zeroshot                                    |      1|none             |     0|acc        |â†‘  | 0.3407|Â±  |0.0203|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3407|Â±  |0.0203|
|gpqa_main_cot_n_shot                                      |      2|flexible-extract |     0|exact_match|â†‘  | 0.1317|Â±  |0.0160|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0045|Â±  |0.0032|
|gpqa_main_cot_zeroshot                                    |      1|flexible-extract |     0|exact_match|â†‘  | 0.1183|Â±  |0.0153|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_generative_n_shot                               |      2|flexible-extract |     0|exact_match|â†‘  | 0.2344|Â±  |0.0200|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_n_shot                                          |      2|none             |     0|acc        |â†‘  | 0.3147|Â±  |0.0220|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3147|Â±  |0.0220|
|gpqa_main_zeroshot                                        |      1|none             |     0|acc        |â†‘  | 0.3281|Â±  |0.0222|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3281|Â±  |0.0222|
|gsm8k                                                     |      3|flexible-extract |     5|exact_match|â†‘  | 0.6497|Â±  |0.0131|
|                                                          |       |strict-match     |     5|exact_match|â†‘  | 0.6422|Â±  |0.0132|
|hellaswag                                                 |      1|none             |     0|acc        |â†‘  | 0.5225|Â±  |0.0050|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.7054|Â±  |0.0045|
|mmlu                                                      |      2|none             |      |acc        |â†‘  | 0.6052|Â±  |0.0040|
| - humanities                                             |      2|none             |      |acc        |â†‘  | 0.5949|Â±  |0.0070|
|  - formal_logic                                          |      1|none             |     0|acc        |â†‘  | 0.3968|Â±  |0.0438|
|  - high_school_european_history                          |      1|none             |     0|acc        |â†‘  | 0.7333|Â±  |0.0345|
|  - high_school_us_history                                |      1|none             |     0|acc        |â†‘  | 0.7500|Â±  |0.0304|
|  - high_school_world_history                             |      1|none             |     0|acc        |â†‘  | 0.7975|Â±  |0.0262|
|  - international_law                                     |      1|none             |     0|acc        |â†‘  | 0.7355|Â±  |0.0403|
|  - jurisprudence                                         |      1|none             |     0|acc        |â†‘  | 0.6204|Â±  |0.0469|
|  - logical_fallacies                                     |      1|none             |     0|acc        |â†‘  | 0.7239|Â±  |0.0351|
|  - moral_disputes                                        |      1|none             |     0|acc        |â†‘  | 0.6503|Â±  |0.0257|
|  - moral_scenarios                                       |      1|none             |     0|acc        |â†‘  | 0.5955|Â±  |0.0164|
|  - philosophy                                            |      1|none             |     0|acc        |â†‘  | 0.6559|Â±  |0.0270|
|  - prehistory                                            |      1|none             |     0|acc        |â†‘  | 0.6512|Â±  |0.0265|
|  - professional_law                                      |      1|none             |     0|acc        |â†‘  | 0.4622|Â±  |0.0127|
|  - world_religions                                       |      1|none             |     0|acc        |â†‘  | 0.7602|Â±  |0.0327|
| - other                                                  |      2|none             |      |acc        |â†‘  | 0.6598|Â±  |0.0082|
|  - business_ethics                                       |      1|none             |     0|acc        |â†‘  | 0.5600|Â±  |0.0499|
|  - clinical_knowledge                                    |      1|none             |     0|acc        |â†‘  | 0.6226|Â±  |0.0298|
|  - college_medicine                                      |      1|none             |     0|acc        |â†‘  | 0.5896|Â±  |0.0375|
|  - global_facts                                          |      1|none             |     0|acc        |â†‘  | 0.3400|Â±  |0.0476|
|  - human_aging                                           |      1|none             |     0|acc        |â†‘  | 0.5830|Â±  |0.0331|
|  - management                                            |      1|none             |     0|acc        |â†‘  | 0.7670|Â±  |0.0419|
|  - marketing                                             |      1|none             |     0|acc        |â†‘  | 0.8761|Â±  |0.0216|
|  - medical_genetics                                      |      1|none             |     0|acc        |â†‘  | 0.7400|Â±  |0.0441|
|  - miscellaneous                                         |      1|none             |     0|acc        |â†‘  | 0.7535|Â±  |0.0154|
|  - nutrition                                             |      1|none             |     0|acc        |â†‘  | 0.6634|Â±  |0.0271|
|  - professional_accounting                               |      1|none             |     0|acc        |â†‘  | 0.4752|Â±  |0.0298|
|  - professional_medicine                                 |      1|none             |     0|acc        |â†‘  | 0.7463|Â±  |0.0264|
|  - virology                                              |      1|none             |     0|acc        |â†‘  | 0.4518|Â±  |0.0387|
| - social sciences                                        |      2|none             |      |acc        |â†‘  | 0.6675|Â±  |0.0083|
|  - econometrics                                          |      1|none             |     0|acc        |â†‘  | 0.3947|Â±  |0.0460|
|  - high_school_geography                                 |      1|none             |     0|acc        |â†‘  | 0.7273|Â±  |0.0317|
|  - high_school_government_and_politics                   |      1|none             |     0|acc        |â†‘  | 0.7513|Â±  |0.0312|
|  - high_school_macroeconomics                            |      1|none             |     0|acc        |â†‘  | 0.5590|Â±  |0.0252|
|  - high_school_microeconomics                            |      1|none             |     0|acc        |â†‘  | 0.6218|Â±  |0.0315|
|  - high_school_psychology                                |      1|none             |     0|acc        |â†‘  | 0.7651|Â±  |0.0182|
|  - human_sexuality                                       |      1|none             |     0|acc        |â†‘  | 0.6794|Â±  |0.0409|
|  - professional_psychology                               |      1|none             |     0|acc        |â†‘  | 0.6111|Â±  |0.0197|
|  - public_relations                                      |      1|none             |     0|acc        |â†‘  | 0.6091|Â±  |0.0467|
|  - security_studies                                      |      1|none             |     0|acc        |â†‘  | 0.6612|Â±  |0.0303|
|  - sociology                                             |      1|none             |     0|acc        |â†‘  | 0.8109|Â±  |0.0277|
|  - us_foreign_policy                                     |      1|none             |     0|acc        |â†‘  | 0.8200|Â±  |0.0386|
| - stem                                                   |      2|none             |      |acc        |â†‘  | 0.5059|Â±  |0.0086|
|  - abstract_algebra                                      |      1|none             |     0|acc        |â†‘  | 0.3100|Â±  |0.0465|
|  - anatomy                                               |      1|none             |     0|acc        |â†‘  | 0.6000|Â±  |0.0423|
|  - astronomy                                             |      1|none             |     0|acc        |â†‘  | 0.6776|Â±  |0.0380|
|  - college_biology                                       |      1|none             |     0|acc        |â†‘  | 0.7083|Â±  |0.0380|
|  - college_chemistry                                     |      1|none             |     0|acc        |â†‘  | 0.3600|Â±  |0.0482|
|  - college_computer_science                              |      1|none             |     0|acc        |â†‘  | 0.4700|Â±  |0.0502|
|  - college_mathematics                                   |      1|none             |     0|acc        |â†‘  | 0.3400|Â±  |0.0476|
|  - college_physics                                       |      1|none             |     0|acc        |â†‘  | 0.3529|Â±  |0.0476|
|  - computer_security                                     |      1|none             |     0|acc        |â†‘  | 0.7000|Â±  |0.0461|
|  - conceptual_physics                                    |      1|none             |     0|acc        |â†‘  | 0.5106|Â±  |0.0327|
|  - electrical_engineering                                |      1|none             |     0|acc        |â†‘  | 0.5793|Â±  |0.0411|
|  - elementary_mathematics                                |      1|none             |     0|acc        |â†‘  | 0.4127|Â±  |0.0254|
|  - high_school_biology                                   |      1|none             |     0|acc        |â†‘  | 0.7065|Â±  |0.0259|
|  - high_school_chemistry                                 |      1|none             |     0|acc        |â†‘  | 0.5369|Â±  |0.0351|
|  - high_school_computer_science                          |      1|none             |     0|acc        |â†‘  | 0.6100|Â±  |0.0490|
|  - high_school_mathematics                               |      1|none             |     0|acc        |â†‘  | 0.3667|Â±  |0.0294|
|  - high_school_physics                                   |      1|none             |     0|acc        |â†‘  | 0.4040|Â±  |0.0401|
|  - high_school_statistics                                |      1|none             |     0|acc        |â†‘  | 0.4167|Â±  |0.0336|
|  - machine_learning                                      |      1|none             |     0|acc        |â†‘  | 0.5000|Â±  |0.0475|
|nq_open                                                   |      4|remove_whitespace|     0|exact_match|â†‘  | 0.1391|Â±  |0.0058|
|openbookqa                                                |      1|none             |     0|acc        |â†‘  | 0.2740|Â±  |0.0200|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3580|Â±  |0.0215|
|piqa                                                      |      1|none             |     0|acc        |â†‘  | 0.7552|Â±  |0.0100|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.7552|Â±  |0.0100|
|qnli                                                      |      1|none             |     0|acc        |â†‘  | 0.5451|Â±  |0.0067|
|sciq                                                      |      1|none             |     0|acc        |â†‘  | 0.9520|Â±  |0.0068|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.9320|Â±  |0.0080|
|triviaqa                                                  |      3|remove_whitespace|     0|exact_match|â†‘  | 0.3389|Â±  |0.0035|
|truthfulqa_gen                                            |      3|none             |     0|bleu_acc   |â†‘  | 0.5581|Â±  |0.0174|
|                                                          |       |none             |     0|bleu_diff  |â†‘  |13.1234|Â±  |1.1545|
|                                                          |       |none             |     0|bleu_max   |â†‘  |35.5541|Â±  |0.8709|
|                                                          |       |none             |     0|rouge1_acc |â†‘  | 0.5508|Â±  |0.0174|
|                                                          |       |none             |     0|rouge1_diff|â†‘  |18.8892|Â±  |1.6130|
|                                                          |       |none             |     0|rouge1_max |â†‘  |60.6706|Â±  |0.9979|
|                                                          |       |none             |     0|rouge2_acc |â†‘  | 0.5067|Â±  |0.0175|
|                                                          |       |none             |     0|rouge2_diff|â†‘  |19.4222|Â±  |1.7185|
|                                                          |       |none             |     0|rouge2_max |â†‘  |47.9947|Â±  |1.2171|
|                                                          |       |none             |     0|rougeL_acc |â†‘  | 0.5361|Â±  |0.0175|
|                                                          |       |none             |     0|rougeL_diff|â†‘  |18.4665|Â±  |1.6326|
|                                                          |       |none             |     0|rougeL_max |â†‘  |58.5696|Â±  |1.0349|
|truthfulqa_mc1                                            |      2|none             |     0|acc        |â†‘  | 0.3268|Â±  |0.0164|
|truthfulqa_mc2                                            |      3|none             |     0|acc        |â†‘  | 0.4976|Â±  |0.0148|
|winogrande                                                |      1|none             |     0|acc        |â†‘  | 0.6709|Â±  |0.0132|

|      Groups      |Version|  Filter  |n-shot|  Metric   |   |Value |   |Stderr|
|------------------|------:|----------|------|-----------|---|-----:|---|-----:|
|bbh               |      3|get-answer|      |exact_match|â†‘  |0.5564|Â±  |0.0056|
|mmlu              |      2|none      |      |acc        |â†‘  |0.6052|Â±  |0.0040|
| - humanities     |      2|none      |      |acc        |â†‘  |0.5949|Â±  |0.0070|
| - other          |      2|none      |      |acc        |â†‘  |0.6598|Â±  |0.0082|
| - social sciences|      2|none      |      |acc        |â†‘  |0.6675|Â±  |0.0083|
| - stem           |      2|none      |      |acc        |â†‘  |0.5059|Â±  |0.0086|

meta-llama_Llama-3.2-3B-Instruct: 7h 12m 29s
âœ… Benchmark completed for meta-llama_Llama-3.2-3B-Instruct

ðŸ”¥ Starting benchmark for meta-llama_Llama-3.1-8B-Instruct
Passed argument batch_size = auto:1. Detecting largest batch size
Determined largest batch size: 1
Passed argument batch_size = auto. Detecting largest batch size
Determined Largest batch size: 1
hf (pretrained=/home/jaymin/Documents/llm/llm_models/meta-llama_Llama-3.1-8B-Instruct), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto (1)
|                          Tasks                           |Version|     Filter      |n-shot|  Metric   |   | Value |   |Stderr|
|----------------------------------------------------------|------:|-----------------|-----:|-----------|---|------:|---|-----:|
|anli_r1                                                   |      1|none             |     0|acc        |â†‘  | 0.4820|Â±  |0.0158|
|anli_r2                                                   |      1|none             |     0|acc        |â†‘  | 0.4670|Â±  |0.0158|
|anli_r3                                                   |      1|none             |     0|acc        |â†‘  | 0.4433|Â±  |0.0143|
|arc_challenge                                             |      1|none             |     0|acc        |â†‘  | 0.5179|Â±  |0.0146|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.5503|Â±  |0.0145|
|bbh                                                       |      3|get-answer       |      |exact_match|â†‘  | 0.7156|Â±  |0.0051|
| - bbh_cot_fewshot_boolean_expressions                    |      4|get-answer       |     3|exact_match|â†‘  | 0.9160|Â±  |0.0176|
| - bbh_cot_fewshot_causal_judgement                       |      4|get-answer       |     3|exact_match|â†‘  | 0.5775|Â±  |0.0362|
| - bbh_cot_fewshot_date_understanding                     |      4|get-answer       |     3|exact_match|â†‘  | 0.8000|Â±  |0.0253|
| - bbh_cot_fewshot_disambiguation_qa                      |      4|get-answer       |     3|exact_match|â†‘  | 0.6160|Â±  |0.0308|
| - bbh_cot_fewshot_dyck_languages                         |      4|get-answer       |     3|exact_match|â†‘  | 0.1200|Â±  |0.0206|
| - bbh_cot_fewshot_formal_fallacies                       |      4|get-answer       |     3|exact_match|â†‘  | 0.4920|Â±  |0.0317|
| - bbh_cot_fewshot_geometric_shapes                       |      4|get-answer       |     3|exact_match|â†‘  | 0.5160|Â±  |0.0317|
| - bbh_cot_fewshot_hyperbaton                             |      4|get-answer       |     3|exact_match|â†‘  | 0.8480|Â±  |0.0228|
| - bbh_cot_fewshot_logical_deduction_five_objects         |      4|get-answer       |     3|exact_match|â†‘  | 0.5400|Â±  |0.0316|
| - bbh_cot_fewshot_logical_deduction_seven_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.4280|Â±  |0.0314|
| - bbh_cot_fewshot_logical_deduction_three_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.8400|Â±  |0.0232|
| - bbh_cot_fewshot_movie_recommendation                   |      4|get-answer       |     3|exact_match|â†‘  | 0.7800|Â±  |0.0263|
| - bbh_cot_fewshot_multistep_arithmetic_two               |      4|get-answer       |     3|exact_match|â†‘  | 0.6680|Â±  |0.0298|
| - bbh_cot_fewshot_navigate                               |      4|get-answer       |     3|exact_match|â†‘  | 0.9080|Â±  |0.0183|
| - bbh_cot_fewshot_object_counting                        |      4|get-answer       |     3|exact_match|â†‘  | 0.8760|Â±  |0.0209|
| - bbh_cot_fewshot_penguins_in_a_table                    |      4|get-answer       |     3|exact_match|â†‘  | 0.7945|Â±  |0.0336|
| - bbh_cot_fewshot_reasoning_about_colored_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.7600|Â±  |0.0271|
| - bbh_cot_fewshot_ruin_names                             |      4|get-answer       |     3|exact_match|â†‘  | 0.7680|Â±  |0.0268|
| - bbh_cot_fewshot_salient_translation_error_detection    |      4|get-answer       |     3|exact_match|â†‘  | 0.5720|Â±  |0.0314|
| - bbh_cot_fewshot_snarks                                 |      4|get-answer       |     3|exact_match|â†‘  | 0.6517|Â±  |0.0358|
| - bbh_cot_fewshot_sports_understanding                   |      4|get-answer       |     3|exact_match|â†‘  | 0.9400|Â±  |0.0151|
| - bbh_cot_fewshot_temporal_sequences                     |      4|get-answer       |     3|exact_match|â†‘  | 0.8840|Â±  |0.0203|
| - bbh_cot_fewshot_tracking_shuffled_objects_five_objects |      4|get-answer       |     3|exact_match|â†‘  | 0.8280|Â±  |0.0239|
| - bbh_cot_fewshot_tracking_shuffled_objects_seven_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.8000|Â±  |0.0253|
| - bbh_cot_fewshot_tracking_shuffled_objects_three_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.8400|Â±  |0.0232|
| - bbh_cot_fewshot_web_of_lies                            |      4|get-answer       |     3|exact_match|â†‘  | 1.0000|Â±  |0.0000|
| - bbh_cot_fewshot_word_sorting                           |      4|get-answer       |     3|exact_match|â†‘  | 0.5360|Â±  |0.0316|
|boolq                                                     |      2|none             |     0|acc        |â†‘  | 0.8416|Â±  |0.0064|
|drop                                                      |      3|none             |     0|em         |â†‘  | 0.0448|Â±  |0.0021|
|                                                          |       |none             |     0|f1         |â†‘  | 0.1937|Â±  |0.0028|
|gpqa_diamond_cot_n_shot                                   |      2|flexible-extract |     0|exact_match|â†‘  | 0.1010|Â±  |0.0215|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_cot_zeroshot                                 |      1|flexible-extract |     0|exact_match|â†‘  | 0.1313|Â±  |0.0241|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_generative_n_shot                            |      2|flexible-extract |     0|exact_match|â†‘  | 0.3182|Â±  |0.0332|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_n_shot                                       |      2|none             |     0|acc        |â†‘  | 0.3535|Â±  |0.0341|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3535|Â±  |0.0341|
|gpqa_diamond_zeroshot                                     |      1|none             |     0|acc        |â†‘  | 0.3232|Â±  |0.0333|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3232|Â±  |0.0333|
|gpqa_extended_cot_n_shot                                  |      2|flexible-extract |     0|exact_match|â†‘  | 0.1374|Â±  |0.0147|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_cot_zeroshot                                |      1|flexible-extract |     0|exact_match|â†‘  | 0.1703|Â±  |0.0161|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_generative_n_shot                           |      2|flexible-extract |     0|exact_match|â†‘  | 0.2894|Â±  |0.0194|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_n_shot                                      |      2|none             |     0|acc        |â†‘  | 0.3150|Â±  |0.0199|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3150|Â±  |0.0199|
|gpqa_extended_zeroshot                                    |      1|none             |     0|acc        |â†‘  | 0.3132|Â±  |0.0199|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3132|Â±  |0.0199|
|gpqa_main_cot_n_shot                                      |      2|flexible-extract |     0|exact_match|â†‘  | 0.1362|Â±  |0.0162|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_cot_zeroshot                                    |      1|flexible-extract |     0|exact_match|â†‘  | 0.1272|Â±  |0.0158|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_generative_n_shot                               |      2|flexible-extract |     0|exact_match|â†‘  | 0.2723|Â±  |0.0211|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_n_shot                                          |      2|none             |     0|acc        |â†‘  | 0.3393|Â±  |0.0224|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3393|Â±  |0.0224|
|gpqa_main_zeroshot                                        |      1|none             |     0|acc        |â†‘  | 0.3438|Â±  |0.0225|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3438|Â±  |0.0225|
|gsm8k                                                     |      3|flexible-extract |     5|exact_match|â†‘  | 0.7779|Â±  |0.0114|
|                                                          |       |strict-match     |     5|exact_match|â†‘  | 0.7544|Â±  |0.0119|
|hellaswag                                                 |      1|none             |     0|acc        |â†‘  | 0.5909|Â±  |0.0049|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.7921|Â±  |0.0040|
|mmlu                                                      |      2|none             |      |acc        |â†‘  | 0.6793|Â±  |0.0038|
| - humanities                                             |      2|none             |      |acc        |â†‘  | 0.6427|Â±  |0.0067|
|  - formal_logic                                          |      1|none             |     0|acc        |â†‘  | 0.4762|Â±  |0.0447|
|  - high_school_european_history                          |      1|none             |     0|acc        |â†‘  | 0.7636|Â±  |0.0332|
|  - high_school_us_history                                |      1|none             |     0|acc        |â†‘  | 0.8431|Â±  |0.0255|
|  - high_school_world_history                             |      1|none             |     0|acc        |â†‘  | 0.8565|Â±  |0.0228|
|  - international_law                                     |      1|none             |     0|acc        |â†‘  | 0.8182|Â±  |0.0352|
|  - jurisprudence                                         |      1|none             |     0|acc        |â†‘  | 0.7778|Â±  |0.0402|
|  - logical_fallacies                                     |      1|none             |     0|acc        |â†‘  | 0.7914|Â±  |0.0319|
|  - moral_disputes                                        |      1|none             |     0|acc        |â†‘  | 0.7457|Â±  |0.0234|
|  - moral_scenarios                                       |      1|none             |     0|acc        |â†‘  | 0.5721|Â±  |0.0165|
|  - philosophy                                            |      1|none             |     0|acc        |â†‘  | 0.7203|Â±  |0.0255|
|  - prehistory                                            |      1|none             |     0|acc        |â†‘  | 0.7438|Â±  |0.0243|
|  - professional_law                                      |      1|none             |     0|acc        |â†‘  | 0.5039|Â±  |0.0128|
|  - world_religions                                       |      1|none             |     0|acc        |â†‘  | 0.8363|Â±  |0.0284|
| - other                                                  |      2|none             |      |acc        |â†‘  | 0.7422|Â±  |0.0075|
|  - business_ethics                                       |      1|none             |     0|acc        |â†‘  | 0.6800|Â±  |0.0469|
|  - clinical_knowledge                                    |      1|none             |     0|acc        |â†‘  | 0.7849|Â±  |0.0253|
|  - college_medicine                                      |      1|none             |     0|acc        |â†‘  | 0.6936|Â±  |0.0351|
|  - global_facts                                          |      1|none             |     0|acc        |â†‘  | 0.3800|Â±  |0.0488|
|  - human_aging                                           |      1|none             |     0|acc        |â†‘  | 0.7040|Â±  |0.0306|
|  - management                                            |      1|none             |     0|acc        |â†‘  | 0.8155|Â±  |0.0384|
|  - marketing                                             |      1|none             |     0|acc        |â†‘  | 0.8932|Â±  |0.0202|
|  - medical_genetics                                      |      1|none             |     0|acc        |â†‘  | 0.7800|Â±  |0.0416|
|  - miscellaneous                                         |      1|none             |     0|acc        |â†‘  | 0.8404|Â±  |0.0131|
|  - nutrition                                             |      1|none             |     0|acc        |â†‘  | 0.7549|Â±  |0.0246|
|  - professional_accounting                               |      1|none             |     0|acc        |â†‘  | 0.5532|Â±  |0.0297|
|  - professional_medicine                                 |      1|none             |     0|acc        |â†‘  | 0.7831|Â±  |0.0250|
|  - virology                                              |      1|none             |     0|acc        |â†‘  | 0.5181|Â±  |0.0389|
| - social sciences                                        |      2|none             |      |acc        |â†‘  | 0.7689|Â±  |0.0074|
|  - econometrics                                          |      1|none             |     0|acc        |â†‘  | 0.5000|Â±  |0.0470|
|  - high_school_geography                                 |      1|none             |     0|acc        |â†‘  | 0.7929|Â±  |0.0289|
|  - high_school_government_and_politics                   |      1|none             |     0|acc        |â†‘  | 0.8756|Â±  |0.0238|
|  - high_school_macroeconomics                            |      1|none             |     0|acc        |â†‘  | 0.6795|Â±  |0.0237|
|  - high_school_microeconomics                            |      1|none             |     0|acc        |â†‘  | 0.7941|Â±  |0.0263|
|  - high_school_psychology                                |      1|none             |     0|acc        |â†‘  | 0.8587|Â±  |0.0149|
|  - human_sexuality                                       |      1|none             |     0|acc        |â†‘  | 0.7863|Â±  |0.0360|
|  - professional_psychology                               |      1|none             |     0|acc        |â†‘  | 0.7173|Â±  |0.0182|
|  - public_relations                                      |      1|none             |     0|acc        |â†‘  | 0.6818|Â±  |0.0446|
|  - security_studies                                      |      1|none             |     0|acc        |â†‘  | 0.7510|Â±  |0.0277|
|  - sociology                                             |      1|none             |     0|acc        |â†‘  | 0.8607|Â±  |0.0245|
|  - us_foreign_policy                                     |      1|none             |     0|acc        |â†‘  | 0.8700|Â±  |0.0338|
| - stem                                                   |      2|none             |      |acc        |â†‘  | 0.5845|Â±  |0.0084|
|  - abstract_algebra                                      |      1|none             |     0|acc        |â†‘  | 0.3400|Â±  |0.0476|
|  - anatomy                                               |      1|none             |     0|acc        |â†‘  | 0.6889|Â±  |0.0400|
|  - astronomy                                             |      1|none             |     0|acc        |â†‘  | 0.7566|Â±  |0.0349|
|  - college_biology                                       |      1|none             |     0|acc        |â†‘  | 0.8125|Â±  |0.0326|
|  - college_chemistry                                     |      1|none             |     0|acc        |â†‘  | 0.4600|Â±  |0.0501|
|  - college_computer_science                              |      1|none             |     0|acc        |â†‘  | 0.5800|Â±  |0.0496|
|  - college_mathematics                                   |      1|none             |     0|acc        |â†‘  | 0.3400|Â±  |0.0476|
|  - college_physics                                       |      1|none             |     0|acc        |â†‘  | 0.4314|Â±  |0.0493|
|  - computer_security                                     |      1|none             |     0|acc        |â†‘  | 0.7500|Â±  |0.0435|
|  - conceptual_physics                                    |      1|none             |     0|acc        |â†‘  | 0.6000|Â±  |0.0320|
|  - electrical_engineering                                |      1|none             |     0|acc        |â†‘  | 0.6552|Â±  |0.0396|
|  - elementary_mathematics                                |      1|none             |     0|acc        |â†‘  | 0.4868|Â±  |0.0257|
|  - high_school_biology                                   |      1|none             |     0|acc        |â†‘  | 0.8065|Â±  |0.0225|
|  - high_school_chemistry                                 |      1|none             |     0|acc        |â†‘  | 0.6404|Â±  |0.0338|
|  - high_school_computer_science                          |      1|none             |     0|acc        |â†‘  | 0.7500|Â±  |0.0435|
|  - high_school_mathematics                               |      1|none             |     0|acc        |â†‘  | 0.4148|Â±  |0.0300|
|  - high_school_physics                                   |      1|none             |     0|acc        |â†‘  | 0.4636|Â±  |0.0407|
|  - high_school_statistics                                |      1|none             |     0|acc        |â†‘  | 0.5463|Â±  |0.0340|
|  - machine_learning                                      |      1|none             |     0|acc        |â†‘  | 0.4643|Â±  |0.0473|
|nq_open                                                   |      4|remove_whitespace|     0|exact_match|â†‘  | 0.1776|Â±  |0.0064|
|openbookqa                                                |      1|none             |     0|acc        |â†‘  | 0.3360|Â±  |0.0211|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.4320|Â±  |0.0222|
|piqa                                                      |      1|none             |     0|acc        |â†‘  | 0.8009|Â±  |0.0093|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.8063|Â±  |0.0092|
|qnli                                                      |      1|none             |     0|acc        |â†‘  | 0.5014|Â±  |0.0068|
|sciq                                                      |      1|none             |     0|acc        |â†‘  | 0.9670|Â±  |0.0057|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.9620|Â±  |0.0060|
|triviaqa                                                  |      3|remove_whitespace|     0|exact_match|â†‘  | 0.5182|Â±  |0.0037|
|truthfulqa_gen                                            |      3|none             |     0|bleu_acc   |â†‘  | 0.6255|Â±  |0.0169|
|                                                          |       |none             |     0|bleu_diff  |â†‘  |15.3392|Â±  |1.0952|
|                                                          |       |none             |     0|bleu_max   |â†‘  |36.1393|Â±  |0.8796|
|                                                          |       |none             |     0|rouge1_acc |â†‘  | 0.6083|Â±  |0.0171|
|                                                          |       |none             |     0|rouge1_diff|â†‘  |21.4366|Â±  |1.5980|
|                                                          |       |none             |     0|rouge1_max |â†‘  |60.7499|Â±  |0.9981|
|                                                          |       |none             |     0|rouge2_acc |â†‘  | 0.5606|Â±  |0.0174|
|                                                          |       |none             |     0|rouge2_diff|â†‘  |23.0331|Â±  |1.6393|
|                                                          |       |none             |     0|rouge2_max |â†‘  |48.3161|Â±  |1.2088|
|                                                          |       |none             |     0|rougeL_acc |â†‘  | 0.6083|Â±  |0.0171|
|                                                          |       |none             |     0|rougeL_diff|â†‘  |21.4950|Â±  |1.6133|
|                                                          |       |none             |     0|rougeL_max |â†‘  |58.8871|Â±  |1.0298|
|truthfulqa_mc1                                            |      2|none             |     0|acc        |â†‘  | 0.3660|Â±  |0.0169|
|truthfulqa_mc2                                            |      3|none             |     0|acc        |â†‘  | 0.5412|Â±  |0.0150|
|winogrande                                                |      1|none             |     0|acc        |â†‘  | 0.7388|Â±  |0.0123|

|      Groups      |Version|  Filter  |n-shot|  Metric   |   |Value |   |Stderr|
|------------------|------:|----------|------|-----------|---|-----:|---|-----:|
|bbh               |      3|get-answer|      |exact_match|â†‘  |0.7156|Â±  |0.0051|
|mmlu              |      2|none      |      |acc        |â†‘  |0.6793|Â±  |0.0038|
| - humanities     |      2|none      |      |acc        |â†‘  |0.6427|Â±  |0.0067|
| - other          |      2|none      |      |acc        |â†‘  |0.7422|Â±  |0.0075|
| - social sciences|      2|none      |      |acc        |â†‘  |0.7689|Â±  |0.0074|
| - stem           |      2|none      |      |acc        |â†‘  |0.5845|Â±  |0.0084|

meta-llama_Llama-3.1-8B-Instruct: 12h 19m 31s
âœ… Benchmark completed for meta-llama_Llama-3.1-8B-Instruct

ðŸ”¥ Starting benchmark for meta-llama_Llama-2-7b-chat-hf
Passed argument batch_size = auto:1. Detecting largest batch size
Determined largest batch size: 4
Passed argument batch_size = auto. Detecting largest batch size
Determined Largest batch size: 4
hf (pretrained=/home/jaymin/Documents/llm/llm_models/meta-llama_Llama-2-7b-chat-hf), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto (4)
|                          Tasks                           |Version|     Filter      |n-shot|  Metric   |   | Value |   |Stderr|
|----------------------------------------------------------|------:|-----------------|-----:|-----------|---|------:|---|-----:|
|anli_r1                                                   |      1|none             |     0|acc        |â†‘  | 0.4170|Â±  |0.0156|
|anli_r2                                                   |      1|none             |     0|acc        |â†‘  | 0.4100|Â±  |0.0156|
|anli_r3                                                   |      1|none             |     0|acc        |â†‘  | 0.4075|Â±  |0.0142|
|arc_challenge                                             |      1|none             |     0|acc        |â†‘  | 0.4411|Â±  |0.0145|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.4428|Â±  |0.0145|
|bbh                                                       |      3|get-answer       |      |exact_match|â†‘  | 0.4013|Â±  |0.0055|
| - bbh_cot_fewshot_boolean_expressions                    |      4|get-answer       |     3|exact_match|â†‘  | 0.6520|Â±  |0.0302|
| - bbh_cot_fewshot_causal_judgement                       |      4|get-answer       |     3|exact_match|â†‘  | 0.5455|Â±  |0.0365|
| - bbh_cot_fewshot_date_understanding                     |      4|get-answer       |     3|exact_match|â†‘  | 0.5680|Â±  |0.0314|
| - bbh_cot_fewshot_disambiguation_qa                      |      4|get-answer       |     3|exact_match|â†‘  | 0.3920|Â±  |0.0309|
| - bbh_cot_fewshot_dyck_languages                         |      4|get-answer       |     3|exact_match|â†‘  | 0.0360|Â±  |0.0118|
| - bbh_cot_fewshot_formal_fallacies                       |      4|get-answer       |     3|exact_match|â†‘  | 0.5280|Â±  |0.0316|
| - bbh_cot_fewshot_geometric_shapes                       |      4|get-answer       |     3|exact_match|â†‘  | 0.3280|Â±  |0.0298|
| - bbh_cot_fewshot_hyperbaton                             |      4|get-answer       |     3|exact_match|â†‘  | 0.5440|Â±  |0.0316|
| - bbh_cot_fewshot_logical_deduction_five_objects         |      4|get-answer       |     3|exact_match|â†‘  | 0.3560|Â±  |0.0303|
| - bbh_cot_fewshot_logical_deduction_seven_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.1960|Â±  |0.0252|
| - bbh_cot_fewshot_logical_deduction_three_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.6080|Â±  |0.0309|
| - bbh_cot_fewshot_movie_recommendation                   |      4|get-answer       |     3|exact_match|â†‘  | 0.6800|Â±  |0.0296|
| - bbh_cot_fewshot_multistep_arithmetic_two               |      4|get-answer       |     3|exact_match|â†‘  | 0.0200|Â±  |0.0089|
| - bbh_cot_fewshot_navigate                               |      4|get-answer       |     3|exact_match|â†‘  | 0.6040|Â±  |0.0310|
| - bbh_cot_fewshot_object_counting                        |      4|get-answer       |     3|exact_match|â†‘  | 0.4960|Â±  |0.0317|
| - bbh_cot_fewshot_penguins_in_a_table                    |      4|get-answer       |     3|exact_match|â†‘  | 0.3288|Â±  |0.0390|
| - bbh_cot_fewshot_reasoning_about_colored_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.3840|Â±  |0.0308|
| - bbh_cot_fewshot_ruin_names                             |      4|get-answer       |     3|exact_match|â†‘  | 0.4720|Â±  |0.0316|
| - bbh_cot_fewshot_salient_translation_error_detection    |      4|get-answer       |     3|exact_match|â†‘  | 0.4320|Â±  |0.0314|
| - bbh_cot_fewshot_snarks                                 |      4|get-answer       |     3|exact_match|â†‘  | 0.5056|Â±  |0.0376|
| - bbh_cot_fewshot_sports_understanding                   |      4|get-answer       |     3|exact_match|â†‘  | 0.9040|Â±  |0.0187|
| - bbh_cot_fewshot_temporal_sequences                     |      4|get-answer       |     3|exact_match|â†‘  | 0.1160|Â±  |0.0203|
| - bbh_cot_fewshot_tracking_shuffled_objects_five_objects |      4|get-answer       |     3|exact_match|â†‘  | 0.1680|Â±  |0.0237|
| - bbh_cot_fewshot_tracking_shuffled_objects_seven_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.1560|Â±  |0.0230|
| - bbh_cot_fewshot_tracking_shuffled_objects_three_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.3040|Â±  |0.0292|
| - bbh_cot_fewshot_web_of_lies                            |      4|get-answer       |     3|exact_match|â†‘  | 0.4840|Â±  |0.0317|
| - bbh_cot_fewshot_word_sorting                           |      4|get-answer       |     3|exact_match|â†‘  | 0.0640|Â±  |0.0155|
|boolq                                                     |      2|none             |     0|acc        |â†‘  | 0.7979|Â±  |0.0070|
|drop                                                      |      3|none             |     0|em         |â†‘  | 0.0358|Â±  |0.0019|
|                                                          |       |none             |     0|f1         |â†‘  | 0.1175|Â±  |0.0025|
|gpqa_diamond_cot_n_shot                                   |      2|flexible-extract |     0|exact_match|â†‘  | 0.1717|Â±  |0.0269|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_cot_zeroshot                                 |      1|flexible-extract |     0|exact_match|â†‘  | 0.1717|Â±  |0.0269|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_generative_n_shot                            |      2|flexible-extract |     0|exact_match|â†‘  | 0.2980|Â±  |0.0326|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_n_shot                                       |      2|none             |     0|acc        |â†‘  | 0.2929|Â±  |0.0324|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2929|Â±  |0.0324|
|gpqa_diamond_zeroshot                                     |      1|none             |     0|acc        |â†‘  | 0.2525|Â±  |0.0310|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2525|Â±  |0.0310|
|gpqa_extended_cot_n_shot                                  |      2|flexible-extract |     0|exact_match|â†‘  | 0.2216|Â±  |0.0178|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_cot_zeroshot                                |      1|flexible-extract |     0|exact_match|â†‘  | 0.2106|Â±  |0.0175|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_generative_n_shot                           |      2|flexible-extract |     0|exact_match|â†‘  | 0.2546|Â±  |0.0187|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_n_shot                                      |      2|none             |     0|acc        |â†‘  | 0.2637|Â±  |0.0189|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2637|Â±  |0.0189|
|gpqa_extended_zeroshot                                    |      1|none             |     0|acc        |â†‘  | 0.2692|Â±  |0.0190|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2692|Â±  |0.0190|
|gpqa_main_cot_n_shot                                      |      2|flexible-extract |     0|exact_match|â†‘  | 0.1629|Â±  |0.0175|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_cot_zeroshot                                    |      1|flexible-extract |     0|exact_match|â†‘  | 0.1786|Â±  |0.0181|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_generative_n_shot                               |      2|flexible-extract |     0|exact_match|â†‘  | 0.2522|Â±  |0.0205|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_n_shot                                          |      2|none             |     0|acc        |â†‘  | 0.2946|Â±  |0.0216|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2946|Â±  |0.0216|
|gpqa_main_zeroshot                                        |      1|none             |     0|acc        |â†‘  | 0.2612|Â±  |0.0208|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2612|Â±  |0.0208|
|gsm8k                                                     |      3|flexible-extract |     5|exact_match|â†‘  | 0.2320|Â±  |0.0116|
|                                                          |       |strict-match     |     5|exact_match|â†‘  | 0.2320|Â±  |0.0116|
|hellaswag                                                 |      1|none             |     0|acc        |â†‘  | 0.5779|Â±  |0.0049|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.7548|Â±  |0.0043|
|mmlu                                                      |      2|none             |      |acc        |â†‘  | 0.4636|Â±  |0.0040|
| - humanities                                             |      2|none             |      |acc        |â†‘  | 0.4332|Â±  |0.0069|
|  - formal_logic                                          |      1|none             |     0|acc        |â†‘  | 0.2381|Â±  |0.0381|
|  - high_school_european_history                          |      1|none             |     0|acc        |â†‘  | 0.5818|Â±  |0.0385|
|  - high_school_us_history                                |      1|none             |     0|acc        |â†‘  | 0.6618|Â±  |0.0332|
|  - high_school_world_history                             |      1|none             |     0|acc        |â†‘  | 0.6203|Â±  |0.0316|
|  - international_law                                     |      1|none             |     0|acc        |â†‘  | 0.5950|Â±  |0.0448|
|  - jurisprudence                                         |      1|none             |     0|acc        |â†‘  | 0.5741|Â±  |0.0478|
|  - logical_fallacies                                     |      1|none             |     0|acc        |â†‘  | 0.5767|Â±  |0.0388|
|  - moral_disputes                                        |      1|none             |     0|acc        |â†‘  | 0.5058|Â±  |0.0269|
|  - moral_scenarios                                       |      1|none             |     0|acc        |â†‘  | 0.2425|Â±  |0.0143|
|  - philosophy                                            |      1|none             |     0|acc        |â†‘  | 0.5273|Â±  |0.0284|
|  - prehistory                                            |      1|none             |     0|acc        |â†‘  | 0.5463|Â±  |0.0277|
|  - professional_law                                      |      1|none             |     0|acc        |â†‘  | 0.3592|Â±  |0.0123|
|  - world_religions                                       |      1|none             |     0|acc        |â†‘  | 0.6901|Â±  |0.0355|
| - other                                                  |      2|none             |      |acc        |â†‘  | 0.5488|Â±  |0.0086|
|  - business_ethics                                       |      1|none             |     0|acc        |â†‘  | 0.4500|Â±  |0.0500|
|  - clinical_knowledge                                    |      1|none             |     0|acc        |â†‘  | 0.5509|Â±  |0.0306|
|  - college_medicine                                      |      1|none             |     0|acc        |â†‘  | 0.3815|Â±  |0.0370|
|  - global_facts                                          |      1|none             |     0|acc        |â†‘  | 0.4000|Â±  |0.0492|
|  - human_aging                                           |      1|none             |     0|acc        |â†‘  | 0.5830|Â±  |0.0331|
|  - management                                            |      1|none             |     0|acc        |â†‘  | 0.6796|Â±  |0.0462|
|  - marketing                                             |      1|none             |     0|acc        |â†‘  | 0.7564|Â±  |0.0281|
|  - medical_genetics                                      |      1|none             |     0|acc        |â†‘  | 0.4800|Â±  |0.0502|
|  - miscellaneous                                         |      1|none             |     0|acc        |â†‘  | 0.6897|Â±  |0.0165|
|  - nutrition                                             |      1|none             |     0|acc        |â†‘  | 0.4902|Â±  |0.0286|
|  - professional_accounting                               |      1|none             |     0|acc        |â†‘  | 0.3652|Â±  |0.0287|
|  - professional_medicine                                 |      1|none             |     0|acc        |â†‘  | 0.4154|Â±  |0.0299|
|  - virology                                              |      1|none             |     0|acc        |â†‘  | 0.4639|Â±  |0.0388|
| - social sciences                                        |      2|none             |      |acc        |â†‘  | 0.5304|Â±  |0.0087|
|  - econometrics                                          |      1|none             |     0|acc        |â†‘  | 0.2982|Â±  |0.0430|
|  - high_school_geography                                 |      1|none             |     0|acc        |â†‘  | 0.5909|Â±  |0.0350|
|  - high_school_government_and_politics                   |      1|none             |     0|acc        |â†‘  | 0.6839|Â±  |0.0336|
|  - high_school_macroeconomics                            |      1|none             |     0|acc        |â†‘  | 0.4000|Â±  |0.0248|
|  - high_school_microeconomics                            |      1|none             |     0|acc        |â†‘  | 0.3613|Â±  |0.0312|
|  - high_school_psychology                                |      1|none             |     0|acc        |â†‘  | 0.6349|Â±  |0.0206|
|  - human_sexuality                                       |      1|none             |     0|acc        |â†‘  | 0.5649|Â±  |0.0435|
|  - professional_psychology                               |      1|none             |     0|acc        |â†‘  | 0.4673|Â±  |0.0202|
|  - public_relations                                      |      1|none             |     0|acc        |â†‘  | 0.5364|Â±  |0.0478|
|  - security_studies                                      |      1|none             |     0|acc        |â†‘  | 0.4980|Â±  |0.0320|
|  - sociology                                             |      1|none             |     0|acc        |â†‘  | 0.7413|Â±  |0.0310|
|  - us_foreign_policy                                     |      1|none             |     0|acc        |â†‘  | 0.7100|Â±  |0.0456|
| - stem                                                   |      2|none             |      |acc        |â†‘  | 0.3600|Â±  |0.0084|
|  - abstract_algebra                                      |      1|none             |     0|acc        |â†‘  | 0.3000|Â±  |0.0461|
|  - anatomy                                               |      1|none             |     0|acc        |â†‘  | 0.4444|Â±  |0.0429|
|  - astronomy                                             |      1|none             |     0|acc        |â†‘  | 0.4934|Â±  |0.0407|
|  - college_biology                                       |      1|none             |     0|acc        |â†‘  | 0.4514|Â±  |0.0416|
|  - college_chemistry                                     |      1|none             |     0|acc        |â†‘  | 0.2500|Â±  |0.0435|
|  - college_computer_science                              |      1|none             |     0|acc        |â†‘  | 0.3200|Â±  |0.0469|
|  - college_mathematics                                   |      1|none             |     0|acc        |â†‘  | 0.3100|Â±  |0.0465|
|  - college_physics                                       |      1|none             |     0|acc        |â†‘  | 0.1961|Â±  |0.0395|
|  - computer_security                                     |      1|none             |     0|acc        |â†‘  | 0.6000|Â±  |0.0492|
|  - conceptual_physics                                    |      1|none             |     0|acc        |â†‘  | 0.4000|Â±  |0.0320|
|  - electrical_engineering                                |      1|none             |     0|acc        |â†‘  | 0.4483|Â±  |0.0414|
|  - elementary_mathematics                                |      1|none             |     0|acc        |â†‘  | 0.2751|Â±  |0.0230|
|  - high_school_biology                                   |      1|none             |     0|acc        |â†‘  | 0.4935|Â±  |0.0284|
|  - high_school_chemistry                                 |      1|none             |     0|acc        |â†‘  | 0.3350|Â±  |0.0332|
|  - high_school_computer_science                          |      1|none             |     0|acc        |â†‘  | 0.4000|Â±  |0.0492|
|  - high_school_mathematics                               |      1|none             |     0|acc        |â†‘  | 0.2704|Â±  |0.0271|
|  - high_school_physics                                   |      1|none             |     0|acc        |â†‘  | 0.2781|Â±  |0.0366|
|  - high_school_statistics                                |      1|none             |     0|acc        |â†‘  | 0.2685|Â±  |0.0302|
|  - machine_learning                                      |      1|none             |     0|acc        |â†‘  | 0.3571|Â±  |0.0455|
|nq_open                                                   |      4|remove_whitespace|     0|exact_match|â†‘  | 0.0668|Â±  |0.0042|
|openbookqa                                                |      1|none             |     0|acc        |â†‘  | 0.3340|Â±  |0.0211|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.4380|Â±  |0.0222|
|piqa                                                      |      1|none             |     0|acc        |â†‘  | 0.7644|Â±  |0.0099|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.7715|Â±  |0.0098|
|qnli                                                      |      1|none             |     0|acc        |â†‘  | 0.5801|Â±  |0.0067|
|sciq                                                      |      1|none             |     0|acc        |â†‘  | 0.9400|Â±  |0.0075|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.8780|Â±  |0.0104|
|triviaqa                                                  |      3|remove_whitespace|     0|exact_match|â†‘  | 0.1904|Â±  |0.0029|
|truthfulqa_gen                                            |      3|none             |     0|bleu_acc   |â†‘  | 0.4517|Â±  |0.0174|
|                                                          |       |none             |     0|bleu_diff  |â†‘  |-1.6714|Â±  |0.6136|
|                                                          |       |none             |     0|bleu_max   |â†‘  |20.5268|Â±  |0.7001|
|                                                          |       |none             |     0|rouge1_acc |â†‘  | 0.4468|Â±  |0.0174|
|                                                          |       |none             |     0|rouge1_diff|â†‘  |-1.6626|Â±  |0.7580|
|                                                          |       |none             |     0|rouge1_max |â†‘  |45.4458|Â±  |0.8003|
|                                                          |       |none             |     0|rouge2_acc |â†‘  | 0.3807|Â±  |0.0170|
|                                                          |       |none             |     0|rouge2_diff|â†‘  |-3.1513|Â±  |0.8822|
|                                                          |       |none             |     0|rouge2_max |â†‘  |30.2564|Â±  |0.8906|
|                                                          |       |none             |     0|rougeL_acc |â†‘  | 0.4480|Â±  |0.0174|
|                                                          |       |none             |     0|rougeL_diff|â†‘  |-1.9429|Â±  |0.7563|
|                                                          |       |none             |     0|rougeL_max |â†‘  |42.1653|Â±  |0.8032|
|truthfulqa_mc1                                            |      2|none             |     0|acc        |â†‘  | 0.3023|Â±  |0.0161|
|truthfulqa_mc2                                            |      3|none             |     0|acc        |â†‘  | 0.4532|Â±  |0.0156|
|winogrande                                                |      1|none             |     0|acc        |â†‘  | 0.6646|Â±  |0.0133|

|      Groups      |Version|  Filter  |n-shot|  Metric   |   |Value |   |Stderr|
|------------------|------:|----------|------|-----------|---|-----:|---|-----:|
|bbh               |      3|get-answer|      |exact_match|â†‘  |0.4013|Â±  |0.0055|
|mmlu              |      2|none      |      |acc        |â†‘  |0.4636|Â±  |0.0040|
| - humanities     |      2|none      |      |acc        |â†‘  |0.4332|Â±  |0.0069|
| - other          |      2|none      |      |acc        |â†‘  |0.5488|Â±  |0.0086|
| - social sciences|      2|none      |      |acc        |â†‘  |0.5304|Â±  |0.0087|
| - stem           |      2|none      |      |acc        |â†‘  |0.3600|Â±  |0.0084|

meta-llama_Llama-2-7b-chat-hf: 6h 58m 9s
âœ… Benchmark completed for meta-llama_Llama-2-7b-chat-hf

ðŸ”¥ Starting benchmark for mistralai_Mistral-Nemo-Instruct-2407
Passed argument batch_size = auto:1. Detecting largest batch size
Determined largest batch size: 1
mistralai_Mistral-Nemo-Instruct-2407: 0h 7m 53s
âœ… Benchmark completed for mistralai_Mistral-Nemo-Instruct-2407

ðŸ”¥ Starting benchmark for mistralai_Ministral-8B-Instruct-2410
Passed argument batch_size = auto:1. Detecting largest batch size
Determined largest batch size: 1
Passed argument batch_size = auto. Detecting largest batch size
Determined Largest batch size: 1
hf (pretrained=/home/jaymin/Documents/llm/llm_models/mistralai_Ministral-8B-Instruct-2410), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto (1)
|                          Tasks                           |Version|     Filter      |n-shot|  Metric   |   | Value |   |Stderr|
|----------------------------------------------------------|------:|-----------------|-----:|-----------|---|------:|---|-----:|
|anli_r1                                                   |      1|none             |     0|acc        |â†‘  | 0.4880|Â±  |0.0158|
|anli_r2                                                   |      1|none             |     0|acc        |â†‘  | 0.4870|Â±  |0.0158|
|anli_r3                                                   |      1|none             |     0|acc        |â†‘  | 0.4658|Â±  |0.0144|
|arc_challenge                                             |      1|none             |     0|acc        |â†‘  | 0.5452|Â±  |0.0146|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.5623|Â±  |0.0145|
|bbh                                                       |      3|get-answer       |      |exact_match|â†‘  | 0.6925|Â±  |0.0051|
| - bbh_cot_fewshot_boolean_expressions                    |      4|get-answer       |     3|exact_match|â†‘  | 0.8920|Â±  |0.0197|
| - bbh_cot_fewshot_causal_judgement                       |      4|get-answer       |     3|exact_match|â†‘  | 0.6096|Â±  |0.0358|
| - bbh_cot_fewshot_date_understanding                     |      4|get-answer       |     3|exact_match|â†‘  | 0.8200|Â±  |0.0243|
| - bbh_cot_fewshot_disambiguation_qa                      |      4|get-answer       |     3|exact_match|â†‘  | 0.6000|Â±  |0.0310|
| - bbh_cot_fewshot_dyck_languages                         |      4|get-answer       |     3|exact_match|â†‘  | 0.0560|Â±  |0.0146|
| - bbh_cot_fewshot_formal_fallacies                       |      4|get-answer       |     3|exact_match|â†‘  | 0.5400|Â±  |0.0316|
| - bbh_cot_fewshot_geometric_shapes                       |      4|get-answer       |     3|exact_match|â†‘  | 0.6240|Â±  |0.0307|
| - bbh_cot_fewshot_hyperbaton                             |      4|get-answer       |     3|exact_match|â†‘  | 0.9600|Â±  |0.0124|
| - bbh_cot_fewshot_logical_deduction_five_objects         |      4|get-answer       |     3|exact_match|â†‘  | 0.5800|Â±  |0.0313|
| - bbh_cot_fewshot_logical_deduction_seven_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.4280|Â±  |0.0314|
| - bbh_cot_fewshot_logical_deduction_three_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.9080|Â±  |0.0183|
| - bbh_cot_fewshot_movie_recommendation                   |      4|get-answer       |     3|exact_match|â†‘  | 0.7200|Â±  |0.0285|
| - bbh_cot_fewshot_multistep_arithmetic_two               |      4|get-answer       |     3|exact_match|â†‘  | 0.6360|Â±  |0.0305|
| - bbh_cot_fewshot_navigate                               |      4|get-answer       |     3|exact_match|â†‘  | 0.8160|Â±  |0.0246|
| - bbh_cot_fewshot_object_counting                        |      4|get-answer       |     3|exact_match|â†‘  | 0.9000|Â±  |0.0190|
| - bbh_cot_fewshot_penguins_in_a_table                    |      4|get-answer       |     3|exact_match|â†‘  | 0.7260|Â±  |0.0370|
| - bbh_cot_fewshot_reasoning_about_colored_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.8000|Â±  |0.0253|
| - bbh_cot_fewshot_ruin_names                             |      4|get-answer       |     3|exact_match|â†‘  | 0.6400|Â±  |0.0304|
| - bbh_cot_fewshot_salient_translation_error_detection    |      4|get-answer       |     3|exact_match|â†‘  | 0.4800|Â±  |0.0317|
| - bbh_cot_fewshot_snarks                                 |      4|get-answer       |     3|exact_match|â†‘  | 0.6517|Â±  |0.0358|
| - bbh_cot_fewshot_sports_understanding                   |      4|get-answer       |     3|exact_match|â†‘  | 0.9240|Â±  |0.0168|
| - bbh_cot_fewshot_temporal_sequences                     |      4|get-answer       |     3|exact_match|â†‘  | 0.5760|Â±  |0.0313|
| - bbh_cot_fewshot_tracking_shuffled_objects_five_objects |      4|get-answer       |     3|exact_match|â†‘  | 0.8000|Â±  |0.0253|
| - bbh_cot_fewshot_tracking_shuffled_objects_seven_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.7720|Â±  |0.0266|
| - bbh_cot_fewshot_tracking_shuffled_objects_three_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.8000|Â±  |0.0253|
| - bbh_cot_fewshot_web_of_lies                            |      4|get-answer       |     3|exact_match|â†‘  | 1.0000|Â±  |0.0000|
| - bbh_cot_fewshot_word_sorting                           |      4|get-answer       |     3|exact_match|â†‘  | 0.4200|Â±  |0.0313|
|boolq                                                     |      2|none             |     0|acc        |â†‘  | 0.8602|Â±  |0.0061|
|drop                                                      |      3|none             |     0|em         |â†‘  | 0.0229|Â±  |0.0015|
|                                                          |       |none             |     0|f1         |â†‘  | 0.0714|Â±  |0.0021|
|gpqa_diamond_cot_n_shot                                   |      2|flexible-extract |     0|exact_match|â†‘  | 0.1566|Â±  |0.0259|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_cot_zeroshot                                 |      1|flexible-extract |     0|exact_match|â†‘  | 0.2121|Â±  |0.0291|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_generative_n_shot                            |      2|flexible-extract |     0|exact_match|â†‘  | 0.2828|Â±  |0.0321|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_n_shot                                       |      2|none             |     0|acc        |â†‘  | 0.3081|Â±  |0.0329|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3081|Â±  |0.0329|
|gpqa_diamond_zeroshot                                     |      1|none             |     0|acc        |â†‘  | 0.3384|Â±  |0.0337|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3384|Â±  |0.0337|
|gpqa_extended_cot_n_shot                                  |      2|flexible-extract |     0|exact_match|â†‘  | 0.1978|Â±  |0.0171|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0018|Â±  |0.0018|
|gpqa_extended_cot_zeroshot                                |      1|flexible-extract |     0|exact_match|â†‘  | 0.2546|Â±  |0.0187|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_generative_n_shot                           |      2|flexible-extract |     0|exact_match|â†‘  | 0.3095|Â±  |0.0198|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_n_shot                                      |      2|none             |     0|acc        |â†‘  | 0.3645|Â±  |0.0206|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3645|Â±  |0.0206|
|gpqa_extended_zeroshot                                    |      1|none             |     0|acc        |â†‘  | 0.2985|Â±  |0.0196|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2985|Â±  |0.0196|
|gpqa_main_cot_n_shot                                      |      2|flexible-extract |     0|exact_match|â†‘  | 0.2299|Â±  |0.0199|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0022|Â±  |0.0022|
|gpqa_main_cot_zeroshot                                    |      1|flexible-extract |     0|exact_match|â†‘  | 0.2254|Â±  |0.0198|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_generative_n_shot                               |      2|flexible-extract |     0|exact_match|â†‘  | 0.3058|Â±  |0.0218|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_n_shot                                          |      2|none             |     0|acc        |â†‘  | 0.3125|Â±  |0.0219|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3125|Â±  |0.0219|
|gpqa_main_zeroshot                                        |      1|none             |     0|acc        |â†‘  | 0.3415|Â±  |0.0224|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3415|Â±  |0.0224|
|gsm8k                                                     |      3|flexible-extract |     5|exact_match|â†‘  | 0.7786|Â±  |0.0114|
|                                                          |       |strict-match     |     5|exact_match|â†‘  | 0.7748|Â±  |0.0115|
|hellaswag                                                 |      1|none             |     0|acc        |â†‘  | 0.5959|Â±  |0.0049|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.7911|Â±  |0.0041|
|mmlu                                                      |      2|none             |      |acc        |â†‘  | 0.6407|Â±  |0.0038|
| - humanities                                             |      2|none             |      |acc        |â†‘  | 0.5792|Â±  |0.0068|
|  - formal_logic                                          |      1|none             |     0|acc        |â†‘  | 0.4365|Â±  |0.0444|
|  - high_school_european_history                          |      1|none             |     0|acc        |â†‘  | 0.7515|Â±  |0.0337|
|  - high_school_us_history                                |      1|none             |     0|acc        |â†‘  | 0.8480|Â±  |0.0252|
|  - high_school_world_history                             |      1|none             |     0|acc        |â†‘  | 0.8186|Â±  |0.0251|
|  - international_law                                     |      1|none             |     0|acc        |â†‘  | 0.7851|Â±  |0.0375|
|  - jurisprudence                                         |      1|none             |     0|acc        |â†‘  | 0.7870|Â±  |0.0396|
|  - logical_fallacies                                     |      1|none             |     0|acc        |â†‘  | 0.7546|Â±  |0.0338|
|  - moral_disputes                                        |      1|none             |     0|acc        |â†‘  | 0.6792|Â±  |0.0251|
|  - moral_scenarios                                       |      1|none             |     0|acc        |â†‘  | 0.3453|Â±  |0.0159|
|  - philosophy                                            |      1|none             |     0|acc        |â†‘  | 0.7042|Â±  |0.0259|
|  - prehistory                                            |      1|none             |     0|acc        |â†‘  | 0.6821|Â±  |0.0259|
|  - professional_law                                      |      1|none             |     0|acc        |â†‘  | 0.4922|Â±  |0.0128|
|  - world_religions                                       |      1|none             |     0|acc        |â†‘  | 0.8012|Â±  |0.0306|
| - other                                                  |      2|none             |      |acc        |â†‘  | 0.7123|Â±  |0.0079|
|  - business_ethics                                       |      1|none             |     0|acc        |â†‘  | 0.6200|Â±  |0.0488|
|  - clinical_knowledge                                    |      1|none             |     0|acc        |â†‘  | 0.6792|Â±  |0.0287|
|  - college_medicine                                      |      1|none             |     0|acc        |â†‘  | 0.6590|Â±  |0.0361|
|  - global_facts                                          |      1|none             |     0|acc        |â†‘  | 0.4400|Â±  |0.0499|
|  - human_aging                                           |      1|none             |     0|acc        |â†‘  | 0.6906|Â±  |0.0310|
|  - management                                            |      1|none             |     0|acc        |â†‘  | 0.7767|Â±  |0.0412|
|  - marketing                                             |      1|none             |     0|acc        |â†‘  | 0.8675|Â±  |0.0222|
|  - medical_genetics                                      |      1|none             |     0|acc        |â†‘  | 0.7100|Â±  |0.0456|
|  - miscellaneous                                         |      1|none             |     0|acc        |â†‘  | 0.8186|Â±  |0.0138|
|  - nutrition                                             |      1|none             |     0|acc        |â†‘  | 0.7549|Â±  |0.0246|
|  - professional_accounting                               |      1|none             |     0|acc        |â†‘  | 0.5142|Â±  |0.0298|
|  - professional_medicine                                 |      1|none             |     0|acc        |â†‘  | 0.7206|Â±  |0.0273|
|  - virology                                              |      1|none             |     0|acc        |â†‘  | 0.5542|Â±  |0.0387|
| - social sciences                                        |      2|none             |      |acc        |â†‘  | 0.7439|Â±  |0.0077|
|  - econometrics                                          |      1|none             |     0|acc        |â†‘  | 0.4386|Â±  |0.0467|
|  - high_school_geography                                 |      1|none             |     0|acc        |â†‘  | 0.7980|Â±  |0.0286|
|  - high_school_government_and_politics                   |      1|none             |     0|acc        |â†‘  | 0.8860|Â±  |0.0229|
|  - high_school_macroeconomics                            |      1|none             |     0|acc        |â†‘  | 0.6641|Â±  |0.0239|
|  - high_school_microeconomics                            |      1|none             |     0|acc        |â†‘  | 0.7017|Â±  |0.0297|
|  - high_school_psychology                                |      1|none             |     0|acc        |â†‘  | 0.8532|Â±  |0.0152|
|  - human_sexuality                                       |      1|none             |     0|acc        |â†‘  | 0.7481|Â±  |0.0381|
|  - professional_psychology                               |      1|none             |     0|acc        |â†‘  | 0.6667|Â±  |0.0191|
|  - public_relations                                      |      1|none             |     0|acc        |â†‘  | 0.6818|Â±  |0.0446|
|  - security_studies                                      |      1|none             |     0|acc        |â†‘  | 0.7469|Â±  |0.0278|
|  - sociology                                             |      1|none             |     0|acc        |â†‘  | 0.8358|Â±  |0.0262|
|  - us_foreign_policy                                     |      1|none             |     0|acc        |â†‘  | 0.8700|Â±  |0.0338|
| - stem                                                   |      2|none             |      |acc        |â†‘  | 0.5614|Â±  |0.0085|
|  - abstract_algebra                                      |      1|none             |     0|acc        |â†‘  | 0.3500|Â±  |0.0479|
|  - anatomy                                               |      1|none             |     0|acc        |â†‘  | 0.6741|Â±  |0.0405|
|  - astronomy                                             |      1|none             |     0|acc        |â†‘  | 0.7237|Â±  |0.0364|
|  - college_biology                                       |      1|none             |     0|acc        |â†‘  | 0.7708|Â±  |0.0351|
|  - college_chemistry                                     |      1|none             |     0|acc        |â†‘  | 0.4600|Â±  |0.0501|
|  - college_computer_science                              |      1|none             |     0|acc        |â†‘  | 0.5900|Â±  |0.0494|
|  - college_mathematics                                   |      1|none             |     0|acc        |â†‘  | 0.3700|Â±  |0.0485|
|  - college_physics                                       |      1|none             |     0|acc        |â†‘  | 0.3725|Â±  |0.0481|
|  - computer_security                                     |      1|none             |     0|acc        |â†‘  | 0.7000|Â±  |0.0461|
|  - conceptual_physics                                    |      1|none             |     0|acc        |â†‘  | 0.5745|Â±  |0.0323|
|  - electrical_engineering                                |      1|none             |     0|acc        |â†‘  | 0.5517|Â±  |0.0414|
|  - elementary_mathematics                                |      1|none             |     0|acc        |â†‘  | 0.4577|Â±  |0.0257|
|  - high_school_biology                                   |      1|none             |     0|acc        |â†‘  | 0.7968|Â±  |0.0229|
|  - high_school_chemistry                                 |      1|none             |     0|acc        |â†‘  | 0.5320|Â±  |0.0351|
|  - high_school_computer_science                          |      1|none             |     0|acc        |â†‘  | 0.7200|Â±  |0.0451|
|  - high_school_mathematics                               |      1|none             |     0|acc        |â†‘  | 0.3889|Â±  |0.0297|
|  - high_school_physics                                   |      1|none             |     0|acc        |â†‘  | 0.4371|Â±  |0.0405|
|  - high_school_statistics                                |      1|none             |     0|acc        |â†‘  | 0.5972|Â±  |0.0334|
|  - machine_learning                                      |      1|none             |     0|acc        |â†‘  | 0.5179|Â±  |0.0474|
|nq_open                                                   |      4|remove_whitespace|     0|exact_match|â†‘  | 0.1576|Â±  |0.0061|
|openbookqa                                                |      1|none             |     0|acc        |â†‘  | 0.3640|Â±  |0.0215|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.4660|Â±  |0.0223|
|piqa                                                      |      1|none             |     0|acc        |â†‘  | 0.8096|Â±  |0.0092|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.8232|Â±  |0.0089|
|qnli                                                      |      1|none             |     0|acc        |â†‘  | 0.4950|Â±  |0.0068|
|sciq                                                      |      1|none             |     0|acc        |â†‘  | 0.9680|Â±  |0.0056|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.9560|Â±  |0.0065|
|triviaqa                                                  |      3|remove_whitespace|     0|exact_match|â†‘  | 0.5278|Â±  |0.0037|
|truthfulqa_gen                                            |      3|none             |     0|bleu_acc   |â†‘  | 0.0734|Â±  |0.0091|
|                                                          |       |none             |     0|bleu_diff  |â†‘  |-1.3264|Â±  |0.3132|
|                                                          |       |none             |     0|bleu_max   |â†‘  | 4.1401|Â±  |0.4048|
|                                                          |       |none             |     0|rouge1_acc |â†‘  | 0.0759|Â±  |0.0093|
|                                                          |       |none             |     0|rouge1_diff|â†‘  |-2.7245|Â±  |0.4932|
|                                                          |       |none             |     0|rouge1_max |â†‘  | 9.7318|Â±  |0.7309|
|                                                          |       |none             |     0|rouge2_acc |â†‘  | 0.0661|Â±  |0.0087|
|                                                          |       |none             |     0|rouge2_diff|â†‘  |-2.5882|Â±  |0.5097|
|                                                          |       |none             |     0|rouge2_max |â†‘  | 6.3193|Â±  |0.5845|
|                                                          |       |none             |     0|rougeL_acc |â†‘  | 0.0722|Â±  |0.0091|
|                                                          |       |none             |     0|rougeL_diff|â†‘  |-2.8073|Â±  |0.4921|
|                                                          |       |none             |     0|rougeL_max |â†‘  | 9.1139|Â±  |0.6993|
|truthfulqa_mc1                                            |      2|none             |     0|acc        |â†‘  | 0.3256|Â±  |0.0164|
|truthfulqa_mc2                                            |      3|none             |     0|acc        |â†‘  | 0.4867|Â±  |0.0147|
|winogrande                                                |      1|none             |     0|acc        |â†‘  | 0.7380|Â±  |0.0124|

|      Groups      |Version|  Filter  |n-shot|  Metric   |   |Value |   |Stderr|
|------------------|------:|----------|------|-----------|---|-----:|---|-----:|
|bbh               |      3|get-answer|      |exact_match|â†‘  |0.6925|Â±  |0.0051|
|mmlu              |      2|none      |      |acc        |â†‘  |0.6407|Â±  |0.0038|
| - humanities     |      2|none      |      |acc        |â†‘  |0.5792|Â±  |0.0068|
| - other          |      2|none      |      |acc        |â†‘  |0.7123|Â±  |0.0079|
| - social sciences|      2|none      |      |acc        |â†‘  |0.7439|Â±  |0.0077|
| - stem           |      2|none      |      |acc        |â†‘  |0.5614|Â±  |0.0085|

mistralai_Ministral-8B-Instruct-2410: 10h 46m 2s
âœ… Benchmark completed for mistralai_Ministral-8B-Instruct-2410

ðŸ”¥ Starting benchmark for mistralai_Ministral-8B-Instruct-2410
Passed argument batch_size = auto:1. Detecting largest batch size
Determined largest batch size: 1
Passed argument batch_size = auto. Detecting largest batch size
Determined Largest batch size: 1
hf (pretrained=/home/jaymin/Documents/llm/llm_models/mistralai_Ministral-8B-Instruct-2410), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto (1)
|                          Tasks                           |Version|     Filter      |n-shot|  Metric   |   | Value |   |Stderr|
|----------------------------------------------------------|------:|-----------------|-----:|-----------|---|------:|---|-----:|
|anli_r1                                                   |      1|none             |     0|acc        |â†‘  | 0.4880|Â±  |0.0158|
|anli_r2                                                   |      1|none             |     0|acc        |â†‘  | 0.4870|Â±  |0.0158|
|anli_r3                                                   |      1|none             |     0|acc        |â†‘  | 0.4658|Â±  |0.0144|
|arc_challenge                                             |      1|none             |     0|acc        |â†‘  | 0.5452|Â±  |0.0146|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.5623|Â±  |0.0145|
|bbh                                                       |      3|get-answer       |      |exact_match|â†‘  | 0.6925|Â±  |0.0051|
| - bbh_cot_fewshot_boolean_expressions                    |      4|get-answer       |     3|exact_match|â†‘  | 0.8920|Â±  |0.0197|
| - bbh_cot_fewshot_causal_judgement                       |      4|get-answer       |     3|exact_match|â†‘  | 0.6096|Â±  |0.0358|
| - bbh_cot_fewshot_date_understanding                     |      4|get-answer       |     3|exact_match|â†‘  | 0.8200|Â±  |0.0243|
| - bbh_cot_fewshot_disambiguation_qa                      |      4|get-answer       |     3|exact_match|â†‘  | 0.6000|Â±  |0.0310|
| - bbh_cot_fewshot_dyck_languages                         |      4|get-answer       |     3|exact_match|â†‘  | 0.0560|Â±  |0.0146|
| - bbh_cot_fewshot_formal_fallacies                       |      4|get-answer       |     3|exact_match|â†‘  | 0.5400|Â±  |0.0316|
| - bbh_cot_fewshot_geometric_shapes                       |      4|get-answer       |     3|exact_match|â†‘  | 0.6240|Â±  |0.0307|
| - bbh_cot_fewshot_hyperbaton                             |      4|get-answer       |     3|exact_match|â†‘  | 0.9600|Â±  |0.0124|
| - bbh_cot_fewshot_logical_deduction_five_objects         |      4|get-answer       |     3|exact_match|â†‘  | 0.5800|Â±  |0.0313|
| - bbh_cot_fewshot_logical_deduction_seven_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.4280|Â±  |0.0314|
| - bbh_cot_fewshot_logical_deduction_three_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.9080|Â±  |0.0183|
| - bbh_cot_fewshot_movie_recommendation                   |      4|get-answer       |     3|exact_match|â†‘  | 0.7200|Â±  |0.0285|
| - bbh_cot_fewshot_multistep_arithmetic_two               |      4|get-answer       |     3|exact_match|â†‘  | 0.6360|Â±  |0.0305|
| - bbh_cot_fewshot_navigate                               |      4|get-answer       |     3|exact_match|â†‘  | 0.8160|Â±  |0.0246|
| - bbh_cot_fewshot_object_counting                        |      4|get-answer       |     3|exact_match|â†‘  | 0.9000|Â±  |0.0190|
| - bbh_cot_fewshot_penguins_in_a_table                    |      4|get-answer       |     3|exact_match|â†‘  | 0.7260|Â±  |0.0370|
| - bbh_cot_fewshot_reasoning_about_colored_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.8000|Â±  |0.0253|
| - bbh_cot_fewshot_ruin_names                             |      4|get-answer       |     3|exact_match|â†‘  | 0.6400|Â±  |0.0304|
| - bbh_cot_fewshot_salient_translation_error_detection    |      4|get-answer       |     3|exact_match|â†‘  | 0.4800|Â±  |0.0317|
| - bbh_cot_fewshot_snarks                                 |      4|get-answer       |     3|exact_match|â†‘  | 0.6517|Â±  |0.0358|
| - bbh_cot_fewshot_sports_understanding                   |      4|get-answer       |     3|exact_match|â†‘  | 0.9240|Â±  |0.0168|
| - bbh_cot_fewshot_temporal_sequences                     |      4|get-answer       |     3|exact_match|â†‘  | 0.5760|Â±  |0.0313|
| - bbh_cot_fewshot_tracking_shuffled_objects_five_objects |      4|get-answer       |     3|exact_match|â†‘  | 0.8000|Â±  |0.0253|
| - bbh_cot_fewshot_tracking_shuffled_objects_seven_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.7720|Â±  |0.0266|
| - bbh_cot_fewshot_tracking_shuffled_objects_three_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.8000|Â±  |0.0253|
| - bbh_cot_fewshot_web_of_lies                            |      4|get-answer       |     3|exact_match|â†‘  | 1.0000|Â±  |0.0000|
| - bbh_cot_fewshot_word_sorting                           |      4|get-answer       |     3|exact_match|â†‘  | 0.4200|Â±  |0.0313|
|boolq                                                     |      2|none             |     0|acc        |â†‘  | 0.8602|Â±  |0.0061|
|drop                                                      |      3|none             |     0|em         |â†‘  | 0.0229|Â±  |0.0015|
|                                                          |       |none             |     0|f1         |â†‘  | 0.0714|Â±  |0.0021|
|gpqa_diamond_cot_n_shot                                   |      2|flexible-extract |     0|exact_match|â†‘  | 0.1566|Â±  |0.0259|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_cot_zeroshot                                 |      1|flexible-extract |     0|exact_match|â†‘  | 0.2121|Â±  |0.0291|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_generative_n_shot                            |      2|flexible-extract |     0|exact_match|â†‘  | 0.2828|Â±  |0.0321|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_n_shot                                       |      2|none             |     0|acc        |â†‘  | 0.3081|Â±  |0.0329|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3081|Â±  |0.0329|
|gpqa_diamond_zeroshot                                     |      1|none             |     0|acc        |â†‘  | 0.3384|Â±  |0.0337|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3384|Â±  |0.0337|
|gpqa_extended_cot_n_shot                                  |      2|flexible-extract |     0|exact_match|â†‘  | 0.1978|Â±  |0.0171|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0018|Â±  |0.0018|
|gpqa_extended_cot_zeroshot                                |      1|flexible-extract |     0|exact_match|â†‘  | 0.2546|Â±  |0.0187|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_generative_n_shot                           |      2|flexible-extract |     0|exact_match|â†‘  | 0.3095|Â±  |0.0198|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_n_shot                                      |      2|none             |     0|acc        |â†‘  | 0.3645|Â±  |0.0206|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3645|Â±  |0.0206|
|gpqa_extended_zeroshot                                    |      1|none             |     0|acc        |â†‘  | 0.2985|Â±  |0.0196|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2985|Â±  |0.0196|
|gpqa_main_cot_n_shot                                      |      2|flexible-extract |     0|exact_match|â†‘  | 0.2299|Â±  |0.0199|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0022|Â±  |0.0022|
|gpqa_main_cot_zeroshot                                    |      1|flexible-extract |     0|exact_match|â†‘  | 0.2254|Â±  |0.0198|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_generative_n_shot                               |      2|flexible-extract |     0|exact_match|â†‘  | 0.3058|Â±  |0.0218|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_n_shot                                          |      2|none             |     0|acc        |â†‘  | 0.3125|Â±  |0.0219|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3125|Â±  |0.0219|
|gpqa_main_zeroshot                                        |      1|none             |     0|acc        |â†‘  | 0.3415|Â±  |0.0224|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3415|Â±  |0.0224|
|gsm8k                                                     |      3|flexible-extract |     5|exact_match|â†‘  | 0.7786|Â±  |0.0114|
|                                                          |       |strict-match     |     5|exact_match|â†‘  | 0.7748|Â±  |0.0115|
|hellaswag                                                 |      1|none             |     0|acc        |â†‘  | 0.5959|Â±  |0.0049|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.7911|Â±  |0.0041|
|mmlu                                                      |      2|none             |      |acc        |â†‘  | 0.6407|Â±  |0.0038|
| - humanities                                             |      2|none             |      |acc        |â†‘  | 0.5792|Â±  |0.0068|
|  - formal_logic                                          |      1|none             |     0|acc        |â†‘  | 0.4365|Â±  |0.0444|
|  - high_school_european_history                          |      1|none             |     0|acc        |â†‘  | 0.7515|Â±  |0.0337|
|  - high_school_us_history                                |      1|none             |     0|acc        |â†‘  | 0.8480|Â±  |0.0252|
|  - high_school_world_history                             |      1|none             |     0|acc        |â†‘  | 0.8186|Â±  |0.0251|
|  - international_law                                     |      1|none             |     0|acc        |â†‘  | 0.7851|Â±  |0.0375|
|  - jurisprudence                                         |      1|none             |     0|acc        |â†‘  | 0.7870|Â±  |0.0396|
|  - logical_fallacies                                     |      1|none             |     0|acc        |â†‘  | 0.7546|Â±  |0.0338|
|  - moral_disputes                                        |      1|none             |     0|acc        |â†‘  | 0.6792|Â±  |0.0251|
|  - moral_scenarios                                       |      1|none             |     0|acc        |â†‘  | 0.3453|Â±  |0.0159|
|  - philosophy                                            |      1|none             |     0|acc        |â†‘  | 0.7042|Â±  |0.0259|
|  - prehistory                                            |      1|none             |     0|acc        |â†‘  | 0.6821|Â±  |0.0259|
|  - professional_law                                      |      1|none             |     0|acc        |â†‘  | 0.4922|Â±  |0.0128|
|  - world_religions                                       |      1|none             |     0|acc        |â†‘  | 0.8012|Â±  |0.0306|
| - other                                                  |      2|none             |      |acc        |â†‘  | 0.7123|Â±  |0.0079|
|  - business_ethics                                       |      1|none             |     0|acc        |â†‘  | 0.6200|Â±  |0.0488|
|  - clinical_knowledge                                    |      1|none             |     0|acc        |â†‘  | 0.6792|Â±  |0.0287|
|  - college_medicine                                      |      1|none             |     0|acc        |â†‘  | 0.6590|Â±  |0.0361|
|  - global_facts                                          |      1|none             |     0|acc        |â†‘  | 0.4400|Â±  |0.0499|
|  - human_aging                                           |      1|none             |     0|acc        |â†‘  | 0.6906|Â±  |0.0310|
|  - management                                            |      1|none             |     0|acc        |â†‘  | 0.7767|Â±  |0.0412|
|  - marketing                                             |      1|none             |     0|acc        |â†‘  | 0.8675|Â±  |0.0222|
|  - medical_genetics                                      |      1|none             |     0|acc        |â†‘  | 0.7100|Â±  |0.0456|
|  - miscellaneous                                         |      1|none             |     0|acc        |â†‘  | 0.8186|Â±  |0.0138|
|  - nutrition                                             |      1|none             |     0|acc        |â†‘  | 0.7549|Â±  |0.0246|
|  - professional_accounting                               |      1|none             |     0|acc        |â†‘  | 0.5142|Â±  |0.0298|
|  - professional_medicine                                 |      1|none             |     0|acc        |â†‘  | 0.7206|Â±  |0.0273|
|  - virology                                              |      1|none             |     0|acc        |â†‘  | 0.5542|Â±  |0.0387|
| - social sciences                                        |      2|none             |      |acc        |â†‘  | 0.7439|Â±  |0.0077|
|  - econometrics                                          |      1|none             |     0|acc        |â†‘  | 0.4386|Â±  |0.0467|
|  - high_school_geography                                 |      1|none             |     0|acc        |â†‘  | 0.7980|Â±  |0.0286|
|  - high_school_government_and_politics                   |      1|none             |     0|acc        |â†‘  | 0.8860|Â±  |0.0229|
|  - high_school_macroeconomics                            |      1|none             |     0|acc        |â†‘  | 0.6641|Â±  |0.0239|
|  - high_school_microeconomics                            |      1|none             |     0|acc        |â†‘  | 0.7017|Â±  |0.0297|
|  - high_school_psychology                                |      1|none             |     0|acc        |â†‘  | 0.8532|Â±  |0.0152|
|  - human_sexuality                                       |      1|none             |     0|acc        |â†‘  | 0.7481|Â±  |0.0381|
|  - professional_psychology                               |      1|none             |     0|acc        |â†‘  | 0.6667|Â±  |0.0191|
|  - public_relations                                      |      1|none             |     0|acc        |â†‘  | 0.6818|Â±  |0.0446|
|  - security_studies                                      |      1|none             |     0|acc        |â†‘  | 0.7469|Â±  |0.0278|
|  - sociology                                             |      1|none             |     0|acc        |â†‘  | 0.8358|Â±  |0.0262|
|  - us_foreign_policy                                     |      1|none             |     0|acc        |â†‘  | 0.8700|Â±  |0.0338|
| - stem                                                   |      2|none             |      |acc        |â†‘  | 0.5614|Â±  |0.0085|
|  - abstract_algebra                                      |      1|none             |     0|acc        |â†‘  | 0.3500|Â±  |0.0479|
|  - anatomy                                               |      1|none             |     0|acc        |â†‘  | 0.6741|Â±  |0.0405|
|  - astronomy                                             |      1|none             |     0|acc        |â†‘  | 0.7237|Â±  |0.0364|
|  - college_biology                                       |      1|none             |     0|acc        |â†‘  | 0.7708|Â±  |0.0351|
|  - college_chemistry                                     |      1|none             |     0|acc        |â†‘  | 0.4600|Â±  |0.0501|
|  - college_computer_science                              |      1|none             |     0|acc        |â†‘  | 0.5900|Â±  |0.0494|
|  - college_mathematics                                   |      1|none             |     0|acc        |â†‘  | 0.3700|Â±  |0.0485|
|  - college_physics                                       |      1|none             |     0|acc        |â†‘  | 0.3725|Â±  |0.0481|
|  - computer_security                                     |      1|none             |     0|acc        |â†‘  | 0.7000|Â±  |0.0461|
|  - conceptual_physics                                    |      1|none             |     0|acc        |â†‘  | 0.5745|Â±  |0.0323|
|  - electrical_engineering                                |      1|none             |     0|acc        |â†‘  | 0.5517|Â±  |0.0414|
|  - elementary_mathematics                                |      1|none             |     0|acc        |â†‘  | 0.4577|Â±  |0.0257|
|  - high_school_biology                                   |      1|none             |     0|acc        |â†‘  | 0.7968|Â±  |0.0229|
|  - high_school_chemistry                                 |      1|none             |     0|acc        |â†‘  | 0.5320|Â±  |0.0351|
|  - high_school_computer_science                          |      1|none             |     0|acc        |â†‘  | 0.7200|Â±  |0.0451|
|  - high_school_mathematics                               |      1|none             |     0|acc        |â†‘  | 0.3889|Â±  |0.0297|
|  - high_school_physics                                   |      1|none             |     0|acc        |â†‘  | 0.4371|Â±  |0.0405|
|  - high_school_statistics                                |      1|none             |     0|acc        |â†‘  | 0.5972|Â±  |0.0334|
|  - machine_learning                                      |      1|none             |     0|acc        |â†‘  | 0.5179|Â±  |0.0474|
|nq_open                                                   |      4|remove_whitespace|     0|exact_match|â†‘  | 0.1576|Â±  |0.0061|
|openbookqa                                                |      1|none             |     0|acc        |â†‘  | 0.3640|Â±  |0.0215|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.4660|Â±  |0.0223|
|piqa                                                      |      1|none             |     0|acc        |â†‘  | 0.8096|Â±  |0.0092|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.8232|Â±  |0.0089|
|qnli                                                      |      1|none             |     0|acc        |â†‘  | 0.4950|Â±  |0.0068|
|sciq                                                      |      1|none             |     0|acc        |â†‘  | 0.9680|Â±  |0.0056|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.9560|Â±  |0.0065|
|triviaqa                                                  |      3|remove_whitespace|     0|exact_match|â†‘  | 0.5278|Â±  |0.0037|
|truthfulqa_gen                                            |      3|none             |     0|bleu_acc   |â†‘  | 0.0734|Â±  |0.0091|
|                                                          |       |none             |     0|bleu_diff  |â†‘  |-1.3264|Â±  |0.3132|
|                                                          |       |none             |     0|bleu_max   |â†‘  | 4.1401|Â±  |0.4048|
|                                                          |       |none             |     0|rouge1_acc |â†‘  | 0.0759|Â±  |0.0093|
|                                                          |       |none             |     0|rouge1_diff|â†‘  |-2.7245|Â±  |0.4932|
|                                                          |       |none             |     0|rouge1_max |â†‘  | 9.7318|Â±  |0.7309|
|                                                          |       |none             |     0|rouge2_acc |â†‘  | 0.0661|Â±  |0.0087|
|                                                          |       |none             |     0|rouge2_diff|â†‘  |-2.5882|Â±  |0.5097|
|                                                          |       |none             |     0|rouge2_max |â†‘  | 6.3193|Â±  |0.5845|
|                                                          |       |none             |     0|rougeL_acc |â†‘  | 0.0722|Â±  |0.0091|
|                                                          |       |none             |     0|rougeL_diff|â†‘  |-2.8073|Â±  |0.4921|
|                                                          |       |none             |     0|rougeL_max |â†‘  | 9.1139|Â±  |0.6993|
|truthfulqa_mc1                                            |      2|none             |     0|acc        |â†‘  | 0.3256|Â±  |0.0164|
|truthfulqa_mc2                                            |      3|none             |     0|acc        |â†‘  | 0.4867|Â±  |0.0147|
|winogrande                                                |      1|none             |     0|acc        |â†‘  | 0.7380|Â±  |0.0124|

|      Groups      |Version|  Filter  |n-shot|  Metric   |   |Value |   |Stderr|
|------------------|------:|----------|------|-----------|---|-----:|---|-----:|
|bbh               |      3|get-answer|      |exact_match|â†‘  |0.6925|Â±  |0.0051|
|mmlu              |      2|none      |      |acc        |â†‘  |0.6407|Â±  |0.0038|
| - humanities     |      2|none      |      |acc        |â†‘  |0.5792|Â±  |0.0068|
| - other          |      2|none      |      |acc        |â†‘  |0.7123|Â±  |0.0079|
| - social sciences|      2|none      |      |acc        |â†‘  |0.7439|Â±  |0.0077|
| - stem           |      2|none      |      |acc        |â†‘  |0.5614|Â±  |0.0085|

mistralai_Ministral-8B-Instruct-2410: 10h 46m 19s
âœ… Benchmark completed for mistralai_Ministral-8B-Instruct-2410

ðŸ”¥ Starting benchmark for google_gemma-3-4b-it
Passed argument batch_size = auto:1. Detecting largest batch size
Determined largest batch size: 4
Passed argument batch_size = auto. Detecting largest batch size
Determined Largest batch size: 4
hf (pretrained=/home/jaymin/Documents/llm/llm_models/google_gemma-3-4b-it), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto (4)
|                          Tasks                           |Version|     Filter      |n-shot|  Metric   |   | Value |   |Stderr|
|----------------------------------------------------------|------:|-----------------|-----:|-----------|---|------:|---|-----:|
|anli_r1                                                   |      1|none             |     0|acc        |â†‘  | 0.4920|Â±  |0.0158|
|anli_r2                                                   |      1|none             |     0|acc        |â†‘  | 0.4710|Â±  |0.0158|
|anli_r3                                                   |      1|none             |     0|acc        |â†‘  | 0.4683|Â±  |0.0144|
|arc_challenge                                             |      1|none             |     0|acc        |â†‘  | 0.5341|Â±  |0.0146|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.5708|Â±  |0.0145|
|bbh                                                       |      3|get-answer       |      |exact_match|â†‘  | 0.7094|Â±  |0.0050|
| - bbh_cot_fewshot_boolean_expressions                    |      4|get-answer       |     3|exact_match|â†‘  | 0.9000|Â±  |0.0190|
| - bbh_cot_fewshot_causal_judgement                       |      4|get-answer       |     3|exact_match|â†‘  | 0.5775|Â±  |0.0362|
| - bbh_cot_fewshot_date_understanding                     |      4|get-answer       |     3|exact_match|â†‘  | 0.7760|Â±  |0.0264|
| - bbh_cot_fewshot_disambiguation_qa                      |      4|get-answer       |     3|exact_match|â†‘  | 0.5640|Â±  |0.0314|
| - bbh_cot_fewshot_dyck_languages                         |      4|get-answer       |     3|exact_match|â†‘  | 0.5440|Â±  |0.0316|
| - bbh_cot_fewshot_formal_fallacies                       |      4|get-answer       |     3|exact_match|â†‘  | 0.3440|Â±  |0.0301|
| - bbh_cot_fewshot_geometric_shapes                       |      4|get-answer       |     3|exact_match|â†‘  | 0.2280|Â±  |0.0266|
| - bbh_cot_fewshot_hyperbaton                             |      4|get-answer       |     3|exact_match|â†‘  | 0.9760|Â±  |0.0097|
| - bbh_cot_fewshot_logical_deduction_five_objects         |      4|get-answer       |     3|exact_match|â†‘  | 0.6040|Â±  |0.0310|
| - bbh_cot_fewshot_logical_deduction_seven_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.4880|Â±  |0.0317|
| - bbh_cot_fewshot_logical_deduction_three_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.9080|Â±  |0.0183|
| - bbh_cot_fewshot_movie_recommendation                   |      4|get-answer       |     3|exact_match|â†‘  | 0.7000|Â±  |0.0290|
| - bbh_cot_fewshot_multistep_arithmetic_two               |      4|get-answer       |     3|exact_match|â†‘  | 0.8400|Â±  |0.0232|
| - bbh_cot_fewshot_navigate                               |      4|get-answer       |     3|exact_match|â†‘  | 0.8680|Â±  |0.0215|
| - bbh_cot_fewshot_object_counting                        |      4|get-answer       |     3|exact_match|â†‘  | 0.9120|Â±  |0.0180|
| - bbh_cot_fewshot_penguins_in_a_table                    |      4|get-answer       |     3|exact_match|â†‘  | 0.7466|Â±  |0.0361|
| - bbh_cot_fewshot_reasoning_about_colored_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.7240|Â±  |0.0283|
| - bbh_cot_fewshot_ruin_names                             |      4|get-answer       |     3|exact_match|â†‘  | 0.6760|Â±  |0.0297|
| - bbh_cot_fewshot_salient_translation_error_detection    |      4|get-answer       |     3|exact_match|â†‘  | 0.5240|Â±  |0.0316|
| - bbh_cot_fewshot_snarks                                 |      4|get-answer       |     3|exact_match|â†‘  | 0.6404|Â±  |0.0361|
| - bbh_cot_fewshot_sports_understanding                   |      4|get-answer       |     3|exact_match|â†‘  | 0.8880|Â±  |0.0200|
| - bbh_cot_fewshot_temporal_sequences                     |      4|get-answer       |     3|exact_match|â†‘  | 0.8600|Â±  |0.0220|
| - bbh_cot_fewshot_tracking_shuffled_objects_five_objects |      4|get-answer       |     3|exact_match|â†‘  | 0.8080|Â±  |0.0250|
| - bbh_cot_fewshot_tracking_shuffled_objects_seven_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.7840|Â±  |0.0261|
| - bbh_cot_fewshot_tracking_shuffled_objects_three_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.9760|Â±  |0.0097|
| - bbh_cot_fewshot_web_of_lies                            |      4|get-answer       |     3|exact_match|â†‘  | 0.9960|Â±  |0.0040|
| - bbh_cot_fewshot_word_sorting                           |      4|get-answer       |     3|exact_match|â†‘  | 0.2640|Â±  |0.0279|
|boolq                                                     |      2|none             |     0|acc        |â†‘  | 0.8398|Â±  |0.0064|
|drop                                                      |      3|none             |     0|em         |â†‘  | 0.0055|Â±  |0.0008|
|                                                          |       |none             |     0|f1         |â†‘  | 0.0893|Â±  |0.0018|
|gpqa_diamond_cot_n_shot                                   |      2|flexible-extract |     0|exact_match|â†‘  | 0.1667|Â±  |0.0266|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0152|Â±  |0.0087|
|gpqa_diamond_cot_zeroshot                                 |      1|flexible-extract |     0|exact_match|â†‘  | 0.1515|Â±  |0.0255|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_generative_n_shot                            |      2|flexible-extract |     0|exact_match|â†‘  | 0.3384|Â±  |0.0337|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_n_shot                                       |      2|none             |     0|acc        |â†‘  | 0.3434|Â±  |0.0338|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3434|Â±  |0.0338|
|gpqa_diamond_zeroshot                                     |      1|none             |     0|acc        |â†‘  | 0.3535|Â±  |0.0341|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3535|Â±  |0.0341|
|gpqa_extended_cot_n_shot                                  |      2|flexible-extract |     0|exact_match|â†‘  | 0.1740|Â±  |0.0162|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0147|Â±  |0.0051|
|gpqa_extended_cot_zeroshot                                |      1|flexible-extract |     0|exact_match|â†‘  | 0.1612|Â±  |0.0158|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0018|Â±  |0.0018|
|gpqa_extended_generative_n_shot                           |      2|flexible-extract |     0|exact_match|â†‘  | 0.2985|Â±  |0.0196|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_n_shot                                      |      2|none             |     0|acc        |â†‘  | 0.2711|Â±  |0.0190|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2711|Â±  |0.0190|
|gpqa_extended_zeroshot                                    |      1|none             |     0|acc        |â†‘  | 0.3407|Â±  |0.0203|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3407|Â±  |0.0203|
|gpqa_main_cot_n_shot                                      |      2|flexible-extract |     0|exact_match|â†‘  | 0.1496|Â±  |0.0169|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0067|Â±  |0.0039|
|gpqa_main_cot_zeroshot                                    |      1|flexible-extract |     0|exact_match|â†‘  | 0.1496|Â±  |0.0169|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_generative_n_shot                               |      2|flexible-extract |     0|exact_match|â†‘  | 0.3125|Â±  |0.0219|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_n_shot                                          |      2|none             |     0|acc        |â†‘  | 0.3080|Â±  |0.0218|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3080|Â±  |0.0218|
|gpqa_main_zeroshot                                        |      1|none             |     0|acc        |â†‘  | 0.2879|Â±  |0.0214|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2879|Â±  |0.0214|
|gsm8k                                                     |      3|flexible-extract |     5|exact_match|â†‘  | 0.7665|Â±  |0.0117|
|                                                          |       |strict-match     |     5|exact_match|â†‘  | 0.7619|Â±  |0.0117|
|hellaswag                                                 |      1|none             |     0|acc        |â†‘  | 0.5599|Â±  |0.0050|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.7414|Â±  |0.0044|
|mmlu                                                      |      2|none             |      |acc        |â†‘  | 0.5756|Â±  |0.0039|
| - humanities                                             |      2|none             |      |acc        |â†‘  | 0.5163|Â±  |0.0068|
|  - formal_logic                                          |      1|none             |     0|acc        |â†‘  | 0.3571|Â±  |0.0429|
|  - high_school_european_history                          |      1|none             |     0|acc        |â†‘  | 0.7455|Â±  |0.0340|
|  - high_school_us_history                                |      1|none             |     0|acc        |â†‘  | 0.7451|Â±  |0.0306|
|  - high_school_world_history                             |      1|none             |     0|acc        |â†‘  | 0.7468|Â±  |0.0283|
|  - international_law                                     |      1|none             |     0|acc        |â†‘  | 0.7438|Â±  |0.0398|
|  - jurisprudence                                         |      1|none             |     0|acc        |â†‘  | 0.7037|Â±  |0.0441|
|  - logical_fallacies                                     |      1|none             |     0|acc        |â†‘  | 0.7178|Â±  |0.0354|
|  - moral_disputes                                        |      1|none             |     0|acc        |â†‘  | 0.6301|Â±  |0.0260|
|  - moral_scenarios                                       |      1|none             |     0|acc        |â†‘  | 0.2480|Â±  |0.0144|
|  - philosophy                                            |      1|none             |     0|acc        |â†‘  | 0.6592|Â±  |0.0269|
|  - prehistory                                            |      1|none             |     0|acc        |â†‘  | 0.6821|Â±  |0.0259|
|  - professional_law                                      |      1|none             |     0|acc        |â†‘  | 0.4237|Â±  |0.0126|
|  - world_religions                                       |      1|none             |     0|acc        |â†‘  | 0.7778|Â±  |0.0319|
| - other                                                  |      2|none             |      |acc        |â†‘  | 0.6369|Â±  |0.0083|
|  - business_ethics                                       |      1|none             |     0|acc        |â†‘  | 0.6000|Â±  |0.0492|
|  - clinical_knowledge                                    |      1|none             |     0|acc        |â†‘  | 0.6566|Â±  |0.0292|
|  - college_medicine                                      |      1|none             |     0|acc        |â†‘  | 0.5723|Â±  |0.0377|
|  - global_facts                                          |      1|none             |     0|acc        |â†‘  | 0.2900|Â±  |0.0456|
|  - human_aging                                           |      1|none             |     0|acc        |â†‘  | 0.6278|Â±  |0.0324|
|  - management                                            |      1|none             |     0|acc        |â†‘  | 0.7282|Â±  |0.0441|
|  - marketing                                             |      1|none             |     0|acc        |â†‘  | 0.8462|Â±  |0.0236|
|  - medical_genetics                                      |      1|none             |     0|acc        |â†‘  | 0.6300|Â±  |0.0485|
|  - miscellaneous                                         |      1|none             |     0|acc        |â†‘  | 0.7573|Â±  |0.0153|
|  - nutrition                                             |      1|none             |     0|acc        |â†‘  | 0.6438|Â±  |0.0274|
|  - professional_accounting                               |      1|none             |     0|acc        |â†‘  | 0.3901|Â±  |0.0291|
|  - professional_medicine                                 |      1|none             |     0|acc        |â†‘  | 0.5772|Â±  |0.0300|
|  - virology                                              |      1|none             |     0|acc        |â†‘  | 0.5060|Â±  |0.0389|
| - social sciences                                        |      2|none             |      |acc        |â†‘  | 0.6744|Â±  |0.0083|
|  - econometrics                                          |      1|none             |     0|acc        |â†‘  | 0.4649|Â±  |0.0469|
|  - high_school_geography                                 |      1|none             |     0|acc        |â†‘  | 0.7020|Â±  |0.0326|
|  - high_school_government_and_politics                   |      1|none             |     0|acc        |â†‘  | 0.8135|Â±  |0.0281|
|  - high_school_macroeconomics                            |      1|none             |     0|acc        |â†‘  | 0.5718|Â±  |0.0251|
|  - high_school_microeconomics                            |      1|none             |     0|acc        |â†‘  | 0.6387|Â±  |0.0312|
|  - high_school_psychology                                |      1|none             |     0|acc        |â†‘  | 0.7780|Â±  |0.0178|
|  - human_sexuality                                       |      1|none             |     0|acc        |â†‘  | 0.6641|Â±  |0.0414|
|  - professional_psychology                               |      1|none             |     0|acc        |â†‘  | 0.5948|Â±  |0.0199|
|  - public_relations                                      |      1|none             |     0|acc        |â†‘  | 0.6455|Â±  |0.0458|
|  - security_studies                                      |      1|none             |     0|acc        |â†‘  | 0.6980|Â±  |0.0294|
|  - sociology                                             |      1|none             |     0|acc        |â†‘  | 0.7612|Â±  |0.0301|
|  - us_foreign_policy                                     |      1|none             |     0|acc        |â†‘  | 0.8100|Â±  |0.0394|
| - stem                                                   |      2|none             |      |acc        |â†‘  | 0.5071|Â±  |0.0086|
|  - abstract_algebra                                      |      1|none             |     0|acc        |â†‘  | 0.3200|Â±  |0.0469|
|  - anatomy                                               |      1|none             |     0|acc        |â†‘  | 0.5481|Â±  |0.0430|
|  - astronomy                                             |      1|none             |     0|acc        |â†‘  | 0.6908|Â±  |0.0376|
|  - college_biology                                       |      1|none             |     0|acc        |â†‘  | 0.6875|Â±  |0.0388|
|  - college_chemistry                                     |      1|none             |     0|acc        |â†‘  | 0.4000|Â±  |0.0492|
|  - college_computer_science                              |      1|none             |     0|acc        |â†‘  | 0.4700|Â±  |0.0502|
|  - college_mathematics                                   |      1|none             |     0|acc        |â†‘  | 0.3700|Â±  |0.0485|
|  - college_physics                                       |      1|none             |     0|acc        |â†‘  | 0.3725|Â±  |0.0481|
|  - computer_security                                     |      1|none             |     0|acc        |â†‘  | 0.6600|Â±  |0.0476|
|  - conceptual_physics                                    |      1|none             |     0|acc        |â†‘  | 0.5404|Â±  |0.0326|
|  - electrical_engineering                                |      1|none             |     0|acc        |â†‘  | 0.5310|Â±  |0.0416|
|  - elementary_mathematics                                |      1|none             |     0|acc        |â†‘  | 0.4841|Â±  |0.0257|
|  - high_school_biology                                   |      1|none             |     0|acc        |â†‘  | 0.7065|Â±  |0.0259|
|  - high_school_chemistry                                 |      1|none             |     0|acc        |â†‘  | 0.5074|Â±  |0.0352|
|  - high_school_computer_science                          |      1|none             |     0|acc        |â†‘  | 0.7200|Â±  |0.0451|
|  - high_school_mathematics                               |      1|none             |     0|acc        |â†‘  | 0.3815|Â±  |0.0296|
|  - high_school_physics                                   |      1|none             |     0|acc        |â†‘  | 0.3245|Â±  |0.0382|
|  - high_school_statistics                                |      1|none             |     0|acc        |â†‘  | 0.4074|Â±  |0.0335|
|  - machine_learning                                      |      1|none             |     0|acc        |â†‘  | 0.3571|Â±  |0.0455|
|nq_open                                                   |      4|remove_whitespace|     0|exact_match|â†‘  | 0.1094|Â±  |0.0052|
|openbookqa                                                |      1|none             |     0|acc        |â†‘  | 0.3640|Â±  |0.0215|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.4660|Â±  |0.0223|
|piqa                                                      |      1|none             |     0|acc        |â†‘  | 0.7628|Â±  |0.0099|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.7720|Â±  |0.0098|
|qnli                                                      |      1|none             |     0|acc        |â†‘  | 0.5660|Â±  |0.0067|
|sciq                                                      |      1|none             |     0|acc        |â†‘  | 0.9550|Â±  |0.0066|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.9310|Â±  |0.0080|
|triviaqa                                                  |      3|remove_whitespace|     0|exact_match|â†‘  | 0.3148|Â±  |0.0035|
|truthfulqa_gen                                            |      3|none             |     0|bleu_acc   |â†‘  | 0.4333|Â±  |0.0173|
|                                                          |       |none             |     0|bleu_diff  |â†‘  |-1.4479|Â±  |0.5140|
|                                                          |       |none             |     0|bleu_max   |â†‘  |18.0994|Â±  |0.6738|
|                                                          |       |none             |     0|rouge1_acc |â†‘  | 0.4235|Â±  |0.0173|
|                                                          |       |none             |     0|rouge1_diff|â†‘  |-2.6851|Â±  |0.6899|
|                                                          |       |none             |     0|rouge1_max |â†‘  |41.5023|Â±  |0.8412|
|                                                          |       |none             |     0|rouge2_acc |â†‘  | 0.3195|Â±  |0.0163|
|                                                          |       |none             |     0|rouge2_diff|â†‘  |-4.2870|Â±  |0.7901|
|                                                          |       |none             |     0|rouge2_max |â†‘  |25.1379|Â±  |0.9049|
|                                                          |       |none             |     0|rougeL_acc |â†‘  | 0.4247|Â±  |0.0173|
|                                                          |       |none             |     0|rougeL_diff|â†‘  |-2.9853|Â±  |0.6819|
|                                                          |       |none             |     0|rougeL_max |â†‘  |38.8207|Â±  |0.8446|
|truthfulqa_mc1                                            |      2|none             |     0|acc        |â†‘  | 0.3488|Â±  |0.0167|
|truthfulqa_mc2                                            |      3|none             |     0|acc        |â†‘  | 0.5188|Â±  |0.0160|
|winogrande                                                |      1|none             |     0|acc        |â†‘  | 0.7009|Â±  |0.0129|

|      Groups      |Version|  Filter  |n-shot|  Metric   |   |Value |   |Stderr|
|------------------|------:|----------|------|-----------|---|-----:|---|-----:|
|bbh               |      3|get-answer|      |exact_match|â†‘  |0.7094|Â±  |0.0050|
|mmlu              |      2|none      |      |acc        |â†‘  |0.5756|Â±  |0.0039|
| - humanities     |      2|none      |      |acc        |â†‘  |0.5163|Â±  |0.0068|
| - other          |      2|none      |      |acc        |â†‘  |0.6369|Â±  |0.0083|
| - social sciences|      2|none      |      |acc        |â†‘  |0.6744|Â±  |0.0083|
| - stem           |      2|none      |      |acc        |â†‘  |0.5071|Â±  |0.0086|

google_gemma-3-4b-it: 4h 51m 14s
âœ… Benchmark completed for google_gemma-3-4b-it

ðŸ”¥ Starting benchmark for google_gemma-3-1b-it
Passed argument batch_size = auto:1. Detecting largest batch size
Determined largest batch size: 1
Passed argument batch_size = auto. Detecting largest batch size
Determined Largest batch size: 1
hf (pretrained=/home/jaymin/Documents/llm/llm_models/google_gemma-3-1b-it), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto (1)
|                          Tasks                           |Version|     Filter      |n-shot|  Metric   |   | Value  |   |Stderr|
|----------------------------------------------------------|------:|-----------------|-----:|-----------|---|-------:|---|-----:|
|anli_r1                                                   |      1|none             |     0|acc        |â†‘  |  0.3320|Â±  |0.0149|
|anli_r2                                                   |      1|none             |     0|acc        |â†‘  |  0.3540|Â±  |0.0151|
|anli_r3                                                   |      1|none             |     0|acc        |â†‘  |  0.3567|Â±  |0.0138|
|arc_challenge                                             |      1|none             |     0|acc        |â†‘  |  0.3532|Â±  |0.0140|
|                                                          |       |none             |     0|acc_norm   |â†‘  |  0.3805|Â±  |0.0142|
|bbh                                                       |      3|get-answer       |      |exact_match|â†‘  |  0.3823|Â±  |0.0053|
| - bbh_cot_fewshot_boolean_expressions                    |      4|get-answer       |     3|exact_match|â†‘  |  0.8320|Â±  |0.0237|
| - bbh_cot_fewshot_causal_judgement                       |      4|get-answer       |     3|exact_match|â†‘  |  0.5134|Â±  |0.0366|
| - bbh_cot_fewshot_date_understanding                     |      4|get-answer       |     3|exact_match|â†‘  |  0.2680|Â±  |0.0281|
| - bbh_cot_fewshot_disambiguation_qa                      |      4|get-answer       |     3|exact_match|â†‘  |  0.3800|Â±  |0.0308|
| - bbh_cot_fewshot_dyck_languages                         |      4|get-answer       |     3|exact_match|â†‘  |  0.0920|Â±  |0.0183|
| - bbh_cot_fewshot_formal_fallacies                       |      4|get-answer       |     3|exact_match|â†‘  |  0.5040|Â±  |0.0317|
| - bbh_cot_fewshot_geometric_shapes                       |      4|get-answer       |     3|exact_match|â†‘  |  0.1240|Â±  |0.0209|
| - bbh_cot_fewshot_hyperbaton                             |      4|get-answer       |     3|exact_match|â†‘  |  0.5240|Â±  |0.0316|
| - bbh_cot_fewshot_logical_deduction_five_objects         |      4|get-answer       |     3|exact_match|â†‘  |  0.2280|Â±  |0.0266|
| - bbh_cot_fewshot_logical_deduction_seven_objects        |      4|get-answer       |     3|exact_match|â†‘  |  0.1560|Â±  |0.0230|
| - bbh_cot_fewshot_logical_deduction_three_objects        |      4|get-answer       |     3|exact_match|â†‘  |  0.3960|Â±  |0.0310|
| - bbh_cot_fewshot_movie_recommendation                   |      4|get-answer       |     3|exact_match|â†‘  |  0.3080|Â±  |0.0293|
| - bbh_cot_fewshot_multistep_arithmetic_two               |      4|get-answer       |     3|exact_match|â†‘  |  0.6560|Â±  |0.0301|
| - bbh_cot_fewshot_navigate                               |      4|get-answer       |     3|exact_match|â†‘  |  0.7440|Â±  |0.0277|
| - bbh_cot_fewshot_object_counting                        |      4|get-answer       |     3|exact_match|â†‘  |  0.5800|Â±  |0.0313|
| - bbh_cot_fewshot_penguins_in_a_table                    |      4|get-answer       |     3|exact_match|â†‘  |  0.1712|Â±  |0.0313|
| - bbh_cot_fewshot_reasoning_about_colored_objects        |      4|get-answer       |     3|exact_match|â†‘  |  0.1600|Â±  |0.0232|
| - bbh_cot_fewshot_ruin_names                             |      4|get-answer       |     3|exact_match|â†‘  |  0.3640|Â±  |0.0305|
| - bbh_cot_fewshot_salient_translation_error_detection    |      4|get-answer       |     3|exact_match|â†‘  |  0.1160|Â±  |0.0203|
| - bbh_cot_fewshot_snarks                                 |      4|get-answer       |     3|exact_match|â†‘  |  0.5112|Â±  |0.0376|
| - bbh_cot_fewshot_sports_understanding                   |      4|get-answer       |     3|exact_match|â†‘  |  0.6120|Â±  |0.0309|
| - bbh_cot_fewshot_temporal_sequences                     |      4|get-answer       |     3|exact_match|â†‘  |  0.3040|Â±  |0.0292|
| - bbh_cot_fewshot_tracking_shuffled_objects_five_objects |      4|get-answer       |     3|exact_match|â†‘  |  0.2240|Â±  |0.0264|
| - bbh_cot_fewshot_tracking_shuffled_objects_seven_objects|      4|get-answer       |     3|exact_match|â†‘  |  0.1600|Â±  |0.0232|
| - bbh_cot_fewshot_tracking_shuffled_objects_three_objects|      4|get-answer       |     3|exact_match|â†‘  |  0.5040|Â±  |0.0317|
| - bbh_cot_fewshot_web_of_lies                            |      4|get-answer       |     3|exact_match|â†‘  |  0.8360|Â±  |0.0235|
| - bbh_cot_fewshot_word_sorting                           |      4|get-answer       |     3|exact_match|â†‘  |  0.0360|Â±  |0.0118|
|boolq                                                     |      2|none             |     0|acc        |â†‘  |  0.7581|Â±  |0.0075|
|drop                                                      |      3|none             |     0|em         |â†‘  |  0.0018|Â±  |0.0004|
|                                                          |       |none             |     0|f1         |â†‘  |  0.0762|Â±  |0.0017|
|gpqa_diamond_cot_n_shot                                   |      2|flexible-extract |     0|exact_match|â†‘  |  0.1263|Â±  |0.0237|
|                                                          |       |strict-match     |     0|exact_match|â†‘  |  0.0000|Â±  |0.0000|
|gpqa_diamond_cot_zeroshot                                 |      1|flexible-extract |     0|exact_match|â†‘  |  0.1364|Â±  |0.0245|
|                                                          |       |strict-match     |     0|exact_match|â†‘  |  0.0000|Â±  |0.0000|
|gpqa_diamond_generative_n_shot                            |      2|flexible-extract |     0|exact_match|â†‘  |  0.2172|Â±  |0.0294|
|                                                          |       |strict-match     |     0|exact_match|â†‘  |  0.0000|Â±  |0.0000|
|gpqa_diamond_n_shot                                       |      2|none             |     0|acc        |â†‘  |  0.2626|Â±  |0.0314|
|                                                          |       |none             |     0|acc_norm   |â†‘  |  0.2626|Â±  |0.0314|
|gpqa_diamond_zeroshot                                     |      1|none             |     0|acc        |â†‘  |  0.2374|Â±  |0.0303|
|                                                          |       |none             |     0|acc_norm   |â†‘  |  0.2374|Â±  |0.0303|
|gpqa_extended_cot_n_shot                                  |      2|flexible-extract |     0|exact_match|â†‘  |  0.1300|Â±  |0.0144|
|                                                          |       |strict-match     |     0|exact_match|â†‘  |  0.0018|Â±  |0.0018|
|gpqa_extended_cot_zeroshot                                |      1|flexible-extract |     0|exact_match|â†‘  |  0.1520|Â±  |0.0154|
|                                                          |       |strict-match     |     0|exact_match|â†‘  |  0.0000|Â±  |0.0000|
|gpqa_extended_generative_n_shot                           |      2|flexible-extract |     0|exact_match|â†‘  |  0.2601|Â±  |0.0188|
|                                                          |       |strict-match     |     0|exact_match|â†‘  |  0.0037|Â±  |0.0026|
|gpqa_extended_n_shot                                      |      2|none             |     0|acc        |â†‘  |  0.2491|Â±  |0.0185|
|                                                          |       |none             |     0|acc_norm   |â†‘  |  0.2491|Â±  |0.0185|
|gpqa_extended_zeroshot                                    |      1|none             |     0|acc        |â†‘  |  0.2637|Â±  |0.0189|
|                                                          |       |none             |     0|acc_norm   |â†‘  |  0.2637|Â±  |0.0189|
|gpqa_main_cot_n_shot                                      |      2|flexible-extract |     0|exact_match|â†‘  |  0.1183|Â±  |0.0153|
|                                                          |       |strict-match     |     0|exact_match|â†‘  |  0.0022|Â±  |0.0022|
|gpqa_main_cot_zeroshot                                    |      1|flexible-extract |     0|exact_match|â†‘  |  0.1607|Â±  |0.0174|
|                                                          |       |strict-match     |     0|exact_match|â†‘  |  0.0000|Â±  |0.0000|
|gpqa_main_generative_n_shot                               |      2|flexible-extract |     0|exact_match|â†‘  |  0.2344|Â±  |0.0200|
|                                                          |       |strict-match     |     0|exact_match|â†‘  |  0.0045|Â±  |0.0032|
|gpqa_main_n_shot                                          |      2|none             |     0|acc        |â†‘  |  0.2679|Â±  |0.0209|
|                                                          |       |none             |     0|acc_norm   |â†‘  |  0.2679|Â±  |0.0209|
|gpqa_main_zeroshot                                        |      1|none             |     0|acc        |â†‘  |  0.2656|Â±  |0.0209|
|                                                          |       |none             |     0|acc_norm   |â†‘  |  0.2656|Â±  |0.0209|
|gsm8k                                                     |      3|flexible-extract |     5|exact_match|â†‘  |  0.2502|Â±  |0.0119|
|                                                          |       |strict-match     |     5|exact_match|â†‘  |  0.2472|Â±  |0.0119|
|hellaswag                                                 |      1|none             |     0|acc        |â†‘  |  0.4338|Â±  |0.0049|
|                                                          |       |none             |     0|acc_norm   |â†‘  |  0.5783|Â±  |0.0049|
|mmlu                                                      |      2|none             |      |acc        |â†‘  |  0.3859|Â±  |0.0040|
| - humanities                                             |      2|none             |      |acc        |â†‘  |  0.3626|Â±  |0.0069|
|  - formal_logic                                          |      1|none             |     0|acc        |â†‘  |  0.3492|Â±  |0.0426|
|  - high_school_european_history                          |      1|none             |     0|acc        |â†‘  |  0.4909|Â±  |0.0390|
|  - high_school_us_history                                |      1|none             |     0|acc        |â†‘  |  0.4706|Â±  |0.0350|
|  - high_school_world_history                             |      1|none             |     0|acc        |â†‘  |  0.4726|Â±  |0.0325|
|  - international_law                                     |      1|none             |     0|acc        |â†‘  |  0.5372|Â±  |0.0455|
|  - jurisprudence                                         |      1|none             |     0|acc        |â†‘  |  0.4722|Â±  |0.0483|
|  - logical_fallacies                                     |      1|none             |     0|acc        |â†‘  |  0.4417|Â±  |0.0390|
|  - moral_disputes                                        |      1|none             |     0|acc        |â†‘  |  0.4220|Â±  |0.0266|
|  - moral_scenarios                                       |      1|none             |     0|acc        |â†‘  |  0.2335|Â±  |0.0141|
|  - philosophy                                            |      1|none             |     0|acc        |â†‘  |  0.4244|Â±  |0.0281|
|  - prehistory                                            |      1|none             |     0|acc        |â†‘  |  0.4414|Â±  |0.0276|
|  - professional_law                                      |      1|none             |     0|acc        |â†‘  |  0.3057|Â±  |0.0118|
|  - world_religions                                       |      1|none             |     0|acc        |â†‘  |  0.5029|Â±  |0.0383|
| - other                                                  |      2|none             |      |acc        |â†‘  |  0.4335|Â±  |0.0087|
|  - business_ethics                                       |      1|none             |     0|acc        |â†‘  |  0.3700|Â±  |0.0485|
|  - clinical_knowledge                                    |      1|none             |     0|acc        |â†‘  |  0.4226|Â±  |0.0304|
|  - college_medicine                                      |      1|none             |     0|acc        |â†‘  |  0.3815|Â±  |0.0370|
|  - global_facts                                          |      1|none             |     0|acc        |â†‘  |  0.3600|Â±  |0.0482|
|  - human_aging                                           |      1|none             |     0|acc        |â†‘  |  0.4529|Â±  |0.0334|
|  - management                                            |      1|none             |     0|acc        |â†‘  |  0.5631|Â±  |0.0491|
|  - marketing                                             |      1|none             |     0|acc        |â†‘  |  0.6239|Â±  |0.0317|
|  - medical_genetics                                      |      1|none             |     0|acc        |â†‘  |  0.4100|Â±  |0.0494|
|  - miscellaneous                                         |      1|none             |     0|acc        |â†‘  |  0.5147|Â±  |0.0179|
|  - nutrition                                             |      1|none             |     0|acc        |â†‘  |  0.4150|Â±  |0.0282|
|  - professional_accounting                               |      1|none             |     0|acc        |â†‘  |  0.2730|Â±  |0.0266|
|  - professional_medicine                                 |      1|none             |     0|acc        |â†‘  |  0.2941|Â±  |0.0277|
|  - virology                                              |      1|none             |     0|acc        |â†‘  |  0.3795|Â±  |0.0378|
| - social sciences                                        |      2|none             |      |acc        |â†‘  |  0.4482|Â±  |0.0088|
|  - econometrics                                          |      1|none             |     0|acc        |â†‘  |  0.2193|Â±  |0.0389|
|  - high_school_geography                                 |      1|none             |     0|acc        |â†‘  |  0.5000|Â±  |0.0356|
|  - high_school_government_and_politics                   |      1|none             |     0|acc        |â†‘  |  0.4870|Â±  |0.0361|
|  - high_school_macroeconomics                            |      1|none             |     0|acc        |â†‘  |  0.3282|Â±  |0.0238|
|  - high_school_microeconomics                            |      1|none             |     0|acc        |â†‘  |  0.3782|Â±  |0.0315|
|  - high_school_psychology                                |      1|none             |     0|acc        |â†‘  |  0.5321|Â±  |0.0214|
|  - human_sexuality                                       |      1|none             |     0|acc        |â†‘  |  0.5038|Â±  |0.0439|
|  - professional_psychology                               |      1|none             |     0|acc        |â†‘  |  0.3644|Â±  |0.0195|
|  - public_relations                                      |      1|none             |     0|acc        |â†‘  |  0.4727|Â±  |0.0478|
|  - security_studies                                      |      1|none             |     0|acc        |â†‘  |  0.5388|Â±  |0.0319|
|  - sociology                                             |      1|none             |     0|acc        |â†‘  |  0.5970|Â±  |0.0347|
|  - us_foreign_policy                                     |      1|none             |     0|acc        |â†‘  |  0.6000|Â±  |0.0492|
| - stem                                                   |      2|none             |      |acc        |â†‘  |  0.3130|Â±  |0.0081|
|  - abstract_algebra                                      |      1|none             |     0|acc        |â†‘  |  0.2700|Â±  |0.0446|
|  - anatomy                                               |      1|none             |     0|acc        |â†‘  |  0.4593|Â±  |0.0430|
|  - astronomy                                             |      1|none             |     0|acc        |â†‘  |  0.3684|Â±  |0.0393|
|  - college_biology                                       |      1|none             |     0|acc        |â†‘  |  0.3403|Â±  |0.0396|
|  - college_chemistry                                     |      1|none             |     0|acc        |â†‘  |  0.3100|Â±  |0.0465|
|  - college_computer_science                              |      1|none             |     0|acc        |â†‘  |  0.2600|Â±  |0.0441|
|  - college_mathematics                                   |      1|none             |     0|acc        |â†‘  |  0.2300|Â±  |0.0423|
|  - college_physics                                       |      1|none             |     0|acc        |â†‘  |  0.2059|Â±  |0.0402|
|  - computer_security                                     |      1|none             |     0|acc        |â†‘  |  0.4600|Â±  |0.0501|
|  - conceptual_physics                                    |      1|none             |     0|acc        |â†‘  |  0.3745|Â±  |0.0316|
|  - electrical_engineering                                |      1|none             |     0|acc        |â†‘  |  0.4345|Â±  |0.0413|
|  - elementary_mathematics                                |      1|none             |     0|acc        |â†‘  |  0.2302|Â±  |0.0217|
|  - high_school_biology                                   |      1|none             |     0|acc        |â†‘  |  0.4323|Â±  |0.0282|
|  - high_school_chemistry                                 |      1|none             |     0|acc        |â†‘  |  0.2857|Â±  |0.0318|
|  - high_school_computer_science                          |      1|none             |     0|acc        |â†‘  |  0.3100|Â±  |0.0465|
|  - high_school_mathematics                               |      1|none             |     0|acc        |â†‘  |  0.2370|Â±  |0.0259|
|  - high_school_physics                                   |      1|none             |     0|acc        |â†‘  |  0.2252|Â±  |0.0341|
|  - high_school_statistics                                |      1|none             |     0|acc        |â†‘  |  0.2083|Â±  |0.0277|
|  - machine_learning                                      |      1|none             |     0|acc        |â†‘  |  0.3750|Â±  |0.0460|
|nq_open                                                   |      4|remove_whitespace|     0|exact_match|â†‘  |  0.0357|Â±  |0.0031|
|openbookqa                                                |      1|none             |     0|acc        |â†‘  |  0.3020|Â±  |0.0206|
|                                                          |       |none             |     0|acc_norm   |â†‘  |  0.3880|Â±  |0.0218|
|piqa                                                      |      1|none             |     0|acc        |â†‘  |  0.7182|Â±  |0.0105|
|                                                          |       |none             |     0|acc_norm   |â†‘  |  0.7209|Â±  |0.0105|
|qnli                                                      |      1|none             |     0|acc        |â†‘  |  0.4941|Â±  |0.0068|
|sciq                                                      |      1|none             |     0|acc        |â†‘  |  0.9040|Â±  |0.0093|
|                                                          |       |none             |     0|acc_norm   |â†‘  |  0.8580|Â±  |0.0110|
|triviaqa                                                  |      3|remove_whitespace|     0|exact_match|â†‘  |  0.1897|Â±  |0.0029|
|truthfulqa_gen                                            |      3|none             |     0|bleu_acc   |â†‘  |  0.3060|Â±  |0.0161|
|                                                          |       |none             |     0|bleu_diff  |â†‘  | -7.1778|Â±  |0.7355|
|                                                          |       |none             |     0|bleu_max   |â†‘  | 23.2944|Â±  |0.7624|
|                                                          |       |none             |     0|rouge1_acc |â†‘  |  0.2644|Â±  |0.0154|
|                                                          |       |none             |     0|rouge1_diff|â†‘  |-10.0231|Â±  |0.7875|
|                                                          |       |none             |     0|rouge1_max |â†‘  | 46.4515|Â±  |0.9083|
|                                                          |       |none             |     0|rouge2_acc |â†‘  |  0.2044|Â±  |0.0141|
|                                                          |       |none             |     0|rouge2_diff|â†‘  |-11.5180|Â±  |0.9589|
|                                                          |       |none             |     0|rouge2_max |â†‘  | 30.6640|Â±  |0.9977|
|                                                          |       |none             |     0|rougeL_acc |â†‘  |  0.2570|Â±  |0.0153|
|                                                          |       |none             |     0|rougeL_diff|â†‘  |-10.3014|Â±  |0.7848|
|                                                          |       |none             |     0|rougeL_max |â†‘  | 43.9439|Â±  |0.9131|
|truthfulqa_mc1                                            |      2|none             |     0|acc        |â†‘  |  0.2460|Â±  |0.0151|
|truthfulqa_mc2                                            |      3|none             |     0|acc        |â†‘  |  0.3875|Â±  |0.0152|
|winogrande                                                |      1|none             |     0|acc        |â†‘  |  0.5896|Â±  |0.0138|

|      Groups      |Version|  Filter  |n-shot|  Metric   |   |Value |   |Stderr|
|------------------|------:|----------|------|-----------|---|-----:|---|-----:|
|bbh               |      3|get-answer|      |exact_match|â†‘  |0.3823|Â±  |0.0053|
|mmlu              |      2|none      |      |acc        |â†‘  |0.3859|Â±  |0.0040|
| - humanities     |      2|none      |      |acc        |â†‘  |0.3626|Â±  |0.0069|
| - other          |      2|none      |      |acc        |â†‘  |0.4335|Â±  |0.0087|
| - social sciences|      2|none      |      |acc        |â†‘  |0.4482|Â±  |0.0088|
| - stem           |      2|none      |      |acc        |â†‘  |0.3130|Â±  |0.0081|

google_gemma-3-1b-it: 6h 50m 53s
âœ… Benchmark completed for google_gemma-3-1b-it

ðŸ”¥ Starting benchmark for google_gemma-3-12b-it
Passed argument batch_size = auto:1. Detecting largest batch size
Determined largest batch size: 2
Passed argument batch_size = auto. Detecting largest batch size
Determined Largest batch size: 2
hf (pretrained=/home/jaymin/Documents/llm/llm_models/google_gemma-3-12b-it), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto (2)
|                          Tasks                           |Version|     Filter      |n-shot|  Metric   |   | Value |   |Stderr|
|----------------------------------------------------------|------:|-----------------|-----:|-----------|---|------:|---|-----:|
|anli_r1                                                   |      1|none             |     0|acc        |â†‘  | 0.6030|Â±  |0.0155|
|anli_r2                                                   |      1|none             |     0|acc        |â†‘  | 0.5600|Â±  |0.0157|
|anli_r3                                                   |      1|none             |     0|acc        |â†‘  | 0.5958|Â±  |0.0142|
|arc_challenge                                             |      1|none             |     0|acc        |â†‘  | 0.6084|Â±  |0.0143|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.6109|Â±  |0.0142|
|bbh                                                       |      3|get-answer       |      |exact_match|â†‘  | 0.8019|Â±  |0.0044|
| - bbh_cot_fewshot_boolean_expressions                    |      4|get-answer       |     3|exact_match|â†‘  | 0.9160|Â±  |0.0176|
| - bbh_cot_fewshot_causal_judgement                       |      4|get-answer       |     3|exact_match|â†‘  | 0.5829|Â±  |0.0362|
| - bbh_cot_fewshot_date_understanding                     |      4|get-answer       |     3|exact_match|â†‘  | 0.8880|Â±  |0.0200|
| - bbh_cot_fewshot_disambiguation_qa                      |      4|get-answer       |     3|exact_match|â†‘  | 0.7640|Â±  |0.0269|
| - bbh_cot_fewshot_dyck_languages                         |      4|get-answer       |     3|exact_match|â†‘  | 0.7080|Â±  |0.0288|
| - bbh_cot_fewshot_formal_fallacies                       |      4|get-answer       |     3|exact_match|â†‘  | 0.6000|Â±  |0.0310|
| - bbh_cot_fewshot_geometric_shapes                       |      4|get-answer       |     3|exact_match|â†‘  | 0.3440|Â±  |0.0301|
| - bbh_cot_fewshot_hyperbaton                             |      4|get-answer       |     3|exact_match|â†‘  | 0.9960|Â±  |0.0040|
| - bbh_cot_fewshot_logical_deduction_five_objects         |      4|get-answer       |     3|exact_match|â†‘  | 0.7960|Â±  |0.0255|
| - bbh_cot_fewshot_logical_deduction_seven_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.4800|Â±  |0.0317|
| - bbh_cot_fewshot_logical_deduction_three_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.9600|Â±  |0.0124|
| - bbh_cot_fewshot_movie_recommendation                   |      4|get-answer       |     3|exact_match|â†‘  | 0.8480|Â±  |0.0228|
| - bbh_cot_fewshot_multistep_arithmetic_two               |      4|get-answer       |     3|exact_match|â†‘  | 0.9520|Â±  |0.0135|
| - bbh_cot_fewshot_navigate                               |      4|get-answer       |     3|exact_match|â†‘  | 0.9760|Â±  |0.0097|
| - bbh_cot_fewshot_object_counting                        |      4|get-answer       |     3|exact_match|â†‘  | 0.9680|Â±  |0.0112|
| - bbh_cot_fewshot_penguins_in_a_table                    |      4|get-answer       |     3|exact_match|â†‘  | 0.8836|Â±  |0.0266|
| - bbh_cot_fewshot_reasoning_about_colored_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.9160|Â±  |0.0176|
| - bbh_cot_fewshot_ruin_names                             |      4|get-answer       |     3|exact_match|â†‘  | 0.7920|Â±  |0.0257|
| - bbh_cot_fewshot_salient_translation_error_detection    |      4|get-answer       |     3|exact_match|â†‘  | 0.5360|Â±  |0.0316|
| - bbh_cot_fewshot_snarks                                 |      4|get-answer       |     3|exact_match|â†‘  | 0.8371|Â±  |0.0278|
| - bbh_cot_fewshot_sports_understanding                   |      4|get-answer       |     3|exact_match|â†‘  | 0.9680|Â±  |0.0112|
| - bbh_cot_fewshot_temporal_sequences                     |      4|get-answer       |     3|exact_match|â†‘  | 0.6760|Â±  |0.0297|
| - bbh_cot_fewshot_tracking_shuffled_objects_five_objects |      4|get-answer       |     3|exact_match|â†‘  | 0.9720|Â±  |0.0105|
| - bbh_cot_fewshot_tracking_shuffled_objects_seven_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.7320|Â±  |0.0281|
| - bbh_cot_fewshot_tracking_shuffled_objects_three_objects|      4|get-answer       |     3|exact_match|â†‘  | 1.0000|Â±  |0.0000|
| - bbh_cot_fewshot_web_of_lies                            |      4|get-answer       |     3|exact_match|â†‘  | 1.0000|Â±  |0.0000|
| - bbh_cot_fewshot_word_sorting                           |      4|get-answer       |     3|exact_match|â†‘  | 0.5480|Â±  |0.0315|
|boolq                                                     |      2|none             |     0|acc        |â†‘  | 0.8746|Â±  |0.0058|
|drop                                                      |      3|none             |     0|em         |â†‘  | 0.0214|Â±  |0.0015|
|                                                          |       |none             |     0|f1         |â†‘  | 0.1396|Â±  |0.0023|
|gpqa_diamond_cot_n_shot                                   |      2|flexible-extract |     0|exact_match|â†‘  | 0.1616|Â±  |0.0262|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_cot_zeroshot                                 |      1|flexible-extract |     0|exact_match|â†‘  | 0.0909|Â±  |0.0205|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_generative_n_shot                            |      2|flexible-extract |     0|exact_match|â†‘  | 0.2374|Â±  |0.0303|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_n_shot                                       |      2|none             |     0|acc        |â†‘  | 0.3434|Â±  |0.0338|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3434|Â±  |0.0338|
|gpqa_diamond_zeroshot                                     |      1|none             |     0|acc        |â†‘  | 0.3384|Â±  |0.0337|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3384|Â±  |0.0337|
|gpqa_extended_cot_n_shot                                  |      2|flexible-extract |     0|exact_match|â†‘  | 0.1575|Â±  |0.0156|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_cot_zeroshot                                |      1|flexible-extract |     0|exact_match|â†‘  | 0.1410|Â±  |0.0149|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_generative_n_shot                           |      2|flexible-extract |     0|exact_match|â†‘  | 0.2436|Â±  |0.0184|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_n_shot                                      |      2|none             |     0|acc        |â†‘  | 0.3278|Â±  |0.0201|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3278|Â±  |0.0201|
|gpqa_extended_zeroshot                                    |      1|none             |     0|acc        |â†‘  | 0.3077|Â±  |0.0198|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3077|Â±  |0.0198|
|gpqa_main_cot_n_shot                                      |      2|flexible-extract |     0|exact_match|â†‘  | 0.1763|Â±  |0.0180|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_cot_zeroshot                                    |      1|flexible-extract |     0|exact_match|â†‘  | 0.1518|Â±  |0.0170|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_generative_n_shot                               |      2|flexible-extract |     0|exact_match|â†‘  | 0.2277|Â±  |0.0198|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_n_shot                                          |      2|none             |     0|acc        |â†‘  | 0.3371|Â±  |0.0224|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3371|Â±  |0.0224|
|gpqa_main_zeroshot                                        |      1|none             |     0|acc        |â†‘  | 0.3371|Â±  |0.0224|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3371|Â±  |0.0224|
|gsm8k                                                     |      3|flexible-extract |     5|exact_match|â†‘  | 0.8848|Â±  |0.0088|
|                                                          |       |strict-match     |     5|exact_match|â†‘  | 0.8772|Â±  |0.0090|
|hellaswag                                                 |      1|none             |     0|acc        |â†‘  | 0.6266|Â±  |0.0048|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.8188|Â±  |0.0038|
|mmlu                                                      |      2|none             |      |acc        |â†‘  | 0.7161|Â±  |0.0036|
| - humanities                                             |      2|none             |      |acc        |â†‘  | 0.6387|Â±  |0.0065|
|  - formal_logic                                          |      1|none             |     0|acc        |â†‘  | 0.5556|Â±  |0.0444|
|  - high_school_european_history                          |      1|none             |     0|acc        |â†‘  | 0.8606|Â±  |0.0270|
|  - high_school_us_history                                |      1|none             |     0|acc        |â†‘  | 0.8431|Â±  |0.0255|
|  - high_school_world_history                             |      1|none             |     0|acc        |â†‘  | 0.8945|Â±  |0.0200|
|  - international_law                                     |      1|none             |     0|acc        |â†‘  | 0.8595|Â±  |0.0317|
|  - jurisprudence                                         |      1|none             |     0|acc        |â†‘  | 0.8056|Â±  |0.0383|
|  - logical_fallacies                                     |      1|none             |     0|acc        |â†‘  | 0.8344|Â±  |0.0292|
|  - moral_disputes                                        |      1|none             |     0|acc        |â†‘  | 0.7717|Â±  |0.0226|
|  - moral_scenarios                                       |      1|none             |     0|acc        |â†‘  | 0.3676|Â±  |0.0161|
|  - philosophy                                            |      1|none             |     0|acc        |â†‘  | 0.7910|Â±  |0.0231|
|  - prehistory                                            |      1|none             |     0|acc        |â†‘  | 0.8148|Â±  |0.0216|
|  - professional_law                                      |      1|none             |     0|acc        |â†‘  | 0.5424|Â±  |0.0127|
|  - world_religions                                       |      1|none             |     0|acc        |â†‘  | 0.8421|Â±  |0.0280|
| - other                                                  |      2|none             |      |acc        |â†‘  | 0.7692|Â±  |0.0073|
|  - business_ethics                                       |      1|none             |     0|acc        |â†‘  | 0.7700|Â±  |0.0423|
|  - clinical_knowledge                                    |      1|none             |     0|acc        |â†‘  | 0.7962|Â±  |0.0248|
|  - college_medicine                                      |      1|none             |     0|acc        |â†‘  | 0.7225|Â±  |0.0341|
|  - global_facts                                          |      1|none             |     0|acc        |â†‘  | 0.4800|Â±  |0.0502|
|  - human_aging                                           |      1|none             |     0|acc        |â†‘  | 0.7668|Â±  |0.0284|
|  - management                                            |      1|none             |     0|acc        |â†‘  | 0.8350|Â±  |0.0368|
|  - marketing                                             |      1|none             |     0|acc        |â†‘  | 0.9017|Â±  |0.0195|
|  - medical_genetics                                      |      1|none             |     0|acc        |â†‘  | 0.8300|Â±  |0.0378|
|  - miscellaneous                                         |      1|none             |     0|acc        |â†‘  | 0.8608|Â±  |0.0124|
|  - nutrition                                             |      1|none             |     0|acc        |â†‘  | 0.7680|Â±  |0.0242|
|  - professional_accounting                               |      1|none             |     0|acc        |â†‘  | 0.5461|Â±  |0.0297|
|  - professional_medicine                                 |      1|none             |     0|acc        |â†‘  | 0.8088|Â±  |0.0239|
|  - virology                                              |      1|none             |     0|acc        |â†‘  | 0.5723|Â±  |0.0385|
| - social sciences                                        |      2|none             |      |acc        |â†‘  | 0.8213|Â±  |0.0068|
|  - econometrics                                          |      1|none             |     0|acc        |â†‘  | 0.6053|Â±  |0.0460|
|  - high_school_geography                                 |      1|none             |     0|acc        |â†‘  | 0.8535|Â±  |0.0252|
|  - high_school_government_and_politics                   |      1|none             |     0|acc        |â†‘  | 0.9326|Â±  |0.0181|
|  - high_school_macroeconomics                            |      1|none             |     0|acc        |â†‘  | 0.7821|Â±  |0.0209|
|  - high_school_microeconomics                            |      1|none             |     0|acc        |â†‘  | 0.8487|Â±  |0.0233|
|  - high_school_psychology                                |      1|none             |     0|acc        |â†‘  | 0.8954|Â±  |0.0131|
|  - human_sexuality                                       |      1|none             |     0|acc        |â†‘  | 0.8321|Â±  |0.0328|
|  - professional_psychology                               |      1|none             |     0|acc        |â†‘  | 0.7712|Â±  |0.0170|
|  - public_relations                                      |      1|none             |     0|acc        |â†‘  | 0.7091|Â±  |0.0435|
|  - security_studies                                      |      1|none             |     0|acc        |â†‘  | 0.7633|Â±  |0.0272|
|  - sociology                                             |      1|none             |     0|acc        |â†‘  | 0.8806|Â±  |0.0229|
|  - us_foreign_policy                                     |      1|none             |     0|acc        |â†‘  | 0.9100|Â±  |0.0288|
| - stem                                                   |      2|none             |      |acc        |â†‘  | 0.6768|Â±  |0.0080|
|  - abstract_algebra                                      |      1|none             |     0|acc        |â†‘  | 0.4300|Â±  |0.0498|
|  - anatomy                                               |      1|none             |     0|acc        |â†‘  | 0.7037|Â±  |0.0394|
|  - astronomy                                             |      1|none             |     0|acc        |â†‘  | 0.8487|Â±  |0.0292|
|  - college_biology                                       |      1|none             |     0|acc        |â†‘  | 0.8819|Â±  |0.0270|
|  - college_chemistry                                     |      1|none             |     0|acc        |â†‘  | 0.4600|Â±  |0.0501|
|  - college_computer_science                              |      1|none             |     0|acc        |â†‘  | 0.5900|Â±  |0.0494|
|  - college_mathematics                                   |      1|none             |     0|acc        |â†‘  | 0.5100|Â±  |0.0502|
|  - college_physics                                       |      1|none             |     0|acc        |â†‘  | 0.6275|Â±  |0.0481|
|  - computer_security                                     |      1|none             |     0|acc        |â†‘  | 0.8000|Â±  |0.0402|
|  - conceptual_physics                                    |      1|none             |     0|acc        |â†‘  | 0.7745|Â±  |0.0273|
|  - electrical_engineering                                |      1|none             |     0|acc        |â†‘  | 0.6690|Â±  |0.0392|
|  - elementary_mathematics                                |      1|none             |     0|acc        |â†‘  | 0.6455|Â±  |0.0246|
|  - high_school_biology                                   |      1|none             |     0|acc        |â†‘  | 0.8677|Â±  |0.0193|
|  - high_school_chemistry                                 |      1|none             |     0|acc        |â†‘  | 0.6847|Â±  |0.0327|
|  - high_school_computer_science                          |      1|none             |     0|acc        |â†‘  | 0.8300|Â±  |0.0378|
|  - high_school_mathematics                               |      1|none             |     0|acc        |â†‘  | 0.4926|Â±  |0.0305|
|  - high_school_physics                                   |      1|none             |     0|acc        |â†‘  | 0.5430|Â±  |0.0407|
|  - high_school_statistics                                |      1|none             |     0|acc        |â†‘  | 0.6713|Â±  |0.0320|
|  - machine_learning                                      |      1|none             |     0|acc        |â†‘  | 0.5893|Â±  |0.0467|
|nq_open                                                   |      4|remove_whitespace|     0|exact_match|â†‘  | 0.1571|Â±  |0.0061|
|openbookqa                                                |      1|none             |     0|acc        |â†‘  | 0.4220|Â±  |0.0221|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.4980|Â±  |0.0224|
|piqa                                                      |      1|none             |     0|acc        |â†‘  | 0.8014|Â±  |0.0093|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.7807|Â±  |0.0097|
|qnli                                                      |      1|none             |     0|acc        |â†‘  | 0.7457|Â±  |0.0059|
|sciq                                                      |      1|none             |     0|acc        |â†‘  | 0.9720|Â±  |0.0052|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.9540|Â±  |0.0066|
|triviaqa                                                  |      3|remove_whitespace|     0|exact_match|â†‘  | 0.2752|Â±  |0.0033|
|truthfulqa_gen                                            |      3|none             |     0|bleu_acc   |â†‘  | 0.4786|Â±  |0.0175|
|                                                          |       |none             |     0|bleu_diff  |â†‘  |-0.4518|Â±  |0.3853|
|                                                          |       |none             |     0|bleu_max   |â†‘  |12.5016|Â±  |0.5371|
|                                                          |       |none             |     0|rouge1_acc |â†‘  | 0.5141|Â±  |0.0175|
|                                                          |       |none             |     0|rouge1_diff|â†‘  |-0.2991|Â±  |0.5781|
|                                                          |       |none             |     0|rouge1_max |â†‘  |35.1025|Â±  |0.7280|
|                                                          |       |none             |     0|rouge2_acc |â†‘  | 0.4125|Â±  |0.0172|
|                                                          |       |none             |     0|rouge2_diff|â†‘  |-1.6691|Â±  |0.6548|
|                                                          |       |none             |     0|rouge2_max |â†‘  |20.2480|Â±  |0.7443|
|                                                          |       |none             |     0|rougeL_acc |â†‘  | 0.4957|Â±  |0.0175|
|                                                          |       |none             |     0|rougeL_diff|â†‘  |-0.8130|Â±  |0.5698|
|                                                          |       |none             |     0|rougeL_max |â†‘  |31.3688|Â±  |0.7201|
|truthfulqa_mc1                                            |      2|none             |     0|acc        |â†‘  | 0.4051|Â±  |0.0172|
|truthfulqa_mc2                                            |      3|none             |     0|acc        |â†‘  | 0.5812|Â±  |0.0160|
|winogrande                                                |      1|none             |     0|acc        |â†‘  | 0.7443|Â±  |0.0123|

|      Groups      |Version|  Filter  |n-shot|  Metric   |   |Value |   |Stderr|
|------------------|------:|----------|------|-----------|---|-----:|---|-----:|
|bbh               |      3|get-answer|      |exact_match|â†‘  |0.8019|Â±  |0.0044|
|mmlu              |      2|none      |      |acc        |â†‘  |0.7161|Â±  |0.0036|
| - humanities     |      2|none      |      |acc        |â†‘  |0.6387|Â±  |0.0065|
| - other          |      2|none      |      |acc        |â†‘  |0.7692|Â±  |0.0073|
| - social sciences|      2|none      |      |acc        |â†‘  |0.8213|Â±  |0.0068|
| - stem           |      2|none      |      |acc        |â†‘  |0.6768|Â±  |0.0080|

google_gemma-3-12b-it: 15h 46m 6s
âœ… Benchmark completed for google_gemma-3-12b-it

ðŸ”¥ Starting benchmark for meta-llama_Llama-2-13b-chat-hf
Passed argument batch_size = auto:1. Detecting largest batch size
Determined largest batch size: 1
Passed argument batch_size = auto. Detecting largest batch size
Determined Largest batch size: 1
hf (pretrained=/home/jaymin/Documents/llm/llm_models/meta-llama_Llama-2-13b-chat-hf), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto (1)
|                          Tasks                           |Version|     Filter      |n-shot|  Metric   |   | Value |   |Stderr|
|----------------------------------------------------------|------:|-----------------|-----:|-----------|---|------:|---|-----:|
|anli_r1                                                   |      1|none             |     0|acc        |â†‘  | 0.4300|Â±  |0.0157|
|anli_r2                                                   |      1|none             |     0|acc        |â†‘  | 0.4300|Â±  |0.0157|
|anli_r3                                                   |      1|none             |     0|acc        |â†‘  | 0.4142|Â±  |0.0142|
|arc_challenge                                             |      1|none             |     0|acc        |â†‘  | 0.4616|Â±  |0.0146|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.5017|Â±  |0.0146|
|bbh                                                       |      3|get-answer       |      |exact_match|â†‘  | 0.4780|Â±  |0.0055|
| - bbh_cot_fewshot_boolean_expressions                    |      4|get-answer       |     3|exact_match|â†‘  | 0.7200|Â±  |0.0285|
| - bbh_cot_fewshot_causal_judgement                       |      4|get-answer       |     3|exact_match|â†‘  | 0.5829|Â±  |0.0362|
| - bbh_cot_fewshot_date_understanding                     |      4|get-answer       |     3|exact_match|â†‘  | 0.6600|Â±  |0.0300|
| - bbh_cot_fewshot_disambiguation_qa                      |      4|get-answer       |     3|exact_match|â†‘  | 0.4560|Â±  |0.0316|
| - bbh_cot_fewshot_dyck_languages                         |      4|get-answer       |     3|exact_match|â†‘  | 0.0640|Â±  |0.0155|
| - bbh_cot_fewshot_formal_fallacies                       |      4|get-answer       |     3|exact_match|â†‘  | 0.5240|Â±  |0.0316|
| - bbh_cot_fewshot_geometric_shapes                       |      4|get-answer       |     3|exact_match|â†‘  | 0.4200|Â±  |0.0313|
| - bbh_cot_fewshot_hyperbaton                             |      4|get-answer       |     3|exact_match|â†‘  | 0.5840|Â±  |0.0312|
| - bbh_cot_fewshot_logical_deduction_five_objects         |      4|get-answer       |     3|exact_match|â†‘  | 0.3960|Â±  |0.0310|
| - bbh_cot_fewshot_logical_deduction_seven_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.3480|Â±  |0.0302|
| - bbh_cot_fewshot_logical_deduction_three_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.7320|Â±  |0.0281|
| - bbh_cot_fewshot_movie_recommendation                   |      4|get-answer       |     3|exact_match|â†‘  | 0.6640|Â±  |0.0299|
| - bbh_cot_fewshot_multistep_arithmetic_two               |      4|get-answer       |     3|exact_match|â†‘  | 0.0320|Â±  |0.0112|
| - bbh_cot_fewshot_navigate                               |      4|get-answer       |     3|exact_match|â†‘  | 0.6440|Â±  |0.0303|
| - bbh_cot_fewshot_object_counting                        |      4|get-answer       |     3|exact_match|â†‘  | 0.5640|Â±  |0.0314|
| - bbh_cot_fewshot_penguins_in_a_table                    |      4|get-answer       |     3|exact_match|â†‘  | 0.4452|Â±  |0.0413|
| - bbh_cot_fewshot_reasoning_about_colored_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.5440|Â±  |0.0316|
| - bbh_cot_fewshot_ruin_names                             |      4|get-answer       |     3|exact_match|â†‘  | 0.4120|Â±  |0.0312|
| - bbh_cot_fewshot_salient_translation_error_detection    |      4|get-answer       |     3|exact_match|â†‘  | 0.4400|Â±  |0.0315|
| - bbh_cot_fewshot_snarks                                 |      4|get-answer       |     3|exact_match|â†‘  | 0.7191|Â±  |0.0338|
| - bbh_cot_fewshot_sports_understanding                   |      4|get-answer       |     3|exact_match|â†‘  | 0.9280|Â±  |0.0164|
| - bbh_cot_fewshot_temporal_sequences                     |      4|get-answer       |     3|exact_match|â†‘  | 0.2000|Â±  |0.0253|
| - bbh_cot_fewshot_tracking_shuffled_objects_five_objects |      4|get-answer       |     3|exact_match|â†‘  | 0.2160|Â±  |0.0261|
| - bbh_cot_fewshot_tracking_shuffled_objects_seven_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.1840|Â±  |0.0246|
| - bbh_cot_fewshot_tracking_shuffled_objects_three_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.3520|Â±  |0.0303|
| - bbh_cot_fewshot_web_of_lies                            |      4|get-answer       |     3|exact_match|â†‘  | 0.9480|Â±  |0.0141|
| - bbh_cot_fewshot_word_sorting                           |      4|get-answer       |     3|exact_match|â†‘  | 0.2080|Â±  |0.0257|
|boolq                                                     |      2|none             |     0|acc        |â†‘  | 0.8165|Â±  |0.0068|
|drop                                                      |      3|none             |     0|em         |â†‘  | 0.0073|Â±  |0.0009|
|                                                          |       |none             |     0|f1         |â†‘  | 0.0915|Â±  |0.0020|
|gpqa_diamond_cot_n_shot                                   |      2|flexible-extract |     0|exact_match|â†‘  | 0.1212|Â±  |0.0233|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_cot_zeroshot                                 |      1|flexible-extract |     0|exact_match|â†‘  | 0.1414|Â±  |0.0248|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_generative_n_shot                            |      2|flexible-extract |     0|exact_match|â†‘  | 0.2071|Â±  |0.0289|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_n_shot                                       |      2|none             |     0|acc        |â†‘  | 0.2424|Â±  |0.0305|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2424|Â±  |0.0305|
|gpqa_diamond_zeroshot                                     |      1|none             |     0|acc        |â†‘  | 0.2222|Â±  |0.0296|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2222|Â±  |0.0296|
|gpqa_extended_cot_n_shot                                  |      2|flexible-extract |     0|exact_match|â†‘  | 0.1630|Â±  |0.0158|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_cot_zeroshot                                |      1|flexible-extract |     0|exact_match|â†‘  | 0.2033|Â±  |0.0172|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_generative_n_shot                           |      2|flexible-extract |     0|exact_match|â†‘  | 0.1960|Â±  |0.0170|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_n_shot                                      |      2|none             |     0|acc        |â†‘  | 0.2875|Â±  |0.0194|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2875|Â±  |0.0194|
|gpqa_extended_zeroshot                                    |      1|none             |     0|acc        |â†‘  | 0.2766|Â±  |0.0192|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2766|Â±  |0.0192|
|gpqa_main_cot_n_shot                                      |      2|flexible-extract |     0|exact_match|â†‘  | 0.1741|Â±  |0.0179|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_cot_zeroshot                                    |      1|flexible-extract |     0|exact_match|â†‘  | 0.1786|Â±  |0.0181|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_generative_n_shot                               |      2|flexible-extract |     0|exact_match|â†‘  | 0.1607|Â±  |0.0174|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_n_shot                                          |      2|none             |     0|acc        |â†‘  | 0.3192|Â±  |0.0220|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3192|Â±  |0.0220|
|gpqa_main_zeroshot                                        |      1|none             |     0|acc        |â†‘  | 0.2991|Â±  |0.0217|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2991|Â±  |0.0217|
|gsm8k                                                     |      3|flexible-extract |     5|exact_match|â†‘  | 0.3556|Â±  |0.0132|
|                                                          |       |strict-match     |     5|exact_match|â†‘  | 0.3472|Â±  |0.0131|
|hellaswag                                                 |      1|none             |     0|acc        |â†‘  | 0.6071|Â±  |0.0049|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.7967|Â±  |0.0040|
|mmlu                                                      |      2|none             |      |acc        |â†‘  | 0.5313|Â±  |0.0040|
| - humanities                                             |      2|none             |      |acc        |â†‘  | 0.4978|Â±  |0.0068|
|  - formal_logic                                          |      1|none             |     0|acc        |â†‘  | 0.2381|Â±  |0.0381|
|  - high_school_european_history                          |      1|none             |     0|acc        |â†‘  | 0.6667|Â±  |0.0368|
|  - high_school_us_history                                |      1|none             |     0|acc        |â†‘  | 0.7402|Â±  |0.0308|
|  - high_school_world_history                             |      1|none             |     0|acc        |â†‘  | 0.7257|Â±  |0.0290|
|  - international_law                                     |      1|none             |     0|acc        |â†‘  | 0.7190|Â±  |0.0410|
|  - jurisprudence                                         |      1|none             |     0|acc        |â†‘  | 0.6944|Â±  |0.0445|
|  - logical_fallacies                                     |      1|none             |     0|acc        |â†‘  | 0.6871|Â±  |0.0364|
|  - moral_disputes                                        |      1|none             |     0|acc        |â†‘  | 0.6012|Â±  |0.0264|
|  - moral_scenarios                                       |      1|none             |     0|acc        |â†‘  | 0.2760|Â±  |0.0150|
|  - philosophy                                            |      1|none             |     0|acc        |â†‘  | 0.6463|Â±  |0.0272|
|  - prehistory                                            |      1|none             |     0|acc        |â†‘  | 0.6265|Â±  |0.0269|
|  - professional_law                                      |      1|none             |     0|acc        |â†‘  | 0.4003|Â±  |0.0125|
|  - world_religions                                       |      1|none             |     0|acc        |â†‘  | 0.7719|Â±  |0.0322|
| - other                                                  |      2|none             |      |acc        |â†‘  | 0.6061|Â±  |0.0084|
|  - business_ethics                                       |      1|none             |     0|acc        |â†‘  | 0.5400|Â±  |0.0501|
|  - clinical_knowledge                                    |      1|none             |     0|acc        |â†‘  | 0.5887|Â±  |0.0303|
|  - college_medicine                                      |      1|none             |     0|acc        |â†‘  | 0.4220|Â±  |0.0377|
|  - global_facts                                          |      1|none             |     0|acc        |â†‘  | 0.3100|Â±  |0.0465|
|  - human_aging                                           |      1|none             |     0|acc        |â†‘  | 0.6233|Â±  |0.0325|
|  - management                                            |      1|none             |     0|acc        |â†‘  | 0.6893|Â±  |0.0458|
|  - marketing                                             |      1|none             |     0|acc        |â†‘  | 0.7991|Â±  |0.0262|
|  - medical_genetics                                      |      1|none             |     0|acc        |â†‘  | 0.5800|Â±  |0.0496|
|  - miscellaneous                                         |      1|none             |     0|acc        |â†‘  | 0.7663|Â±  |0.0151|
|  - nutrition                                             |      1|none             |     0|acc        |â†‘  | 0.6111|Â±  |0.0279|
|  - professional_accounting                               |      1|none             |     0|acc        |â†‘  | 0.4078|Â±  |0.0293|
|  - professional_medicine                                 |      1|none             |     0|acc        |â†‘  | 0.4963|Â±  |0.0304|
|  - virology                                              |      1|none             |     0|acc        |â†‘  | 0.4639|Â±  |0.0388|
| - social sciences                                        |      2|none             |      |acc        |â†‘  | 0.6136|Â±  |0.0085|
|  - econometrics                                          |      1|none             |     0|acc        |â†‘  | 0.2456|Â±  |0.0405|
|  - high_school_geography                                 |      1|none             |     0|acc        |â†‘  | 0.6515|Â±  |0.0339|
|  - high_school_government_and_politics                   |      1|none             |     0|acc        |â†‘  | 0.7565|Â±  |0.0310|
|  - high_school_macroeconomics                            |      1|none             |     0|acc        |â†‘  | 0.5026|Â±  |0.0254|
|  - high_school_microeconomics                            |      1|none             |     0|acc        |â†‘  | 0.5126|Â±  |0.0325|
|  - high_school_psychology                                |      1|none             |     0|acc        |â†‘  | 0.7156|Â±  |0.0193|
|  - human_sexuality                                       |      1|none             |     0|acc        |â†‘  | 0.6412|Â±  |0.0421|
|  - professional_psychology                               |      1|none             |     0|acc        |â†‘  | 0.5408|Â±  |0.0202|
|  - public_relations                                      |      1|none             |     0|acc        |â†‘  | 0.6273|Â±  |0.0463|
|  - security_studies                                      |      1|none             |     0|acc        |â†‘  | 0.6571|Â±  |0.0304|
|  - sociology                                             |      1|none             |     0|acc        |â†‘  | 0.7512|Â±  |0.0306|
|  - us_foreign_policy                                     |      1|none             |     0|acc        |â†‘  | 0.8100|Â±  |0.0394|
| - stem                                                   |      2|none             |      |acc        |â†‘  | 0.4272|Â±  |0.0085|
|  - abstract_algebra                                      |      1|none             |     0|acc        |â†‘  | 0.3000|Â±  |0.0461|
|  - anatomy                                               |      1|none             |     0|acc        |â†‘  | 0.5185|Â±  |0.0432|
|  - astronomy                                             |      1|none             |     0|acc        |â†‘  | 0.5789|Â±  |0.0402|
|  - college_biology                                       |      1|none             |     0|acc        |â†‘  | 0.5833|Â±  |0.0412|
|  - college_chemistry                                     |      1|none             |     0|acc        |â†‘  | 0.3400|Â±  |0.0476|
|  - college_computer_science                              |      1|none             |     0|acc        |â†‘  | 0.4400|Â±  |0.0499|
|  - college_mathematics                                   |      1|none             |     0|acc        |â†‘  | 0.2900|Â±  |0.0456|
|  - college_physics                                       |      1|none             |     0|acc        |â†‘  | 0.2745|Â±  |0.0444|
|  - computer_security                                     |      1|none             |     0|acc        |â†‘  | 0.6600|Â±  |0.0476|
|  - conceptual_physics                                    |      1|none             |     0|acc        |â†‘  | 0.4128|Â±  |0.0322|
|  - electrical_engineering                                |      1|none             |     0|acc        |â†‘  | 0.5448|Â±  |0.0415|
|  - elementary_mathematics                                |      1|none             |     0|acc        |â†‘  | 0.3201|Â±  |0.0240|
|  - high_school_biology                                   |      1|none             |     0|acc        |â†‘  | 0.6419|Â±  |0.0273|
|  - high_school_chemistry                                 |      1|none             |     0|acc        |â†‘  | 0.4286|Â±  |0.0348|
|  - high_school_computer_science                          |      1|none             |     0|acc        |â†‘  | 0.5500|Â±  |0.0500|
|  - high_school_mathematics                               |      1|none             |     0|acc        |â†‘  | 0.2741|Â±  |0.0272|
|  - high_school_physics                                   |      1|none             |     0|acc        |â†‘  | 0.3377|Â±  |0.0386|
|  - high_school_statistics                                |      1|none             |     0|acc        |â†‘  | 0.3426|Â±  |0.0324|
|  - machine_learning                                      |      1|none             |     0|acc        |â†‘  | 0.3304|Â±  |0.0446|
|nq_open                                                   |      4|remove_whitespace|     0|exact_match|â†‘  | 0.1030|Â±  |0.0051|
|openbookqa                                                |      1|none             |     0|acc        |â†‘  | 0.3520|Â±  |0.0214|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.4400|Â±  |0.0222|
|piqa                                                      |      1|none             |     0|acc        |â†‘  | 0.7780|Â±  |0.0097|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.7933|Â±  |0.0094|
|qnli                                                      |      1|none             |     0|acc        |â†‘  | 0.5438|Â±  |0.0067|
|sciq                                                      |      1|none             |     0|acc        |â†‘  | 0.9510|Â±  |0.0068|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.9050|Â±  |0.0093|
|triviaqa                                                  |      3|remove_whitespace|     0|exact_match|â†‘  | 0.2725|Â±  |0.0033|
|truthfulqa_gen                                            |      3|none             |     0|bleu_acc   |â†‘  | 0.4088|Â±  |0.0172|
|                                                          |       |none             |     0|bleu_diff  |â†‘  |-2.0148|Â±  |0.7195|
|                                                          |       |none             |     0|bleu_max   |â†‘  |26.0719|Â±  |0.7783|
|                                                          |       |none             |     0|rouge1_acc |â†‘  | 0.4235|Â±  |0.0173|
|                                                          |       |none             |     0|rouge1_diff|â†‘  |-3.1237|Â±  |0.8531|
|                                                          |       |none             |     0|rouge1_max |â†‘  |51.9853|Â±  |0.8214|
|                                                          |       |none             |     0|rouge2_acc |â†‘  | 0.3501|Â±  |0.0167|
|                                                          |       |none             |     0|rouge2_diff|â†‘  |-4.0918|Â±  |0.9904|
|                                                          |       |none             |     0|rouge2_max |â†‘  |36.4465|Â±  |0.9660|
|                                                          |       |none             |     0|rougeL_acc |â†‘  | 0.4186|Â±  |0.0173|
|                                                          |       |none             |     0|rougeL_diff|â†‘  |-3.1432|Â±  |0.8645|
|                                                          |       |none             |     0|rougeL_max |â†‘  |49.1291|Â±  |0.8443|
|truthfulqa_mc1                                            |      2|none             |     0|acc        |â†‘  | 0.2803|Â±  |0.0157|
|truthfulqa_mc2                                            |      3|none             |     0|acc        |â†‘  | 0.4396|Â±  |0.0157|
|winogrande                                                |      1|none             |     0|acc        |â†‘  | 0.7119|Â±  |0.0127|

|      Groups      |Version|  Filter  |n-shot|  Metric   |   |Value |   |Stderr|
|------------------|------:|----------|------|-----------|---|-----:|---|-----:|
|bbh               |      3|get-answer|      |exact_match|â†‘  |0.4780|Â±  |0.0055|
|mmlu              |      2|none      |      |acc        |â†‘  |0.5313|Â±  |0.0040|
| - humanities     |      2|none      |      |acc        |â†‘  |0.4978|Â±  |0.0068|
| - other          |      2|none      |      |acc        |â†‘  |0.6061|Â±  |0.0084|
| - social sciences|      2|none      |      |acc        |â†‘  |0.6136|Â±  |0.0085|
| - stem           |      2|none      |      |acc        |â†‘  |0.4272|Â±  |0.0085|

meta-llama_Llama-2-13b-chat-hf: 17h 9m 0s
âœ… Benchmark completed for meta-llama_Llama-2-13b-chat-hf

ðŸ”¥ Starting benchmark for meta-llama_Llama-2-13b-hf
Passed argument batch_size = auto:1. Detecting largest batch size
Determined largest batch size: 1
Passed argument batch_size = auto. Detecting largest batch size
Determined Largest batch size: 1
hf (pretrained=/home/jaymin/Documents/llm/llm_models/meta-llama_Llama-2-13b-hf), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto (1)
|                          Tasks                           |Version|     Filter      |n-shot|  Metric   |   | Value  |   |Stderr|
|----------------------------------------------------------|------:|-----------------|-----:|-----------|---|-------:|---|-----:|
|anli_r1                                                   |      1|none             |     0|acc        |â†‘  |  0.3770|Â±  |0.0153|
|anli_r2                                                   |      1|none             |     0|acc        |â†‘  |  0.3900|Â±  |0.0154|
|anli_r3                                                   |      1|none             |     0|acc        |â†‘  |  0.3850|Â±  |0.0141|
|arc_challenge                                             |      1|none             |     0|acc        |â†‘  |  0.4829|Â±  |0.0146|
|                                                          |       |none             |     0|acc_norm   |â†‘  |  0.4898|Â±  |0.0146|
|bbh                                                       |      3|get-answer       |      |exact_match|â†‘  |  0.4777|Â±  |0.0054|
| - bbh_cot_fewshot_boolean_expressions                    |      4|get-answer       |     3|exact_match|â†‘  |  0.7640|Â±  |0.0269|
| - bbh_cot_fewshot_causal_judgement                       |      4|get-answer       |     3|exact_match|â†‘  |  0.5401|Â±  |0.0365|
| - bbh_cot_fewshot_date_understanding                     |      4|get-answer       |     3|exact_match|â†‘  |  0.7240|Â±  |0.0283|
| - bbh_cot_fewshot_disambiguation_qa                      |      4|get-answer       |     3|exact_match|â†‘  |  0.4040|Â±  |0.0311|
| - bbh_cot_fewshot_dyck_languages                         |      4|get-answer       |     3|exact_match|â†‘  |  0.0440|Â±  |0.0130|
| - bbh_cot_fewshot_formal_fallacies                       |      4|get-answer       |     3|exact_match|â†‘  |  0.5120|Â±  |0.0317|
| - bbh_cot_fewshot_geometric_shapes                       |      4|get-answer       |     3|exact_match|â†‘  |  0.4040|Â±  |0.0311|
| - bbh_cot_fewshot_hyperbaton                             |      4|get-answer       |     3|exact_match|â†‘  |  0.5520|Â±  |0.0315|
| - bbh_cot_fewshot_logical_deduction_five_objects         |      4|get-answer       |     3|exact_match|â†‘  |  0.4640|Â±  |0.0316|
| - bbh_cot_fewshot_logical_deduction_seven_objects        |      4|get-answer       |     3|exact_match|â†‘  |  0.4160|Â±  |0.0312|
| - bbh_cot_fewshot_logical_deduction_three_objects        |      4|get-answer       |     3|exact_match|â†‘  |  0.6520|Â±  |0.0302|
| - bbh_cot_fewshot_movie_recommendation                   |      4|get-answer       |     3|exact_match|â†‘  |  0.7280|Â±  |0.0282|
| - bbh_cot_fewshot_multistep_arithmetic_two               |      4|get-answer       |     3|exact_match|â†‘  |  0.0240|Â±  |0.0097|
| - bbh_cot_fewshot_navigate                               |      4|get-answer       |     3|exact_match|â†‘  |  0.7400|Â±  |0.0278|
| - bbh_cot_fewshot_object_counting                        |      4|get-answer       |     3|exact_match|â†‘  |  0.5360|Â±  |0.0316|
| - bbh_cot_fewshot_penguins_in_a_table                    |      4|get-answer       |     3|exact_match|â†‘  |  0.4932|Â±  |0.0415|
| - bbh_cot_fewshot_reasoning_about_colored_objects        |      4|get-answer       |     3|exact_match|â†‘  |  0.6040|Â±  |0.0310|
| - bbh_cot_fewshot_ruin_names                             |      4|get-answer       |     3|exact_match|â†‘  |  0.4080|Â±  |0.0311|
| - bbh_cot_fewshot_salient_translation_error_detection    |      4|get-answer       |     3|exact_match|â†‘  |  0.2760|Â±  |0.0283|
| - bbh_cot_fewshot_snarks                                 |      4|get-answer       |     3|exact_match|â†‘  |  0.5506|Â±  |0.0374|
| - bbh_cot_fewshot_sports_understanding                   |      4|get-answer       |     3|exact_match|â†‘  |  0.9600|Â±  |0.0124|
| - bbh_cot_fewshot_temporal_sequences                     |      4|get-answer       |     3|exact_match|â†‘  |  0.2600|Â±  |0.0278|
| - bbh_cot_fewshot_tracking_shuffled_objects_five_objects |      4|get-answer       |     3|exact_match|â†‘  |  0.2160|Â±  |0.0261|
| - bbh_cot_fewshot_tracking_shuffled_objects_seven_objects|      4|get-answer       |     3|exact_match|â†‘  |  0.1680|Â±  |0.0237|
| - bbh_cot_fewshot_tracking_shuffled_objects_three_objects|      4|get-answer       |     3|exact_match|â†‘  |  0.3560|Â±  |0.0303|
| - bbh_cot_fewshot_web_of_lies                            |      4|get-answer       |     3|exact_match|â†‘  |  0.9480|Â±  |0.0141|
| - bbh_cot_fewshot_word_sorting                           |      4|get-answer       |     3|exact_match|â†‘  |  0.1960|Â±  |0.0252|
|boolq                                                     |      2|none             |     0|acc        |â†‘  |  0.8064|Â±  |0.0069|
|drop                                                      |      3|none             |     0|em         |â†‘  |  0.0033|Â±  |0.0006|
|                                                          |       |none             |     0|f1         |â†‘  |  0.0301|Â±  |0.0011|
|gpqa_diamond_cot_n_shot                                   |      2|flexible-extract |     0|exact_match|â†‘  |  0.1111|Â±  |0.0224|
|                                                          |       |strict-match     |     0|exact_match|â†‘  |  0.0152|Â±  |0.0087|
|gpqa_diamond_cot_zeroshot                                 |      1|flexible-extract |     0|exact_match|â†‘  |  0.1061|Â±  |0.0219|
|                                                          |       |strict-match     |     0|exact_match|â†‘  |  0.0000|Â±  |0.0000|
|gpqa_diamond_generative_n_shot                            |      2|flexible-extract |     0|exact_match|â†‘  |  0.2626|Â±  |0.0314|
|                                                          |       |strict-match     |     0|exact_match|â†‘  |  0.0000|Â±  |0.0000|
|gpqa_diamond_n_shot                                       |      2|none             |     0|acc        |â†‘  |  0.2727|Â±  |0.0317|
|                                                          |       |none             |     0|acc_norm   |â†‘  |  0.2727|Â±  |0.0317|
|gpqa_diamond_zeroshot                                     |      1|none             |     0|acc        |â†‘  |  0.2525|Â±  |0.0310|
|                                                          |       |none             |     0|acc_norm   |â†‘  |  0.2525|Â±  |0.0310|
|gpqa_extended_cot_n_shot                                  |      2|flexible-extract |     0|exact_match|â†‘  |  0.1282|Â±  |0.0143|
|                                                          |       |strict-match     |     0|exact_match|â†‘  |  0.0183|Â±  |0.0057|
|gpqa_extended_cot_zeroshot                                |      1|flexible-extract |     0|exact_match|â†‘  |  0.1410|Â±  |0.0149|
|                                                          |       |strict-match     |     0|exact_match|â†‘  |  0.0055|Â±  |0.0032|
|gpqa_extended_generative_n_shot                           |      2|flexible-extract |     0|exact_match|â†‘  |  0.2985|Â±  |0.0196|
|                                                          |       |strict-match     |     0|exact_match|â†‘  |  0.0000|Â±  |0.0000|
|gpqa_extended_n_shot                                      |      2|none             |     0|acc        |â†‘  |  0.2912|Â±  |0.0195|
|                                                          |       |none             |     0|acc_norm   |â†‘  |  0.2912|Â±  |0.0195|
|gpqa_extended_zeroshot                                    |      1|none             |     0|acc        |â†‘  |  0.2784|Â±  |0.0192|
|                                                          |       |none             |     0|acc_norm   |â†‘  |  0.2784|Â±  |0.0192|
|gpqa_main_cot_n_shot                                      |      2|flexible-extract |     0|exact_match|â†‘  |  0.1362|Â±  |0.0162|
|                                                          |       |strict-match     |     0|exact_match|â†‘  |  0.0134|Â±  |0.0054|
|gpqa_main_cot_zeroshot                                    |      1|flexible-extract |     0|exact_match|â†‘  |  0.1094|Â±  |0.0148|
|                                                          |       |strict-match     |     0|exact_match|â†‘  |  0.0045|Â±  |0.0032|
|gpqa_main_generative_n_shot                               |      2|flexible-extract |     0|exact_match|â†‘  |  0.2879|Â±  |0.0214|
|                                                          |       |strict-match     |     0|exact_match|â†‘  |  0.0000|Â±  |0.0000|
|gpqa_main_n_shot                                          |      2|none             |     0|acc        |â†‘  |  0.2879|Â±  |0.0214|
|                                                          |       |none             |     0|acc_norm   |â†‘  |  0.2879|Â±  |0.0214|
|gpqa_main_zeroshot                                        |      1|none             |     0|acc        |â†‘  |  0.2545|Â±  |0.0206|
|                                                          |       |none             |     0|acc_norm   |â†‘  |  0.2545|Â±  |0.0206|
|gsm8k                                                     |      3|flexible-extract |     5|exact_match|â†‘  |  0.2328|Â±  |0.0116|
|                                                          |       |strict-match     |     5|exact_match|â†‘  |  0.2297|Â±  |0.0116|
|hellaswag                                                 |      1|none             |     0|acc        |â†‘  |  0.6005|Â±  |0.0049|
|                                                          |       |none             |     0|acc_norm   |â†‘  |  0.7939|Â±  |0.0040|
|mmlu                                                      |      2|none             |      |acc        |â†‘  |  0.5209|Â±  |0.0040|
| - humanities                                             |      2|none             |      |acc        |â†‘  |  0.4795|Â±  |0.0069|
|  - formal_logic                                          |      1|none             |     0|acc        |â†‘  |  0.2857|Â±  |0.0404|
|  - high_school_european_history                          |      1|none             |     0|acc        |â†‘  |  0.6182|Â±  |0.0379|
|  - high_school_us_history                                |      1|none             |     0|acc        |â†‘  |  0.6716|Â±  |0.0330|
|  - high_school_world_history                             |      1|none             |     0|acc        |â†‘  |  0.7089|Â±  |0.0296|
|  - international_law                                     |      1|none             |     0|acc        |â†‘  |  0.7190|Â±  |0.0410|
|  - jurisprudence                                         |      1|none             |     0|acc        |â†‘  |  0.6481|Â±  |0.0462|
|  - logical_fallacies                                     |      1|none             |     0|acc        |â†‘  |  0.6319|Â±  |0.0379|
|  - moral_disputes                                        |      1|none             |     0|acc        |â†‘  |  0.5318|Â±  |0.0269|
|  - moral_scenarios                                       |      1|none             |     0|acc        |â†‘  |  0.2469|Â±  |0.0144|
|  - philosophy                                            |      1|none             |     0|acc        |â†‘  |  0.6431|Â±  |0.0272|
|  - prehistory                                            |      1|none             |     0|acc        |â†‘  |  0.6080|Â±  |0.0272|
|  - professional_law                                      |      1|none             |     0|acc        |â†‘  |  0.4048|Â±  |0.0125|
|  - world_religions                                       |      1|none             |     0|acc        |â†‘  |  0.7602|Â±  |0.0327|
| - other                                                  |      2|none             |      |acc        |â†‘  |  0.5935|Â±  |0.0085|
|  - business_ethics                                       |      1|none             |     0|acc        |â†‘  |  0.5100|Â±  |0.0502|
|  - clinical_knowledge                                    |      1|none             |     0|acc        |â†‘  |  0.5887|Â±  |0.0303|
|  - college_medicine                                      |      1|none             |     0|acc        |â†‘  |  0.4971|Â±  |0.0381|
|  - global_facts                                          |      1|none             |     0|acc        |â†‘  |  0.3200|Â±  |0.0469|
|  - human_aging                                           |      1|none             |     0|acc        |â†‘  |  0.5650|Â±  |0.0333|
|  - management                                            |      1|none             |     0|acc        |â†‘  |  0.7379|Â±  |0.0435|
|  - marketing                                             |      1|none             |     0|acc        |â†‘  |  0.7564|Â±  |0.0281|
|  - medical_genetics                                      |      1|none             |     0|acc        |â†‘  |  0.5500|Â±  |0.0500|
|  - miscellaneous                                         |      1|none             |     0|acc        |â†‘  |  0.7229|Â±  |0.0160|
|  - nutrition                                             |      1|none             |     0|acc        |â†‘  |  0.6209|Â±  |0.0278|
|  - professional_accounting                               |      1|none             |     0|acc        |â†‘  |  0.4043|Â±  |0.0293|
|  - professional_medicine                                 |      1|none             |     0|acc        |â†‘  |  0.5257|Â±  |0.0303|
|  - virology                                              |      1|none             |     0|acc        |â†‘  |  0.4337|Â±  |0.0386|
| - social sciences                                        |      2|none             |      |acc        |â†‘  |  0.6113|Â±  |0.0085|
|  - econometrics                                          |      1|none             |     0|acc        |â†‘  |  0.2281|Â±  |0.0395|
|  - high_school_geography                                 |      1|none             |     0|acc        |â†‘  |  0.6818|Â±  |0.0332|
|  - high_school_government_and_politics                   |      1|none             |     0|acc        |â†‘  |  0.7565|Â±  |0.0310|
|  - high_school_macroeconomics                            |      1|none             |     0|acc        |â†‘  |  0.4949|Â±  |0.0253|
|  - high_school_microeconomics                            |      1|none             |     0|acc        |â†‘  |  0.5378|Â±  |0.0324|
|  - high_school_psychology                                |      1|none             |     0|acc        |â†‘  |  0.7083|Â±  |0.0195|
|  - human_sexuality                                       |      1|none             |     0|acc        |â†‘  |  0.6641|Â±  |0.0414|
|  - professional_psychology                               |      1|none             |     0|acc        |â†‘  |  0.5278|Â±  |0.0202|
|  - public_relations                                      |      1|none             |     0|acc        |â†‘  |  0.6091|Â±  |0.0467|
|  - security_studies                                      |      1|none             |     0|acc        |â†‘  |  0.6449|Â±  |0.0306|
|  - sociology                                             |      1|none             |     0|acc        |â†‘  |  0.7512|Â±  |0.0306|
|  - us_foreign_policy                                     |      1|none             |     0|acc        |â†‘  |  0.8100|Â±  |0.0394|
| - stem                                                   |      2|none             |      |acc        |â†‘  |  0.4231|Â±  |0.0085|
|  - abstract_algebra                                      |      1|none             |     0|acc        |â†‘  |  0.2700|Â±  |0.0446|
|  - anatomy                                               |      1|none             |     0|acc        |â†‘  |  0.4815|Â±  |0.0432|
|  - astronomy                                             |      1|none             |     0|acc        |â†‘  |  0.5724|Â±  |0.0403|
|  - college_biology                                       |      1|none             |     0|acc        |â†‘  |  0.5208|Â±  |0.0418|
|  - college_chemistry                                     |      1|none             |     0|acc        |â†‘  |  0.4400|Â±  |0.0499|
|  - college_computer_science                              |      1|none             |     0|acc        |â†‘  |  0.3600|Â±  |0.0482|
|  - college_mathematics                                   |      1|none             |     0|acc        |â†‘  |  0.3200|Â±  |0.0469|
|  - college_physics                                       |      1|none             |     0|acc        |â†‘  |  0.2451|Â±  |0.0428|
|  - computer_security                                     |      1|none             |     0|acc        |â†‘  |  0.6500|Â±  |0.0479|
|  - conceptual_physics                                    |      1|none             |     0|acc        |â†‘  |  0.4043|Â±  |0.0321|
|  - electrical_engineering                                |      1|none             |     0|acc        |â†‘  |  0.5172|Â±  |0.0416|
|  - elementary_mathematics                                |      1|none             |     0|acc        |â†‘  |  0.3122|Â±  |0.0239|
|  - high_school_biology                                   |      1|none             |     0|acc        |â†‘  |  0.6516|Â±  |0.0271|
|  - high_school_chemistry                                 |      1|none             |     0|acc        |â†‘  |  0.4680|Â±  |0.0351|
|  - high_school_computer_science                          |      1|none             |     0|acc        |â†‘  |  0.5300|Â±  |0.0502|
|  - high_school_mathematics                               |      1|none             |     0|acc        |â†‘  |  0.2556|Â±  |0.0266|
|  - high_school_physics                                   |      1|none             |     0|acc        |â†‘  |  0.3179|Â±  |0.0380|
|  - high_school_statistics                                |      1|none             |     0|acc        |â†‘  |  0.4352|Â±  |0.0338|
|  - machine_learning                                      |      1|none             |     0|acc        |â†‘  |  0.2589|Â±  |0.0416|
|nq_open                                                   |      4|remove_whitespace|     0|exact_match|â†‘  |  0.2363|Â±  |0.0071|
|openbookqa                                                |      1|none             |     0|acc        |â†‘  |  0.3520|Â±  |0.0214|
|                                                          |       |none             |     0|acc_norm   |â†‘  |  0.4520|Â±  |0.0223|
|piqa                                                      |      1|none             |     0|acc        |â†‘  |  0.7900|Â±  |0.0095|
|                                                          |       |none             |     0|acc_norm   |â†‘  |  0.8052|Â±  |0.0092|
|qnli                                                      |      1|none             |     0|acc        |â†‘  |  0.4953|Â±  |0.0068|
|sciq                                                      |      1|none             |     0|acc        |â†‘  |  0.9460|Â±  |0.0072|
|                                                          |       |none             |     0|acc_norm   |â†‘  |  0.9350|Â±  |0.0078|
|triviaqa                                                  |      3|remove_whitespace|     0|exact_match|â†‘  |  0.6088|Â±  |0.0036|
|truthfulqa_gen                                            |      3|none             |     0|bleu_acc   |â†‘  |  0.3011|Â±  |0.0161|
|                                                          |       |none             |     0|bleu_diff  |â†‘  |-10.3037|Â±  |0.8896|
|                                                          |       |none             |     0|bleu_max   |â†‘  | 29.5100|Â±  |0.8236|
|                                                          |       |none             |     0|rouge1_acc |â†‘  |  0.3072|Â±  |0.0162|
|                                                          |       |none             |     0|rouge1_diff|â†‘  |-12.4090|Â±  |0.8679|
|                                                          |       |none             |     0|rouge1_max |â†‘  | 55.4793|Â±  |0.8343|
|                                                          |       |none             |     0|rouge2_acc |â†‘  |  0.2791|Â±  |0.0157|
|                                                          |       |none             |     0|rouge2_diff|â†‘  |-14.9613|Â±  |1.1075|
|                                                          |       |none             |     0|rouge2_max |â†‘  | 39.8908|Â±  |1.0021|
|                                                          |       |none             |     0|rougeL_acc |â†‘  |  0.2950|Â±  |0.0160|
|                                                          |       |none             |     0|rougeL_diff|â†‘  |-12.8909|Â±  |0.8812|
|                                                          |       |none             |     0|rougeL_max |â†‘  | 52.5536|Â±  |0.8487|
|truthfulqa_mc1                                            |      2|none             |     0|acc        |â†‘  |  0.2595|Â±  |0.0153|
|truthfulqa_mc2                                            |      3|none             |     0|acc        |â†‘  |  0.3690|Â±  |0.0136|
|winogrande                                                |      1|none             |     0|acc        |â†‘  |  0.7222|Â±  |0.0126|

|      Groups      |Version|  Filter  |n-shot|  Metric   |   |Value |   |Stderr|
|------------------|------:|----------|------|-----------|---|-----:|---|-----:|
|bbh               |      3|get-answer|      |exact_match|â†‘  |0.4777|Â±  |0.0054|
|mmlu              |      2|none      |      |acc        |â†‘  |0.5209|Â±  |0.0040|
| - humanities     |      2|none      |      |acc        |â†‘  |0.4795|Â±  |0.0069|
| - other          |      2|none      |      |acc        |â†‘  |0.5935|Â±  |0.0085|
| - social sciences|      2|none      |      |acc        |â†‘  |0.6113|Â±  |0.0085|
| - stem           |      2|none      |      |acc        |â†‘  |0.4231|Â±  |0.0085|

meta-llama_Llama-2-13b-hf: 19h 21m 36s
âœ… Benchmark completed for meta-llama_Llama-2-13b-hf

ðŸ”¥ Starting benchmark for Qwen_Qwen2-7B-Instruct
Passed argument batch_size = auto:1. Detecting largest batch size
Determined largest batch size: 1
Passed argument batch_size = auto. Detecting largest batch size
Determined Largest batch size: 1
hf (pretrained=/home/jaymin/Documents/llm/llm_models/Qwen_Qwen2-7B-Instruct), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto (1)
|                          Tasks                           |Version|     Filter      |n-shot|  Metric   |   | Value |   |Stderr|
|----------------------------------------------------------|------:|-----------------|-----:|-----------|---|------:|---|-----:|
|anli_r1                                                   |      1|none             |     0|acc        |â†‘  | 0.5730|Â±  |0.0156|
|anli_r2                                                   |      1|none             |     0|acc        |â†‘  | 0.5250|Â±  |0.0158|
|anli_r3                                                   |      1|none             |     0|acc        |â†‘  | 0.5225|Â±  |0.0144|
|arc_challenge                                             |      1|none             |     0|acc        |â†‘  | 0.5085|Â±  |0.0146|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.5401|Â±  |0.0146|
|bbh                                                       |      3|get-answer       |      |exact_match|â†‘  | 0.5775|Â±  |0.0054|
| - bbh_cot_fewshot_boolean_expressions                    |      4|get-answer       |     3|exact_match|â†‘  | 0.9360|Â±  |0.0155|
| - bbh_cot_fewshot_causal_judgement                       |      4|get-answer       |     3|exact_match|â†‘  | 0.4492|Â±  |0.0365|
| - bbh_cot_fewshot_date_understanding                     |      4|get-answer       |     3|exact_match|â†‘  | 0.5480|Â±  |0.0315|
| - bbh_cot_fewshot_disambiguation_qa                      |      4|get-answer       |     3|exact_match|â†‘  | 0.6400|Â±  |0.0304|
| - bbh_cot_fewshot_dyck_languages                         |      4|get-answer       |     3|exact_match|â†‘  | 0.0480|Â±  |0.0135|
| - bbh_cot_fewshot_formal_fallacies                       |      4|get-answer       |     3|exact_match|â†‘  | 0.5440|Â±  |0.0316|
| - bbh_cot_fewshot_geometric_shapes                       |      4|get-answer       |     3|exact_match|â†‘  | 0.3320|Â±  |0.0298|
| - bbh_cot_fewshot_hyperbaton                             |      4|get-answer       |     3|exact_match|â†‘  | 0.9000|Â±  |0.0190|
| - bbh_cot_fewshot_logical_deduction_five_objects         |      4|get-answer       |     3|exact_match|â†‘  | 0.4160|Â±  |0.0312|
| - bbh_cot_fewshot_logical_deduction_seven_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.2560|Â±  |0.0277|
| - bbh_cot_fewshot_logical_deduction_three_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.8240|Â±  |0.0241|
| - bbh_cot_fewshot_movie_recommendation                   |      4|get-answer       |     3|exact_match|â†‘  | 0.5680|Â±  |0.0314|
| - bbh_cot_fewshot_multistep_arithmetic_two               |      4|get-answer       |     3|exact_match|â†‘  | 0.8320|Â±  |0.0237|
| - bbh_cot_fewshot_navigate                               |      4|get-answer       |     3|exact_match|â†‘  | 0.9000|Â±  |0.0190|
| - bbh_cot_fewshot_object_counting                        |      4|get-answer       |     3|exact_match|â†‘  | 0.6280|Â±  |0.0306|
| - bbh_cot_fewshot_penguins_in_a_table                    |      4|get-answer       |     3|exact_match|â†‘  | 0.1918|Â±  |0.0327|
| - bbh_cot_fewshot_reasoning_about_colored_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.4800|Â±  |0.0317|
| - bbh_cot_fewshot_ruin_names                             |      4|get-answer       |     3|exact_match|â†‘  | 0.4720|Â±  |0.0316|
| - bbh_cot_fewshot_salient_translation_error_detection    |      4|get-answer       |     3|exact_match|â†‘  | 0.4960|Â±  |0.0317|
| - bbh_cot_fewshot_snarks                                 |      4|get-answer       |     3|exact_match|â†‘  | 0.6124|Â±  |0.0366|
| - bbh_cot_fewshot_sports_understanding                   |      4|get-answer       |     3|exact_match|â†‘  | 0.9080|Â±  |0.0183|
| - bbh_cot_fewshot_temporal_sequences                     |      4|get-answer       |     3|exact_match|â†‘  | 0.5680|Â±  |0.0314|
| - bbh_cot_fewshot_tracking_shuffled_objects_five_objects |      4|get-answer       |     3|exact_match|â†‘  | 0.5160|Â±  |0.0317|
| - bbh_cot_fewshot_tracking_shuffled_objects_seven_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.4560|Â±  |0.0316|
| - bbh_cot_fewshot_tracking_shuffled_objects_three_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.6840|Â±  |0.0295|
| - bbh_cot_fewshot_web_of_lies                            |      4|get-answer       |     3|exact_match|â†‘  | 0.9760|Â±  |0.0097|
| - bbh_cot_fewshot_word_sorting                           |      4|get-answer       |     3|exact_match|â†‘  | 0.2280|Â±  |0.0266|
|boolq                                                     |      2|none             |     0|acc        |â†‘  | 0.8563|Â±  |0.0061|
|drop                                                      |      3|none             |     0|em         |â†‘  | 0.0001|Â±  |0.0001|
|                                                          |       |none             |     0|f1         |â†‘  | 0.0520|Â±  |0.0012|
|gpqa_diamond_cot_n_shot                                   |      2|flexible-extract |     0|exact_match|â†‘  | 0.1364|Â±  |0.0245|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_cot_zeroshot                                 |      1|flexible-extract |     0|exact_match|â†‘  | 0.1364|Â±  |0.0245|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_generative_n_shot                            |      2|flexible-extract |     0|exact_match|â†‘  | 0.2222|Â±  |0.0296|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_n_shot                                       |      2|none             |     0|acc        |â†‘  | 0.3737|Â±  |0.0345|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3737|Â±  |0.0345|
|gpqa_diamond_zeroshot                                     |      1|none             |     0|acc        |â†‘  | 0.3182|Â±  |0.0332|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3182|Â±  |0.0332|
|gpqa_extended_cot_n_shot                                  |      2|flexible-extract |     0|exact_match|â†‘  | 0.1703|Â±  |0.0161|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_cot_zeroshot                                |      1|flexible-extract |     0|exact_match|â†‘  | 0.1410|Â±  |0.0149|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_generative_n_shot                           |      2|flexible-extract |     0|exact_match|â†‘  | 0.2015|Â±  |0.0172|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_n_shot                                      |      2|none             |     0|acc        |â†‘  | 0.3480|Â±  |0.0204|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3480|Â±  |0.0204|
|gpqa_extended_zeroshot                                    |      1|none             |     0|acc        |â†‘  | 0.3352|Â±  |0.0202|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3352|Â±  |0.0202|
|gpqa_main_cot_n_shot                                      |      2|flexible-extract |     0|exact_match|â†‘  | 0.1429|Â±  |0.0166|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_cot_zeroshot                                    |      1|flexible-extract |     0|exact_match|â†‘  | 0.1384|Â±  |0.0163|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_generative_n_shot                               |      2|flexible-extract |     0|exact_match|â†‘  | 0.1875|Â±  |0.0185|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_n_shot                                          |      2|none             |     0|acc        |â†‘  | 0.3125|Â±  |0.0219|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3125|Â±  |0.0219|
|gpqa_main_zeroshot                                        |      1|none             |     0|acc        |â†‘  | 0.3147|Â±  |0.0220|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3147|Â±  |0.0220|
|gsm8k                                                     |      3|flexible-extract |     5|exact_match|â†‘  | 0.7362|Â±  |0.0121|
|                                                          |       |strict-match     |     5|exact_match|â†‘  | 0.6467|Â±  |0.0132|
|hellaswag                                                 |      1|none             |     0|acc        |â†‘  | 0.6118|Â±  |0.0049|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.8060|Â±  |0.0039|
|mmlu                                                      |      2|none             |      |acc        |â†‘  | 0.6994|Â±  |0.0037|
| - humanities                                             |      2|none             |      |acc        |â†‘  | 0.6338|Â±  |0.0066|
|  - formal_logic                                          |      1|none             |     0|acc        |â†‘  | 0.5079|Â±  |0.0447|
|  - high_school_european_history                          |      1|none             |     0|acc        |â†‘  | 0.8061|Â±  |0.0309|
|  - high_school_us_history                                |      1|none             |     0|acc        |â†‘  | 0.8725|Â±  |0.0234|
|  - high_school_world_history                             |      1|none             |     0|acc        |â†‘  | 0.8397|Â±  |0.0239|
|  - international_law                                     |      1|none             |     0|acc        |â†‘  | 0.8264|Â±  |0.0346|
|  - jurisprudence                                         |      1|none             |     0|acc        |â†‘  | 0.8519|Â±  |0.0343|
|  - logical_fallacies                                     |      1|none             |     0|acc        |â†‘  | 0.8037|Â±  |0.0312|
|  - moral_disputes                                        |      1|none             |     0|acc        |â†‘  | 0.7717|Â±  |0.0226|
|  - moral_scenarios                                       |      1|none             |     0|acc        |â†‘  | 0.4324|Â±  |0.0166|
|  - philosophy                                            |      1|none             |     0|acc        |â†‘  | 0.7814|Â±  |0.0235|
|  - prehistory                                            |      1|none             |     0|acc        |â†‘  | 0.7840|Â±  |0.0229|
|  - professional_law                                      |      1|none             |     0|acc        |â†‘  | 0.5163|Â±  |0.0128|
|  - world_religions                                       |      1|none             |     0|acc        |â†‘  | 0.8304|Â±  |0.0288|
| - other                                                  |      2|none             |      |acc        |â†‘  | 0.7586|Â±  |0.0074|
|  - business_ethics                                       |      1|none             |     0|acc        |â†‘  | 0.7800|Â±  |0.0416|
|  - clinical_knowledge                                    |      1|none             |     0|acc        |â†‘  | 0.7849|Â±  |0.0253|
|  - college_medicine                                      |      1|none             |     0|acc        |â†‘  | 0.6879|Â±  |0.0353|
|  - global_facts                                          |      1|none             |     0|acc        |â†‘  | 0.4700|Â±  |0.0502|
|  - human_aging                                           |      1|none             |     0|acc        |â†‘  | 0.7489|Â±  |0.0291|
|  - management                                            |      1|none             |     0|acc        |â†‘  | 0.7961|Â±  |0.0399|
|  - marketing                                             |      1|none             |     0|acc        |â†‘  | 0.9017|Â±  |0.0195|
|  - medical_genetics                                      |      1|none             |     0|acc        |â†‘  | 0.8300|Â±  |0.0378|
|  - miscellaneous                                         |      1|none             |     0|acc        |â†‘  | 0.8570|Â±  |0.0125|
|  - nutrition                                             |      1|none             |     0|acc        |â†‘  | 0.7778|Â±  |0.0238|
|  - professional_accounting                               |      1|none             |     0|acc        |â†‘  | 0.5887|Â±  |0.0294|
|  - professional_medicine                                 |      1|none             |     0|acc        |â†‘  | 0.7353|Â±  |0.0268|
|  - virology                                              |      1|none             |     0|acc        |â†‘  | 0.5241|Â±  |0.0389|
| - social sciences                                        |      2|none             |      |acc        |â†‘  | 0.8021|Â±  |0.0071|
|  - econometrics                                          |      1|none             |     0|acc        |â†‘  | 0.5877|Â±  |0.0463|
|  - high_school_geography                                 |      1|none             |     0|acc        |â†‘  | 0.8788|Â±  |0.0233|
|  - high_school_government_and_politics                   |      1|none             |     0|acc        |â†‘  | 0.9275|Â±  |0.0187|
|  - high_school_macroeconomics                            |      1|none             |     0|acc        |â†‘  | 0.7692|Â±  |0.0214|
|  - high_school_microeconomics                            |      1|none             |     0|acc        |â†‘  | 0.8319|Â±  |0.0243|
|  - high_school_psychology                                |      1|none             |     0|acc        |â†‘  | 0.8642|Â±  |0.0147|
|  - human_sexuality                                       |      1|none             |     0|acc        |â†‘  | 0.7710|Â±  |0.0369|
|  - professional_psychology                               |      1|none             |     0|acc        |â†‘  | 0.7418|Â±  |0.0177|
|  - public_relations                                      |      1|none             |     0|acc        |â†‘  | 0.7364|Â±  |0.0422|
|  - security_studies                                      |      1|none             |     0|acc        |â†‘  | 0.7388|Â±  |0.0281|
|  - sociology                                             |      1|none             |     0|acc        |â†‘  | 0.8756|Â±  |0.0233|
|  - us_foreign_policy                                     |      1|none             |     0|acc        |â†‘  | 0.8600|Â±  |0.0349|
| - stem                                                   |      2|none             |      |acc        |â†‘  | 0.6388|Â±  |0.0083|
|  - abstract_algebra                                      |      1|none             |     0|acc        |â†‘  | 0.5200|Â±  |0.0502|
|  - anatomy                                               |      1|none             |     0|acc        |â†‘  | 0.6000|Â±  |0.0423|
|  - astronomy                                             |      1|none             |     0|acc        |â†‘  | 0.7697|Â±  |0.0343|
|  - college_biology                                       |      1|none             |     0|acc        |â†‘  | 0.7917|Â±  |0.0340|
|  - college_chemistry                                     |      1|none             |     0|acc        |â†‘  | 0.5100|Â±  |0.0502|
|  - college_computer_science                              |      1|none             |     0|acc        |â†‘  | 0.6200|Â±  |0.0488|
|  - college_mathematics                                   |      1|none             |     0|acc        |â†‘  | 0.4000|Â±  |0.0492|
|  - college_physics                                       |      1|none             |     0|acc        |â†‘  | 0.4020|Â±  |0.0488|
|  - computer_security                                     |      1|none             |     0|acc        |â†‘  | 0.7200|Â±  |0.0451|
|  - conceptual_physics                                    |      1|none             |     0|acc        |â†‘  | 0.7064|Â±  |0.0298|
|  - electrical_engineering                                |      1|none             |     0|acc        |â†‘  | 0.7103|Â±  |0.0378|
|  - elementary_mathematics                                |      1|none             |     0|acc        |â†‘  | 0.6376|Â±  |0.0248|
|  - high_school_biology                                   |      1|none             |     0|acc        |â†‘  | 0.8387|Â±  |0.0209|
|  - high_school_chemistry                                 |      1|none             |     0|acc        |â†‘  | 0.6207|Â±  |0.0341|
|  - high_school_computer_science                          |      1|none             |     0|acc        |â†‘  | 0.7800|Â±  |0.0416|
|  - high_school_mathematics                               |      1|none             |     0|acc        |â†‘  | 0.4963|Â±  |0.0305|
|  - high_school_physics                                   |      1|none             |     0|acc        |â†‘  | 0.5099|Â±  |0.0408|
|  - high_school_statistics                                |      1|none             |     0|acc        |â†‘  | 0.6759|Â±  |0.0319|
|  - machine_learning                                      |      1|none             |     0|acc        |â†‘  | 0.4732|Â±  |0.0474|
|nq_open                                                   |      4|remove_whitespace|     0|exact_match|â†‘  | 0.0133|Â±  |0.0019|
|openbookqa                                                |      1|none             |     0|acc        |â†‘  | 0.3460|Â±  |0.0213|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.4620|Â±  |0.0223|
|piqa                                                      |      1|none             |     0|acc        |â†‘  | 0.7954|Â±  |0.0094|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.8058|Â±  |0.0092|
|qnli                                                      |      1|none             |     0|acc        |â†‘  | 0.5471|Â±  |0.0067|
|sciq                                                      |      1|none             |     0|acc        |â†‘  | 0.9540|Â±  |0.0066|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.9160|Â±  |0.0088|
|triviaqa                                                  |      3|remove_whitespace|     0|exact_match|â†‘  | 0.0081|Â±  |0.0007|
|truthfulqa_gen                                            |      3|none             |     0|bleu_acc   |â†‘  | 0.4774|Â±  |0.0175|
|                                                          |       |none             |     0|bleu_diff  |â†‘  | 4.0052|Â±  |0.6796|
|                                                          |       |none             |     0|bleu_max   |â†‘  |19.4152|Â±  |0.7487|
|                                                          |       |none             |     0|rouge1_acc |â†‘  | 0.5043|Â±  |0.0175|
|                                                          |       |none             |     0|rouge1_diff|â†‘  | 5.0515|Â±  |0.9714|
|                                                          |       |none             |     0|rouge1_max |â†‘  |42.5509|Â±  |0.9066|
|                                                          |       |none             |     0|rouge2_acc |â†‘  | 0.4186|Â±  |0.0173|
|                                                          |       |none             |     0|rouge2_diff|â†‘  | 5.1321|Â±  |1.0491|
|                                                          |       |none             |     0|rouge2_max |â†‘  |29.4151|Â±  |0.9889|
|                                                          |       |none             |     0|rougeL_acc |â†‘  | 0.4908|Â±  |0.0175|
|                                                          |       |none             |     0|rougeL_diff|â†‘  | 5.0408|Â±  |0.9758|
|                                                          |       |none             |     0|rougeL_max |â†‘  |39.6681|Â±  |0.9155|
|truthfulqa_mc1                                            |      2|none             |     0|acc        |â†‘  | 0.4051|Â±  |0.0172|
|truthfulqa_mc2                                            |      3|none             |     0|acc        |â†‘  | 0.5734|Â±  |0.0154|
|winogrande                                                |      1|none             |     0|acc        |â†‘  | 0.6985|Â±  |0.0129|

|      Groups      |Version|  Filter  |n-shot|  Metric   |   |Value |   |Stderr|
|------------------|------:|----------|------|-----------|---|-----:|---|-----:|
|bbh               |      3|get-answer|      |exact_match|â†‘  |0.5775|Â±  |0.0054|
|mmlu              |      2|none      |      |acc        |â†‘  |0.6994|Â±  |0.0037|
| - humanities     |      2|none      |      |acc        |â†‘  |0.6338|Â±  |0.0066|
| - other          |      2|none      |      |acc        |â†‘  |0.7586|Â±  |0.0074|
| - social sciences|      2|none      |      |acc        |â†‘  |0.8021|Â±  |0.0071|
| - stem           |      2|none      |      |acc        |â†‘  |0.6388|Â±  |0.0083|

Qwen_Qwen2-7B-Instruct: 11h 30m 41s
âœ… Benchmark completed for Qwen_Qwen2-7B-Instruct

ðŸ”¥ Starting benchmark for deepseek-ai_DeepSeek-R1-0528-Qwen3-8B
Passed argument batch_size = auto:1. Detecting largest batch size
Determined largest batch size: 1
Passed argument batch_size = auto. Detecting largest batch size
Determined Largest batch size: 1
hf (pretrained=/home/jaymin/Documents/llm/llm_models/deepseek-ai_DeepSeek-R1-0528-Qwen3-8B), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto (1)
|                          Tasks                           |Version|     Filter      |n-shot|  Metric   |   | Value |   |Stderr|
|----------------------------------------------------------|------:|-----------------|-----:|-----------|---|------:|---|-----:|
|anli_r1                                                   |      1|none             |     0|acc        |â†‘  | 0.5110|Â±  |0.0158|
|anli_r2                                                   |      1|none             |     0|acc        |â†‘  | 0.4640|Â±  |0.0158|
|anli_r3                                                   |      1|none             |     0|acc        |â†‘  | 0.4767|Â±  |0.0144|
|arc_challenge                                             |      1|none             |     0|acc        |â†‘  | 0.5137|Â±  |0.0146|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.5495|Â±  |0.0145|
|bbh                                                       |      3|get-answer       |      |exact_match|â†‘  | 0.5841|Â±  |0.0052|
| - bbh_cot_fewshot_boolean_expressions                    |      4|get-answer       |     3|exact_match|â†‘  | 0.8920|Â±  |0.0197|
| - bbh_cot_fewshot_causal_judgement                       |      4|get-answer       |     3|exact_match|â†‘  | 0.5348|Â±  |0.0366|
| - bbh_cot_fewshot_date_understanding                     |      4|get-answer       |     3|exact_match|â†‘  | 0.5000|Â±  |0.0317|
| - bbh_cot_fewshot_disambiguation_qa                      |      4|get-answer       |     3|exact_match|â†‘  | 0.5520|Â±  |0.0315|
| - bbh_cot_fewshot_dyck_languages                         |      4|get-answer       |     3|exact_match|â†‘  | 0.1240|Â±  |0.0209|
| - bbh_cot_fewshot_formal_fallacies                       |      4|get-answer       |     3|exact_match|â†‘  | 0.3280|Â±  |0.0298|
| - bbh_cot_fewshot_geometric_shapes                       |      4|get-answer       |     3|exact_match|â†‘  | 0.3360|Â±  |0.0299|
| - bbh_cot_fewshot_hyperbaton                             |      4|get-answer       |     3|exact_match|â†‘  | 0.7440|Â±  |0.0277|
| - bbh_cot_fewshot_logical_deduction_five_objects         |      4|get-answer       |     3|exact_match|â†‘  | 0.0560|Â±  |0.0146|
| - bbh_cot_fewshot_logical_deduction_seven_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.0000|Â±  |0.0000|
| - bbh_cot_fewshot_logical_deduction_three_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.7080|Â±  |0.0288|
| - bbh_cot_fewshot_movie_recommendation                   |      4|get-answer       |     3|exact_match|â†‘  | 0.6200|Â±  |0.0308|
| - bbh_cot_fewshot_multistep_arithmetic_two               |      4|get-answer       |     3|exact_match|â†‘  | 0.9520|Â±  |0.0135|
| - bbh_cot_fewshot_navigate                               |      4|get-answer       |     3|exact_match|â†‘  | 0.8240|Â±  |0.0241|
| - bbh_cot_fewshot_object_counting                        |      4|get-answer       |     3|exact_match|â†‘  | 0.8640|Â±  |0.0217|
| - bbh_cot_fewshot_penguins_in_a_table                    |      4|get-answer       |     3|exact_match|â†‘  | 0.6918|Â±  |0.0383|
| - bbh_cot_fewshot_reasoning_about_colored_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.6000|Â±  |0.0310|
| - bbh_cot_fewshot_ruin_names                             |      4|get-answer       |     3|exact_match|â†‘  | 0.4880|Â±  |0.0317|
| - bbh_cot_fewshot_salient_translation_error_detection    |      4|get-answer       |     3|exact_match|â†‘  | 0.5840|Â±  |0.0312|
| - bbh_cot_fewshot_snarks                                 |      4|get-answer       |     3|exact_match|â†‘  | 0.5506|Â±  |0.0374|
| - bbh_cot_fewshot_sports_understanding                   |      4|get-answer       |     3|exact_match|â†‘  | 0.8680|Â±  |0.0215|
| - bbh_cot_fewshot_temporal_sequences                     |      4|get-answer       |     3|exact_match|â†‘  | 0.4960|Â±  |0.0317|
| - bbh_cot_fewshot_tracking_shuffled_objects_five_objects |      4|get-answer       |     3|exact_match|â†‘  | 0.7600|Â±  |0.0271|
| - bbh_cot_fewshot_tracking_shuffled_objects_seven_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.5240|Â±  |0.0316|
| - bbh_cot_fewshot_tracking_shuffled_objects_three_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.8400|Â±  |0.0232|
| - bbh_cot_fewshot_web_of_lies                            |      4|get-answer       |     3|exact_match|â†‘  | 0.9920|Â±  |0.0056|
| - bbh_cot_fewshot_word_sorting                           |      4|get-answer       |     3|exact_match|â†‘  | 0.3640|Â±  |0.0305|
|boolq                                                     |      2|none             |     0|acc        |â†‘  | 0.8483|Â±  |0.0063|
|drop                                                      |      3|none             |     0|em         |â†‘  | 0.0018|Â±  |0.0004|
|                                                          |       |none             |     0|f1         |â†‘  | 0.0533|Â±  |0.0013|
|gpqa_diamond_cot_n_shot                                   |      2|flexible-extract |     0|exact_match|â†‘  | 0.0556|Â±  |0.0163|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_cot_zeroshot                                 |      1|flexible-extract |     0|exact_match|â†‘  | 0.0556|Â±  |0.0163|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_generative_n_shot                            |      2|flexible-extract |     0|exact_match|â†‘  | 0.1667|Â±  |0.0266|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_n_shot                                       |      2|none             |     0|acc        |â†‘  | 0.3434|Â±  |0.0338|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3434|Â±  |0.0338|
|gpqa_diamond_zeroshot                                     |      1|none             |     0|acc        |â†‘  | 0.3283|Â±  |0.0335|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3283|Â±  |0.0335|
|gpqa_extended_cot_n_shot                                  |      2|flexible-extract |     0|exact_match|â†‘  | 0.0806|Â±  |0.0117|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_cot_zeroshot                                |      1|flexible-extract |     0|exact_match|â†‘  | 0.0861|Â±  |0.0120|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_generative_n_shot                           |      2|flexible-extract |     0|exact_match|â†‘  | 0.2106|Â±  |0.0175|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0018|Â±  |0.0018|
|gpqa_extended_n_shot                                      |      2|none             |     0|acc        |â†‘  | 0.3516|Â±  |0.0205|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3516|Â±  |0.0205|
|gpqa_extended_zeroshot                                    |      1|none             |     0|acc        |â†‘  | 0.3755|Â±  |0.0207|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3755|Â±  |0.0207|
|gpqa_main_cot_n_shot                                      |      2|flexible-extract |     0|exact_match|â†‘  | 0.0848|Â±  |0.0132|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_cot_zeroshot                                    |      1|flexible-extract |     0|exact_match|â†‘  | 0.0737|Â±  |0.0124|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_generative_n_shot                               |      2|flexible-extract |     0|exact_match|â†‘  | 0.2299|Â±  |0.0199|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_n_shot                                          |      2|none             |     0|acc        |â†‘  | 0.3683|Â±  |0.0228|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3683|Â±  |0.0228|
|gpqa_main_zeroshot                                        |      1|none             |     0|acc        |â†‘  | 0.3728|Â±  |0.0229|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3728|Â±  |0.0229|
|gsm8k                                                     |      3|flexible-extract |     5|exact_match|â†‘  | 0.8241|Â±  |0.0105|
|                                                          |       |strict-match     |     5|exact_match|â†‘  | 0.8127|Â±  |0.0107|
|hellaswag                                                 |      1|none             |     0|acc        |â†‘  | 0.5781|Â±  |0.0049|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.7564|Â±  |0.0043|
|mmlu                                                      |      2|none             |      |acc        |â†‘  | 0.6830|Â±  |0.0037|
| - humanities                                             |      2|none             |      |acc        |â†‘  | 0.5690|Â±  |0.0066|
|  - formal_logic                                          |      1|none             |     0|acc        |â†‘  | 0.6349|Â±  |0.0431|
|  - high_school_european_history                          |      1|none             |     0|acc        |â†‘  | 0.8121|Â±  |0.0305|
|  - high_school_us_history                                |      1|none             |     0|acc        |â†‘  | 0.7941|Â±  |0.0284|
|  - high_school_world_history                             |      1|none             |     0|acc        |â†‘  | 0.8354|Â±  |0.0241|
|  - international_law                                     |      1|none             |     0|acc        |â†‘  | 0.7521|Â±  |0.0394|
|  - jurisprudence                                         |      1|none             |     0|acc        |â†‘  | 0.7685|Â±  |0.0408|
|  - logical_fallacies                                     |      1|none             |     0|acc        |â†‘  | 0.7607|Â±  |0.0335|
|  - moral_disputes                                        |      1|none             |     0|acc        |â†‘  | 0.7139|Â±  |0.0243|
|  - moral_scenarios                                       |      1|none             |     0|acc        |â†‘  | 0.2793|Â±  |0.0150|
|  - philosophy                                            |      1|none             |     0|acc        |â†‘  | 0.7267|Â±  |0.0253|
|  - prehistory                                            |      1|none             |     0|acc        |â†‘  | 0.7747|Â±  |0.0232|
|  - professional_law                                      |      1|none             |     0|acc        |â†‘  | 0.4531|Â±  |0.0127|
|  - world_religions                                       |      1|none             |     0|acc        |â†‘  | 0.7953|Â±  |0.0309|
| - other                                                  |      2|none             |      |acc        |â†‘  | 0.7399|Â±  |0.0076|
|  - business_ethics                                       |      1|none             |     0|acc        |â†‘  | 0.7400|Â±  |0.0441|
|  - clinical_knowledge                                    |      1|none             |     0|acc        |â†‘  | 0.7509|Â±  |0.0266|
|  - college_medicine                                      |      1|none             |     0|acc        |â†‘  | 0.7110|Â±  |0.0346|
|  - global_facts                                          |      1|none             |     0|acc        |â†‘  | 0.4700|Â±  |0.0502|
|  - human_aging                                           |      1|none             |     0|acc        |â†‘  | 0.7085|Â±  |0.0305|
|  - management                                            |      1|none             |     0|acc        |â†‘  | 0.8835|Â±  |0.0318|
|  - marketing                                             |      1|none             |     0|acc        |â†‘  | 0.8632|Â±  |0.0225|
|  - medical_genetics                                      |      1|none             |     0|acc        |â†‘  | 0.7500|Â±  |0.0435|
|  - miscellaneous                                         |      1|none             |     0|acc        |â†‘  | 0.8327|Â±  |0.0133|
|  - nutrition                                             |      1|none             |     0|acc        |â†‘  | 0.7614|Â±  |0.0244|
|  - professional_accounting                               |      1|none             |     0|acc        |â†‘  | 0.5567|Â±  |0.0296|
|  - professional_medicine                                 |      1|none             |     0|acc        |â†‘  | 0.7610|Â±  |0.0259|
|  - virology                                              |      1|none             |     0|acc        |â†‘  | 0.4880|Â±  |0.0389|
| - social sciences                                        |      2|none             |      |acc        |â†‘  | 0.7927|Â±  |0.0072|
|  - econometrics                                          |      1|none             |     0|acc        |â†‘  | 0.5965|Â±  |0.0462|
|  - high_school_geography                                 |      1|none             |     0|acc        |â†‘  | 0.8384|Â±  |0.0262|
|  - high_school_government_and_politics                   |      1|none             |     0|acc        |â†‘  | 0.9016|Â±  |0.0215|
|  - high_school_macroeconomics                            |      1|none             |     0|acc        |â†‘  | 0.7513|Â±  |0.0219|
|  - high_school_microeconomics                            |      1|none             |     0|acc        |â†‘  | 0.8739|Â±  |0.0216|
|  - high_school_psychology                                |      1|none             |     0|acc        |â†‘  | 0.8716|Â±  |0.0143|
|  - human_sexuality                                       |      1|none             |     0|acc        |â†‘  | 0.8397|Â±  |0.0322|
|  - professional_psychology                               |      1|none             |     0|acc        |â†‘  | 0.7059|Â±  |0.0184|
|  - public_relations                                      |      1|none             |     0|acc        |â†‘  | 0.7182|Â±  |0.0431|
|  - security_studies                                      |      1|none             |     0|acc        |â†‘  | 0.7551|Â±  |0.0275|
|  - sociology                                             |      1|none             |     0|acc        |â†‘  | 0.8060|Â±  |0.0280|
|  - us_foreign_policy                                     |      1|none             |     0|acc        |â†‘  | 0.8700|Â±  |0.0338|
| - stem                                                   |      2|none             |      |acc        |â†‘  | 0.6898|Â±  |0.0079|
|  - abstract_algebra                                      |      1|none             |     0|acc        |â†‘  | 0.4900|Â±  |0.0502|
|  - anatomy                                               |      1|none             |     0|acc        |â†‘  | 0.6815|Â±  |0.0402|
|  - astronomy                                             |      1|none             |     0|acc        |â†‘  | 0.8816|Â±  |0.0263|
|  - college_biology                                       |      1|none             |     0|acc        |â†‘  | 0.8681|Â±  |0.0283|
|  - college_chemistry                                     |      1|none             |     0|acc        |â†‘  | 0.5600|Â±  |0.0499|
|  - college_computer_science                              |      1|none             |     0|acc        |â†‘  | 0.6100|Â±  |0.0490|
|  - college_mathematics                                   |      1|none             |     0|acc        |â†‘  | 0.4800|Â±  |0.0502|
|  - college_physics                                       |      1|none             |     0|acc        |â†‘  | 0.5588|Â±  |0.0494|
|  - computer_security                                     |      1|none             |     0|acc        |â†‘  | 0.7500|Â±  |0.0435|
|  - conceptual_physics                                    |      1|none             |     0|acc        |â†‘  | 0.8170|Â±  |0.0253|
|  - electrical_engineering                                |      1|none             |     0|acc        |â†‘  | 0.7517|Â±  |0.0360|
|  - elementary_mathematics                                |      1|none             |     0|acc        |â†‘  | 0.6614|Â±  |0.0244|
|  - high_school_biology                                   |      1|none             |     0|acc        |â†‘  | 0.9000|Â±  |0.0171|
|  - high_school_chemistry                                 |      1|none             |     0|acc        |â†‘  | 0.6700|Â±  |0.0331|
|  - high_school_computer_science                          |      1|none             |     0|acc        |â†‘  | 0.8200|Â±  |0.0386|
|  - high_school_mathematics                               |      1|none             |     0|acc        |â†‘  | 0.4778|Â±  |0.0305|
|  - high_school_physics                                   |      1|none             |     0|acc        |â†‘  | 0.6424|Â±  |0.0391|
|  - high_school_statistics                                |      1|none             |     0|acc        |â†‘  | 0.6852|Â±  |0.0317|
|  - machine_learning                                      |      1|none             |     0|acc        |â†‘  | 0.5000|Â±  |0.0475|
|nq_open                                                   |      4|remove_whitespace|     0|exact_match|â†‘  | 0.0183|Â±  |0.0022|
|openbookqa                                                |      1|none             |     0|acc        |â†‘  | 0.3080|Â±  |0.0207|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.4300|Â±  |0.0222|
|piqa                                                      |      1|none             |     0|acc        |â†‘  | 0.7633|Â±  |0.0099|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.7568|Â±  |0.0100|
|qnli                                                      |      1|none             |     0|acc        |â†‘  | 0.5578|Â±  |0.0067|
|sciq                                                      |      1|none             |     0|acc        |â†‘  | 0.9600|Â±  |0.0062|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.9410|Â±  |0.0075|
|triviaqa                                                  |      3|remove_whitespace|     0|exact_match|â†‘  | 0.0295|Â±  |0.0013|
|truthfulqa_gen                                            |      3|none             |     0|bleu_acc   |â†‘  | 0.5398|Â±  |0.0174|
|                                                          |       |none             |     0|bleu_diff  |â†‘  | 5.8931|Â±  |0.7222|
|                                                          |       |none             |     0|bleu_max   |â†‘  |19.7647|Â±  |0.7053|
|                                                          |       |none             |     0|rouge1_acc |â†‘  | 0.5569|Â±  |0.0174|
|                                                          |       |none             |     0|rouge1_diff|â†‘  | 9.9292|Â±  |1.0724|
|                                                          |       |none             |     0|rouge1_max |â†‘  |45.0401|Â±  |0.8645|
|                                                          |       |none             |     0|rouge2_acc |â†‘  | 0.4627|Â±  |0.0175|
|                                                          |       |none             |     0|rouge2_diff|â†‘  | 9.8762|Â±  |1.1402|
|                                                          |       |none             |     0|rouge2_max |â†‘  |30.6518|Â±  |0.9760|
|                                                          |       |none             |     0|rougeL_acc |â†‘  | 0.5435|Â±  |0.0174|
|                                                          |       |none             |     0|rougeL_diff|â†‘  | 9.8078|Â±  |1.0753|
|                                                          |       |none             |     0|rougeL_max |â†‘  |41.9636|Â±  |0.8847|
|truthfulqa_mc1                                            |      2|none             |     0|acc        |â†‘  | 0.3574|Â±  |0.0168|
|truthfulqa_mc2                                            |      3|none             |     0|acc        |â†‘  | 0.5590|Â±  |0.0152|
|winogrande                                                |      1|none             |     0|acc        |â†‘  | 0.6756|Â±  |0.0132|

|      Groups      |Version|  Filter  |n-shot|  Metric   |   |Value |   |Stderr|
|------------------|------:|----------|------|-----------|---|-----:|---|-----:|
|bbh               |      3|get-answer|      |exact_match|â†‘  |0.5841|Â±  |0.0052|
|mmlu              |      2|none      |      |acc        |â†‘  |0.6830|Â±  |0.0037|
| - humanities     |      2|none      |      |acc        |â†‘  |0.5690|Â±  |0.0066|
| - other          |      2|none      |      |acc        |â†‘  |0.7399|Â±  |0.0076|
| - social sciences|      2|none      |      |acc        |â†‘  |0.7927|Â±  |0.0072|
| - stem           |      2|none      |      |acc        |â†‘  |0.6898|Â±  |0.0079|

deepseek-ai_DeepSeek-R1-0528-Qwen3-8B: 17h 58m 4s
âœ… Benchmark completed for deepseek-ai_DeepSeek-R1-0528-Qwen3-8B

ðŸ”¥ Starting benchmark for 01-ai_Yi-1.5-9B-Chat
2025-07-27:12:03:18 INFO     [loggers.evaluation_tracker:209] Saving results aggregated
hf (pretrained=/home/jaymin/Documents/llm/llm_models/01-ai_Yi-1.5-9B-Chat), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 2
|                          Tasks                           |Version|     Filter      |n-shot|  Metric   |   | Value |   |Stderr|
|----------------------------------------------------------|------:|-----------------|-----:|-----------|---|------:|---|-----:|
|anli_r1                                                   |      1|none             |     0|acc        |â†‘  | 0.5350|Â±  |0.0158|
|anli_r2                                                   |      1|none             |     0|acc        |â†‘  | 0.5090|Â±  |0.0158|
|anli_r3                                                   |      1|none             |     0|acc        |â†‘  | 0.5258|Â±  |0.0144|
|arc_challenge                                             |      1|none             |     0|acc        |â†‘  | 0.5572|Â±  |0.0145|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.5870|Â±  |0.0144|
|bbh                                                       |      3|get-answer       |      |exact_match|â†‘  | 0.6107|Â±  |0.0053|
| - bbh_cot_fewshot_boolean_expressions                    |      4|get-answer       |     3|exact_match|â†‘  | 0.8960|Â±  |0.0193|
| - bbh_cot_fewshot_causal_judgement                       |      4|get-answer       |     3|exact_match|â†‘  | 0.5508|Â±  |0.0365|
| - bbh_cot_fewshot_date_understanding                     |      4|get-answer       |     3|exact_match|â†‘  | 0.7760|Â±  |0.0264|
| - bbh_cot_fewshot_disambiguation_qa                      |      4|get-answer       |     3|exact_match|â†‘  | 0.3040|Â±  |0.0292|
| - bbh_cot_fewshot_dyck_languages                         |      4|get-answer       |     3|exact_match|â†‘  | 0.3640|Â±  |0.0305|
| - bbh_cot_fewshot_formal_fallacies                       |      4|get-answer       |     3|exact_match|â†‘  | 0.0240|Â±  |0.0097|
| - bbh_cot_fewshot_geometric_shapes                       |      4|get-answer       |     3|exact_match|â†‘  | 0.5880|Â±  |0.0312|
| - bbh_cot_fewshot_hyperbaton                             |      4|get-answer       |     3|exact_match|â†‘  | 0.6040|Â±  |0.0310|
| - bbh_cot_fewshot_logical_deduction_five_objects         |      4|get-answer       |     3|exact_match|â†‘  | 0.5560|Â±  |0.0315|
| - bbh_cot_fewshot_logical_deduction_seven_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.3640|Â±  |0.0305|
| - bbh_cot_fewshot_logical_deduction_three_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.9120|Â±  |0.0180|
| - bbh_cot_fewshot_movie_recommendation                   |      4|get-answer       |     3|exact_match|â†‘  | 0.7560|Â±  |0.0272|
| - bbh_cot_fewshot_multistep_arithmetic_two               |      4|get-answer       |     3|exact_match|â†‘  | 0.7600|Â±  |0.0271|
| - bbh_cot_fewshot_navigate                               |      4|get-answer       |     3|exact_match|â†‘  | 0.2680|Â±  |0.0281|
| - bbh_cot_fewshot_object_counting                        |      4|get-answer       |     3|exact_match|â†‘  | 0.8600|Â±  |0.0220|
| - bbh_cot_fewshot_penguins_in_a_table                    |      4|get-answer       |     3|exact_match|â†‘  | 0.8356|Â±  |0.0308|
| - bbh_cot_fewshot_reasoning_about_colored_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.6640|Â±  |0.0299|
| - bbh_cot_fewshot_ruin_names                             |      4|get-answer       |     3|exact_match|â†‘  | 0.3760|Â±  |0.0307|
| - bbh_cot_fewshot_salient_translation_error_detection    |      4|get-answer       |     3|exact_match|â†‘  | 0.5000|Â±  |0.0317|
| - bbh_cot_fewshot_snarks                                 |      4|get-answer       |     3|exact_match|â†‘  | 0.6854|Â±  |0.0349|
| - bbh_cot_fewshot_sports_understanding                   |      4|get-answer       |     3|exact_match|â†‘  | 0.9160|Â±  |0.0176|
| - bbh_cot_fewshot_temporal_sequences                     |      4|get-answer       |     3|exact_match|â†‘  | 0.7160|Â±  |0.0286|
| - bbh_cot_fewshot_tracking_shuffled_objects_five_objects |      4|get-answer       |     3|exact_match|â†‘  | 0.7280|Â±  |0.0282|
| - bbh_cot_fewshot_tracking_shuffled_objects_seven_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.5800|Â±  |0.0313|
| - bbh_cot_fewshot_tracking_shuffled_objects_three_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.6600|Â±  |0.0300|
| - bbh_cot_fewshot_web_of_lies                            |      4|get-answer       |     3|exact_match|â†‘  | 0.9720|Â±  |0.0105|
| - bbh_cot_fewshot_word_sorting                           |      4|get-answer       |     3|exact_match|â†‘  | 0.3720|Â±  |0.0306|
|boolq                                                     |      2|none             |     0|acc        |â†‘  | 0.8682|Â±  |0.0059|
|drop                                                      |      3|none             |     0|em         |â†‘  | 0.0149|Â±  |0.0012|
|                                                          |       |none             |     0|f1         |â†‘  | 0.1253|Â±  |0.0021|
|gpqa_diamond_cot_n_shot                                   |      2|flexible-extract |     0|exact_match|â†‘  | 0.1313|Â±  |0.0241|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_cot_zeroshot                                 |      1|flexible-extract |     0|exact_match|â†‘  | 0.1414|Â±  |0.0248|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_generative_n_shot                            |      2|flexible-extract |     0|exact_match|â†‘  | 0.1667|Â±  |0.0266|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_n_shot                                       |      2|none             |     0|acc        |â†‘  | 0.3384|Â±  |0.0337|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3384|Â±  |0.0337|
|gpqa_diamond_zeroshot                                     |      1|none             |     0|acc        |â†‘  | 0.3131|Â±  |0.0330|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3131|Â±  |0.0330|
|gpqa_extended_cot_n_shot                                  |      2|flexible-extract |     0|exact_match|â†‘  | 0.1795|Â±  |0.0164|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_cot_zeroshot                                |      1|flexible-extract |     0|exact_match|â†‘  | 0.1777|Â±  |0.0164|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_generative_n_shot                           |      2|flexible-extract |     0|exact_match|â†‘  | 0.1960|Â±  |0.0170|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_n_shot                                      |      2|none             |     0|acc        |â†‘  | 0.3187|Â±  |0.0200|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3187|Â±  |0.0200|
|gpqa_extended_zeroshot                                    |      1|none             |     0|acc        |â†‘  | 0.3315|Â±  |0.0202|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3315|Â±  |0.0202|
|gpqa_main_cot_n_shot                                      |      2|flexible-extract |     0|exact_match|â†‘  | 0.1942|Â±  |0.0187|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_cot_zeroshot                                    |      1|flexible-extract |     0|exact_match|â†‘  | 0.1920|Â±  |0.0186|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_generative_n_shot                               |      2|flexible-extract |     0|exact_match|â†‘  | 0.2098|Â±  |0.0193|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_n_shot                                          |      2|none             |     0|acc        |â†‘  | 0.3237|Â±  |0.0221|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3237|Â±  |0.0221|
|gpqa_main_zeroshot                                        |      1|none             |     0|acc        |â†‘  | 0.3036|Â±  |0.0217|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3036|Â±  |0.0217|
|gsm8k                                                     |      3|flexible-extract |     5|exact_match|â†‘  | 0.6732|Â±  |0.0129|
|                                                          |       |strict-match     |     5|exact_match|â†‘  | 0.7081|Â±  |0.0125|
|hellaswag                                                 |      1|none             |     0|acc        |â†‘  | 0.5964|Â±  |0.0049|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.7873|Â±  |0.0041|
|mmlu                                                      |      2|none             |      |acc        |â†‘  | 0.6841|Â±  |0.0037|
| - humanities                                             |      2|none             |      |acc        |â†‘  | 0.6172|Â±  |0.0067|
|  - formal_logic                                          |      1|none             |     0|acc        |â†‘  | 0.5556|Â±  |0.0444|
|  - high_school_european_history                          |      1|none             |     0|acc        |â†‘  | 0.8303|Â±  |0.0293|
|  - high_school_us_history                                |      1|none             |     0|acc        |â†‘  | 0.8676|Â±  |0.0238|
|  - high_school_world_history                             |      1|none             |     0|acc        |â†‘  | 0.8354|Â±  |0.0241|
|  - international_law                                     |      1|none             |     0|acc        |â†‘  | 0.8099|Â±  |0.0358|
|  - jurisprudence                                         |      1|none             |     0|acc        |â†‘  | 0.7778|Â±  |0.0402|
|  - logical_fallacies                                     |      1|none             |     0|acc        |â†‘  | 0.7975|Â±  |0.0316|
|  - moral_disputes                                        |      1|none             |     0|acc        |â†‘  | 0.7283|Â±  |0.0239|
|  - moral_scenarios                                       |      1|none             |     0|acc        |â†‘  | 0.4547|Â±  |0.0167|
|  - philosophy                                            |      1|none             |     0|acc        |â†‘  | 0.7267|Â±  |0.0253|
|  - prehistory                                            |      1|none             |     0|acc        |â†‘  | 0.7191|Â±  |0.0250|
|  - professional_law                                      |      1|none             |     0|acc        |â†‘  | 0.4922|Â±  |0.0128|
|  - world_religions                                       |      1|none             |     0|acc        |â†‘  | 0.8012|Â±  |0.0306|
| - other                                                  |      2|none             |      |acc        |â†‘  | 0.7300|Â±  |0.0077|
|  - business_ethics                                       |      1|none             |     0|acc        |â†‘  | 0.7500|Â±  |0.0435|
|  - clinical_knowledge                                    |      1|none             |     0|acc        |â†‘  | 0.7132|Â±  |0.0278|
|  - college_medicine                                      |      1|none             |     0|acc        |â†‘  | 0.6705|Â±  |0.0358|
|  - global_facts                                          |      1|none             |     0|acc        |â†‘  | 0.4200|Â±  |0.0496|
|  - human_aging                                           |      1|none             |     0|acc        |â†‘  | 0.7085|Â±  |0.0305|
|  - management                                            |      1|none             |     0|acc        |â†‘  | 0.8447|Â±  |0.0359|
|  - marketing                                             |      1|none             |     0|acc        |â†‘  | 0.8974|Â±  |0.0199|
|  - medical_genetics                                      |      1|none             |     0|acc        |â†‘  | 0.7600|Â±  |0.0429|
|  - miscellaneous                                         |      1|none             |     0|acc        |â†‘  | 0.8199|Â±  |0.0137|
|  - nutrition                                             |      1|none             |     0|acc        |â†‘  | 0.7418|Â±  |0.0251|
|  - professional_accounting                               |      1|none             |     0|acc        |â†‘  | 0.5993|Â±  |0.0292|
|  - professional_medicine                                 |      1|none             |     0|acc        |â†‘  | 0.6801|Â±  |0.0283|
|  - virology                                              |      1|none             |     0|acc        |â†‘  | 0.5542|Â±  |0.0387|
| - social sciences                                        |      2|none             |      |acc        |â†‘  | 0.7813|Â±  |0.0073|
|  - econometrics                                          |      1|none             |     0|acc        |â†‘  | 0.6316|Â±  |0.0454|
|  - high_school_geography                                 |      1|none             |     0|acc        |â†‘  | 0.8333|Â±  |0.0266|
|  - high_school_government_and_politics                   |      1|none             |     0|acc        |â†‘  | 0.8808|Â±  |0.0234|
|  - high_school_macroeconomics                            |      1|none             |     0|acc        |â†‘  | 0.7846|Â±  |0.0208|
|  - high_school_microeconomics                            |      1|none             |     0|acc        |â†‘  | 0.8277|Â±  |0.0245|
|  - high_school_psychology                                |      1|none             |     0|acc        |â†‘  | 0.8716|Â±  |0.0143|
|  - human_sexuality                                       |      1|none             |     0|acc        |â†‘  | 0.6870|Â±  |0.0407|
|  - professional_psychology                               |      1|none             |     0|acc        |â†‘  | 0.6977|Â±  |0.0186|
|  - public_relations                                      |      1|none             |     0|acc        |â†‘  | 0.6455|Â±  |0.0458|
|  - security_studies                                      |      1|none             |     0|acc        |â†‘  | 0.7469|Â±  |0.0278|
|  - sociology                                             |      1|none             |     0|acc        |â†‘  | 0.7910|Â±  |0.0287|
|  - us_foreign_policy                                     |      1|none             |     0|acc        |â†‘  | 0.8900|Â±  |0.0314|
| - stem                                                   |      2|none             |      |acc        |â†‘  | 0.6438|Â±  |0.0082|
|  - abstract_algebra                                      |      1|none             |     0|acc        |â†‘  | 0.4500|Â±  |0.0500|
|  - anatomy                                               |      1|none             |     0|acc        |â†‘  | 0.6889|Â±  |0.0400|
|  - astronomy                                             |      1|none             |     0|acc        |â†‘  | 0.7500|Â±  |0.0352|
|  - college_biology                                       |      1|none             |     0|acc        |â†‘  | 0.7917|Â±  |0.0340|
|  - college_chemistry                                     |      1|none             |     0|acc        |â†‘  | 0.5100|Â±  |0.0502|
|  - college_computer_science                              |      1|none             |     0|acc        |â†‘  | 0.6100|Â±  |0.0490|
|  - college_mathematics                                   |      1|none             |     0|acc        |â†‘  | 0.3900|Â±  |0.0490|
|  - college_physics                                       |      1|none             |     0|acc        |â†‘  | 0.4902|Â±  |0.0497|
|  - computer_security                                     |      1|none             |     0|acc        |â†‘  | 0.8400|Â±  |0.0368|
|  - conceptual_physics                                    |      1|none             |     0|acc        |â†‘  | 0.7191|Â±  |0.0294|
|  - electrical_engineering                                |      1|none             |     0|acc        |â†‘  | 0.7172|Â±  |0.0375|
|  - elementary_mathematics                                |      1|none             |     0|acc        |â†‘  | 0.6270|Â±  |0.0249|
|  - high_school_biology                                   |      1|none             |     0|acc        |â†‘  | 0.8516|Â±  |0.0202|
|  - high_school_chemistry                                 |      1|none             |     0|acc        |â†‘  | 0.6305|Â±  |0.0340|
|  - high_school_computer_science                          |      1|none             |     0|acc        |â†‘  | 0.8000|Â±  |0.0402|
|  - high_school_mathematics                               |      1|none             |     0|acc        |â†‘  | 0.4407|Â±  |0.0303|
|  - high_school_physics                                   |      1|none             |     0|acc        |â†‘  | 0.4636|Â±  |0.0407|
|  - high_school_statistics                                |      1|none             |     0|acc        |â†‘  | 0.6759|Â±  |0.0319|
|  - machine_learning                                      |      1|none             |     0|acc        |â†‘  | 0.5536|Â±  |0.0472|
|nq_open                                                   |      4|remove_whitespace|     0|exact_match|â†‘  | 0.0094|Â±  |0.0016|
|openbookqa                                                |      1|none             |     0|acc        |â†‘  | 0.3200|Â±  |0.0209|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.4360|Â±  |0.0222|
|piqa                                                      |      1|none             |     0|acc        |â†‘  | 0.7965|Â±  |0.0094|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.8036|Â±  |0.0093|
|qnli                                                      |      1|none             |     0|acc        |â†‘  | 0.7877|Â±  |0.0055|
|sciq                                                      |      1|none             |     0|acc        |â†‘  | 0.9590|Â±  |0.0063|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.9540|Â±  |0.0066|
|triviaqa                                                  |      3|remove_whitespace|     0|exact_match|â†‘  | 0.3387|Â±  |0.0035|
|truthfulqa_gen                                            |      3|none             |     0|bleu_acc   |â†‘  | 0.4541|Â±  |0.0174|
|                                                          |       |none             |     0|bleu_diff  |â†‘  |-0.7696|Â±  |0.5079|
|                                                          |       |none             |     0|bleu_max   |â†‘  |18.9123|Â±  |0.6279|
|                                                          |       |none             |     0|rouge1_acc |â†‘  | 0.4602|Â±  |0.0174|
|                                                          |       |none             |     0|rouge1_diff|â†‘  |-1.1341|Â±  |0.6159|
|                                                          |       |none             |     0|rouge1_max |â†‘  |44.4829|Â±  |0.7546|
|                                                          |       |none             |     0|rouge2_acc |â†‘  | 0.4027|Â±  |0.0172|
|                                                          |       |none             |     0|rouge2_diff|â†‘  |-1.7922|Â±  |0.7369|
|                                                          |       |none             |     0|rouge2_max |â†‘  |30.3176|Â±  |0.8139|
|                                                          |       |none             |     0|rougeL_acc |â†‘  | 0.4517|Â±  |0.0174|
|                                                          |       |none             |     0|rougeL_diff|â†‘  |-1.6275|Â±  |0.6211|
|                                                          |       |none             |     0|rougeL_max |â†‘  |40.9909|Â±  |0.7553|
|truthfulqa_mc1                                            |      2|none             |     0|acc        |â†‘  | 0.3745|Â±  |0.0169|
|truthfulqa_mc2                                            |      3|none             |     0|acc        |â†‘  | 0.5479|Â±  |0.0159|
|winogrande                                                |      1|none             |     0|acc        |â†‘  | 0.7466|Â±  |0.0122|

|      Groups      |Version|  Filter  |n-shot|  Metric   |   |Value |   |Stderr|
|------------------|------:|----------|------|-----------|---|-----:|---|-----:|
|bbh               |      3|get-answer|      |exact_match|â†‘  |0.6107|Â±  |0.0053|
|mmlu              |      2|none      |      |acc        |â†‘  |0.6841|Â±  |0.0037|
| - humanities     |      2|none      |      |acc        |â†‘  |0.6172|Â±  |0.0067|
| - other          |      2|none      |      |acc        |â†‘  |0.7300|Â±  |0.0077|
| - social sciences|      2|none      |      |acc        |â†‘  |0.7813|Â±  |0.0073|
| - stem           |      2|none      |      |acc        |â†‘  |0.6438|Â±  |0.0082|

01-ai_Yi-1.5-9B-Chat: 13h 54m 25s
âœ… Benchmark completed for 01-ai_Yi-1.5-9B-Chat

ðŸ”¥ Starting benchmark for 01-ai_Yi-1.5-6B-Chat
fatal: not a git repository (or any of the parent directories): .git
2025-07-27:20:07:27 INFO     [loggers.evaluation_tracker:209] Saving results aggregated
hf (pretrained=/home/jaymin/Documents/llm/llm_models/01-ai_Yi-1.5-6B-Chat), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 2
|                          Tasks                           |Version|     Filter      |n-shot|  Metric   |   | Value |   |Stderr|
|----------------------------------------------------------|------:|-----------------|-----:|-----------|---|------:|---|-----:|
|anli_r1                                                   |      1|none             |     0|acc        |â†‘  | 0.4770|Â±  |0.0158|
|anli_r2                                                   |      1|none             |     0|acc        |â†‘  | 0.4530|Â±  |0.0157|
|anli_r3                                                   |      1|none             |     0|acc        |â†‘  | 0.4600|Â±  |0.0144|
|arc_challenge                                             |      1|none             |     0|acc        |â†‘  | 0.5077|Â±  |0.0146|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.5392|Â±  |0.0146|
|bbh                                                       |      3|get-answer       |      |exact_match|â†‘  | 0.5478|Â±  |0.0055|
| - bbh_cot_fewshot_boolean_expressions                    |      4|get-answer       |     3|exact_match|â†‘  | 0.9120|Â±  |0.0180|
| - bbh_cot_fewshot_causal_judgement                       |      4|get-answer       |     3|exact_match|â†‘  | 0.5080|Â±  |0.0367|
| - bbh_cot_fewshot_date_understanding                     |      4|get-answer       |     3|exact_match|â†‘  | 0.7040|Â±  |0.0289|
| - bbh_cot_fewshot_disambiguation_qa                      |      4|get-answer       |     3|exact_match|â†‘  | 0.5160|Â±  |0.0317|
| - bbh_cot_fewshot_dyck_languages                         |      4|get-answer       |     3|exact_match|â†‘  | 0.1000|Â±  |0.0190|
| - bbh_cot_fewshot_formal_fallacies                       |      4|get-answer       |     3|exact_match|â†‘  | 0.5760|Â±  |0.0313|
| - bbh_cot_fewshot_geometric_shapes                       |      4|get-answer       |     3|exact_match|â†‘  | 0.4760|Â±  |0.0316|
| - bbh_cot_fewshot_hyperbaton                             |      4|get-answer       |     3|exact_match|â†‘  | 0.8040|Â±  |0.0252|
| - bbh_cot_fewshot_logical_deduction_five_objects         |      4|get-answer       |     3|exact_match|â†‘  | 0.4040|Â±  |0.0311|
| - bbh_cot_fewshot_logical_deduction_seven_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.3240|Â±  |0.0297|
| - bbh_cot_fewshot_logical_deduction_three_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.7480|Â±  |0.0275|
| - bbh_cot_fewshot_movie_recommendation                   |      4|get-answer       |     3|exact_match|â†‘  | 0.5680|Â±  |0.0314|
| - bbh_cot_fewshot_multistep_arithmetic_two               |      4|get-answer       |     3|exact_match|â†‘  | 0.7120|Â±  |0.0287|
| - bbh_cot_fewshot_navigate                               |      4|get-answer       |     3|exact_match|â†‘  | 0.8800|Â±  |0.0206|
| - bbh_cot_fewshot_object_counting                        |      4|get-answer       |     3|exact_match|â†‘  | 0.7280|Â±  |0.0282|
| - bbh_cot_fewshot_penguins_in_a_table                    |      4|get-answer       |     3|exact_match|â†‘  | 0.6096|Â±  |0.0405|
| - bbh_cot_fewshot_reasoning_about_colored_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.7000|Â±  |0.0290|
| - bbh_cot_fewshot_ruin_names                             |      4|get-answer       |     3|exact_match|â†‘  | 0.3880|Â±  |0.0309|
| - bbh_cot_fewshot_salient_translation_error_detection    |      4|get-answer       |     3|exact_match|â†‘  | 0.4880|Â±  |0.0317|
| - bbh_cot_fewshot_snarks                                 |      4|get-answer       |     3|exact_match|â†‘  | 0.4663|Â±  |0.0375|
| - bbh_cot_fewshot_sports_understanding                   |      4|get-answer       |     3|exact_match|â†‘  | 0.8880|Â±  |0.0200|
| - bbh_cot_fewshot_temporal_sequences                     |      4|get-answer       |     3|exact_match|â†‘  | 0.4960|Â±  |0.0317|
| - bbh_cot_fewshot_tracking_shuffled_objects_five_objects |      4|get-answer       |     3|exact_match|â†‘  | 0.3040|Â±  |0.0292|
| - bbh_cot_fewshot_tracking_shuffled_objects_seven_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.3720|Â±  |0.0306|
| - bbh_cot_fewshot_tracking_shuffled_objects_three_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.1680|Â±  |0.0237|
| - bbh_cot_fewshot_web_of_lies                            |      4|get-answer       |     3|exact_match|â†‘  | 0.7960|Â±  |0.0255|
| - bbh_cot_fewshot_word_sorting                           |      4|get-answer       |     3|exact_match|â†‘  | 0.1480|Â±  |0.0225|
|boolq                                                     |      2|none             |     0|acc        |â†‘  | 0.8474|Â±  |0.0063|
|drop                                                      |      3|none             |     0|em         |â†‘  | 0.0071|Â±  |0.0009|
|                                                          |       |none             |     0|f1         |â†‘  | 0.1161|Â±  |0.0020|
|gpqa_diamond_cot_n_shot                                   |      2|flexible-extract |     0|exact_match|â†‘  | 0.1465|Â±  |0.0252|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_cot_zeroshot                                 |      1|flexible-extract |     0|exact_match|â†‘  | 0.1212|Â±  |0.0233|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_generative_n_shot                            |      2|flexible-extract |     0|exact_match|â†‘  | 0.2475|Â±  |0.0307|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_n_shot                                       |      2|none             |     0|acc        |â†‘  | 0.3333|Â±  |0.0336|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3333|Â±  |0.0336|
|gpqa_diamond_zeroshot                                     |      1|none             |     0|acc        |â†‘  | 0.3182|Â±  |0.0332|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3182|Â±  |0.0332|
|gpqa_extended_cot_n_shot                                  |      2|flexible-extract |     0|exact_match|â†‘  | 0.1392|Â±  |0.0148|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_cot_zeroshot                                |      1|flexible-extract |     0|exact_match|â†‘  | 0.1520|Â±  |0.0154|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_generative_n_shot                           |      2|flexible-extract |     0|exact_match|â†‘  | 0.2198|Â±  |0.0177|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_n_shot                                      |      2|none             |     0|acc        |â†‘  | 0.3095|Â±  |0.0198|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3095|Â±  |0.0198|
|gpqa_extended_zeroshot                                    |      1|none             |     0|acc        |â†‘  | 0.3205|Â±  |0.0200|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3205|Â±  |0.0200|
|gpqa_main_cot_n_shot                                      |      2|flexible-extract |     0|exact_match|â†‘  | 0.1384|Â±  |0.0163|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_cot_zeroshot                                    |      1|flexible-extract |     0|exact_match|â†‘  | 0.1540|Â±  |0.0171|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_generative_n_shot                               |      2|flexible-extract |     0|exact_match|â†‘  | 0.1942|Â±  |0.0187|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_n_shot                                          |      2|none             |     0|acc        |â†‘  | 0.3125|Â±  |0.0219|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3125|Â±  |0.0219|
|gpqa_main_zeroshot                                        |      1|none             |     0|acc        |â†‘  | 0.3571|Â±  |0.0227|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3571|Â±  |0.0227|
|gsm8k                                                     |      3|flexible-extract |     5|exact_match|â†‘  | 0.6785|Â±  |0.0129|
|                                                          |       |strict-match     |     5|exact_match|â†‘  | 0.6702|Â±  |0.0129|
|hellaswag                                                 |      1|none             |     0|acc        |â†‘  | 0.5852|Â±  |0.0049|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.7675|Â±  |0.0042|
|mmlu                                                      |      2|none             |      |acc        |â†‘  | 0.6179|Â±  |0.0039|
| - humanities                                             |      2|none             |      |acc        |â†‘  | 0.5392|Â±  |0.0068|
|  - formal_logic                                          |      1|none             |     0|acc        |â†‘  | 0.4444|Â±  |0.0444|
|  - high_school_european_history                          |      1|none             |     0|acc        |â†‘  | 0.7697|Â±  |0.0329|
|  - high_school_us_history                                |      1|none             |     0|acc        |â†‘  | 0.7696|Â±  |0.0296|
|  - high_school_world_history                             |      1|none             |     0|acc        |â†‘  | 0.7848|Â±  |0.0268|
|  - international_law                                     |      1|none             |     0|acc        |â†‘  | 0.7355|Â±  |0.0403|
|  - jurisprudence                                         |      1|none             |     0|acc        |â†‘  | 0.7222|Â±  |0.0433|
|  - logical_fallacies                                     |      1|none             |     0|acc        |â†‘  | 0.7853|Â±  |0.0323|
|  - moral_disputes                                        |      1|none             |     0|acc        |â†‘  | 0.6763|Â±  |0.0252|
|  - moral_scenarios                                       |      1|none             |     0|acc        |â†‘  | 0.2469|Â±  |0.0144|
|  - philosophy                                            |      1|none             |     0|acc        |â†‘  | 0.6752|Â±  |0.0266|
|  - prehistory                                            |      1|none             |     0|acc        |â†‘  | 0.6512|Â±  |0.0265|
|  - professional_law                                      |      1|none             |     0|acc        |â†‘  | 0.4641|Â±  |0.0127|
|  - world_religions                                       |      1|none             |     0|acc        |â†‘  | 0.7485|Â±  |0.0333|
| - other                                                  |      2|none             |      |acc        |â†‘  | 0.6794|Â±  |0.0081|
|  - business_ethics                                       |      1|none             |     0|acc        |â†‘  | 0.7400|Â±  |0.0441|
|  - clinical_knowledge                                    |      1|none             |     0|acc        |â†‘  | 0.7132|Â±  |0.0278|
|  - college_medicine                                      |      1|none             |     0|acc        |â†‘  | 0.6647|Â±  |0.0360|
|  - global_facts                                          |      1|none             |     0|acc        |â†‘  | 0.3400|Â±  |0.0476|
|  - human_aging                                           |      1|none             |     0|acc        |â†‘  | 0.6413|Â±  |0.0322|
|  - management                                            |      1|none             |     0|acc        |â†‘  | 0.8544|Â±  |0.0349|
|  - marketing                                             |      1|none             |     0|acc        |â†‘  | 0.8590|Â±  |0.0228|
|  - medical_genetics                                      |      1|none             |     0|acc        |â†‘  | 0.7500|Â±  |0.0435|
|  - miscellaneous                                         |      1|none             |     0|acc        |â†‘  | 0.7778|Â±  |0.0149|
|  - nutrition                                             |      1|none             |     0|acc        |â†‘  | 0.6732|Â±  |0.0269|
|  - professional_accounting                               |      1|none             |     0|acc        |â†‘  | 0.4716|Â±  |0.0298|
|  - professional_medicine                                 |      1|none             |     0|acc        |â†‘  | 0.6066|Â±  |0.0297|
|  - virology                                              |      1|none             |     0|acc        |â†‘  | 0.4759|Â±  |0.0389|
| - social sciences                                        |      2|none             |      |acc        |â†‘  | 0.7221|Â±  |0.0079|
|  - econometrics                                          |      1|none             |     0|acc        |â†‘  | 0.5702|Â±  |0.0466|
|  - high_school_geography                                 |      1|none             |     0|acc        |â†‘  | 0.8232|Â±  |0.0272|
|  - high_school_government_and_politics                   |      1|none             |     0|acc        |â†‘  | 0.8238|Â±  |0.0275|
|  - high_school_macroeconomics                            |      1|none             |     0|acc        |â†‘  | 0.7308|Â±  |0.0225|
|  - high_school_microeconomics                            |      1|none             |     0|acc        |â†‘  | 0.8067|Â±  |0.0256|
|  - high_school_psychology                                |      1|none             |     0|acc        |â†‘  | 0.8073|Â±  |0.0169|
|  - human_sexuality                                       |      1|none             |     0|acc        |â†‘  | 0.6107|Â±  |0.0428|
|  - professional_psychology                               |      1|none             |     0|acc        |â†‘  | 0.6046|Â±  |0.0198|
|  - public_relations                                      |      1|none             |     0|acc        |â†‘  | 0.6182|Â±  |0.0465|
|  - security_studies                                      |      1|none             |     0|acc        |â†‘  | 0.6449|Â±  |0.0306|
|  - sociology                                             |      1|none             |     0|acc        |â†‘  | 0.7960|Â±  |0.0285|
|  - us_foreign_policy                                     |      1|none             |     0|acc        |â†‘  | 0.8200|Â±  |0.0386|
| - stem                                                   |      2|none             |      |acc        |â†‘  | 0.5728|Â±  |0.0086|
|  - abstract_algebra                                      |      1|none             |     0|acc        |â†‘  | 0.5100|Â±  |0.0502|
|  - anatomy                                               |      1|none             |     0|acc        |â†‘  | 0.5778|Â±  |0.0427|
|  - astronomy                                             |      1|none             |     0|acc        |â†‘  | 0.6711|Â±  |0.0382|
|  - college_biology                                       |      1|none             |     0|acc        |â†‘  | 0.7153|Â±  |0.0377|
|  - college_chemistry                                     |      1|none             |     0|acc        |â†‘  | 0.4700|Â±  |0.0502|
|  - college_computer_science                              |      1|none             |     0|acc        |â†‘  | 0.5100|Â±  |0.0502|
|  - college_mathematics                                   |      1|none             |     0|acc        |â†‘  | 0.4700|Â±  |0.0502|
|  - college_physics                                       |      1|none             |     0|acc        |â†‘  | 0.4510|Â±  |0.0495|
|  - computer_security                                     |      1|none             |     0|acc        |â†‘  | 0.7100|Â±  |0.0456|
|  - conceptual_physics                                    |      1|none             |     0|acc        |â†‘  | 0.6340|Â±  |0.0315|
|  - electrical_engineering                                |      1|none             |     0|acc        |â†‘  | 0.6345|Â±  |0.0401|
|  - elementary_mathematics                                |      1|none             |     0|acc        |â†‘  | 0.5212|Â±  |0.0257|
|  - high_school_biology                                   |      1|none             |     0|acc        |â†‘  | 0.7968|Â±  |0.0229|
|  - high_school_chemistry                                 |      1|none             |     0|acc        |â†‘  | 0.5468|Â±  |0.0350|
|  - high_school_computer_science                          |      1|none             |     0|acc        |â†‘  | 0.7100|Â±  |0.0456|
|  - high_school_mathematics                               |      1|none             |     0|acc        |â†‘  | 0.4259|Â±  |0.0301|
|  - high_school_physics                                   |      1|none             |     0|acc        |â†‘  | 0.3974|Â±  |0.0400|
|  - high_school_statistics                                |      1|none             |     0|acc        |â†‘  | 0.5694|Â±  |0.0338|
|  - machine_learning                                      |      1|none             |     0|acc        |â†‘  | 0.4018|Â±  |0.0465|
|nq_open                                                   |      4|remove_whitespace|     0|exact_match|â†‘  | 0.0271|Â±  |0.0027|
|openbookqa                                                |      1|none             |     0|acc        |â†‘  | 0.3240|Â±  |0.0210|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.4360|Â±  |0.0222|
|piqa                                                      |      1|none             |     0|acc        |â†‘  | 0.7835|Â±  |0.0096|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.7878|Â±  |0.0095|
|qnli                                                      |      1|none             |     0|acc        |â†‘  | 0.6795|Â±  |0.0063|
|sciq                                                      |      1|none             |     0|acc        |â†‘  | 0.9620|Â±  |0.0060|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.9340|Â±  |0.0079|
|triviaqa                                                  |      3|remove_whitespace|     0|exact_match|â†‘  | 0.3310|Â±  |0.0035|
|truthfulqa_gen                                            |      3|none             |     0|bleu_acc   |â†‘  | 0.4088|Â±  |0.0172|
|                                                          |       |none             |     0|bleu_diff  |â†‘  |-1.4065|Â±  |0.6448|
|                                                          |       |none             |     0|bleu_max   |â†‘  |22.2331|Â±  |0.7119|
|                                                          |       |none             |     0|rouge1_acc |â†‘  | 0.4125|Â±  |0.0172|
|                                                          |       |none             |     0|rouge1_diff|â†‘  |-1.8289|Â±  |0.7973|
|                                                          |       |none             |     0|rouge1_max |â†‘  |48.2058|Â±  |0.7942|
|                                                          |       |none             |     0|rouge2_acc |â†‘  | 0.3525|Â±  |0.0167|
|                                                          |       |none             |     0|rouge2_diff|â†‘  |-2.8970|Â±  |0.9086|
|                                                          |       |none             |     0|rouge2_max |â†‘  |33.2543|Â±  |0.9010|
|                                                          |       |none             |     0|rougeL_acc |â†‘  | 0.4039|Â±  |0.0172|
|                                                          |       |none             |     0|rougeL_diff|â†‘  |-1.9691|Â±  |0.8088|
|                                                          |       |none             |     0|rougeL_max |â†‘  |44.8699|Â±  |0.8098|
|truthfulqa_mc1                                            |      2|none             |     0|acc        |â†‘  | 0.3770|Â±  |0.0170|
|truthfulqa_mc2                                            |      3|none             |     0|acc        |â†‘  | 0.5344|Â±  |0.0159|
|winogrande                                                |      1|none             |     0|acc        |â†‘  | 0.7096|Â±  |0.0128|

|      Groups      |Version|  Filter  |n-shot|  Metric   |   |Value |   |Stderr|
|------------------|------:|----------|------|-----------|---|-----:|---|-----:|
|bbh               |      3|get-answer|      |exact_match|â†‘  |0.5478|Â±  |0.0055|
|mmlu              |      2|none      |      |acc        |â†‘  |0.6179|Â±  |0.0039|
| - humanities     |      2|none      |      |acc        |â†‘  |0.5392|Â±  |0.0068|
| - other          |      2|none      |      |acc        |â†‘  |0.6794|Â±  |0.0081|
| - social sciences|      2|none      |      |acc        |â†‘  |0.7221|Â±  |0.0079|
| - stem           |      2|none      |      |acc        |â†‘  |0.5728|Â±  |0.0086|

01-ai_Yi-1.5-6B-Chat: 8h 4m 9s
âœ… Benchmark completed for 01-ai_Yi-1.5-6B-Chat

ðŸ”¥ Starting benchmark for 01-ai_Yi-1.5-9B
fatal: not a git repository (or any of the parent directories): .git
2025-07-28:07:51:08 INFO     [loggers.evaluation_tracker:209] Saving results aggregated
hf (pretrained=/home/jaymin/Documents/llm/llm_models/01-ai_Yi-1.5-9B), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 2
|                          Tasks                           |Version|     Filter      |n-shot|  Metric   |   | Value |   |Stderr|
|----------------------------------------------------------|------:|-----------------|-----:|-----------|---|------:|---|-----:|
|anli_r1                                                   |      1|none             |     0|acc        |â†‘  | 0.5320|Â±  |0.0158|
|anli_r2                                                   |      1|none             |     0|acc        |â†‘  | 0.4800|Â±  |0.0158|
|anli_r3                                                   |      1|none             |     0|acc        |â†‘  | 0.4392|Â±  |0.0143|
|arc_challenge                                             |      1|none             |     0|acc        |â†‘  | 0.5290|Â±  |0.0146|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.5469|Â±  |0.0145|
|bbh                                                       |      3|get-answer       |      |exact_match|â†‘  | 0.7120|Â±  |0.0052|
| - bbh_cot_fewshot_boolean_expressions                    |      4|get-answer       |     3|exact_match|â†‘  | 0.8800|Â±  |0.0206|
| - bbh_cot_fewshot_causal_judgement                       |      4|get-answer       |     3|exact_match|â†‘  | 0.5668|Â±  |0.0363|
| - bbh_cot_fewshot_date_understanding                     |      4|get-answer       |     3|exact_match|â†‘  | 0.8240|Â±  |0.0241|
| - bbh_cot_fewshot_disambiguation_qa                      |      4|get-answer       |     3|exact_match|â†‘  | 0.7440|Â±  |0.0277|
| - bbh_cot_fewshot_dyck_languages                         |      4|get-answer       |     3|exact_match|â†‘  | 0.3320|Â±  |0.0298|
| - bbh_cot_fewshot_formal_fallacies                       |      4|get-answer       |     3|exact_match|â†‘  | 0.5120|Â±  |0.0317|
| - bbh_cot_fewshot_geometric_shapes                       |      4|get-answer       |     3|exact_match|â†‘  | 0.5440|Â±  |0.0316|
| - bbh_cot_fewshot_hyperbaton                             |      4|get-answer       |     3|exact_match|â†‘  | 0.8800|Â±  |0.0206|
| - bbh_cot_fewshot_logical_deduction_five_objects         |      4|get-answer       |     3|exact_match|â†‘  | 0.5320|Â±  |0.0316|
| - bbh_cot_fewshot_logical_deduction_seven_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.4440|Â±  |0.0315|
| - bbh_cot_fewshot_logical_deduction_three_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.8560|Â±  |0.0222|
| - bbh_cot_fewshot_movie_recommendation                   |      4|get-answer       |     3|exact_match|â†‘  | 0.8320|Â±  |0.0237|
| - bbh_cot_fewshot_multistep_arithmetic_two               |      4|get-answer       |     3|exact_match|â†‘  | 0.7120|Â±  |0.0287|
| - bbh_cot_fewshot_navigate                               |      4|get-answer       |     3|exact_match|â†‘  | 0.9240|Â±  |0.0168|
| - bbh_cot_fewshot_object_counting                        |      4|get-answer       |     3|exact_match|â†‘  | 0.8800|Â±  |0.0206|
| - bbh_cot_fewshot_penguins_in_a_table                    |      4|get-answer       |     3|exact_match|â†‘  | 0.7123|Â±  |0.0376|
| - bbh_cot_fewshot_reasoning_about_colored_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.7280|Â±  |0.0282|
| - bbh_cot_fewshot_ruin_names                             |      4|get-answer       |     3|exact_match|â†‘  | 0.7560|Â±  |0.0272|
| - bbh_cot_fewshot_salient_translation_error_detection    |      4|get-answer       |     3|exact_match|â†‘  | 0.5400|Â±  |0.0316|
| - bbh_cot_fewshot_snarks                                 |      4|get-answer       |     3|exact_match|â†‘  | 0.5618|Â±  |0.0373|
| - bbh_cot_fewshot_sports_understanding                   |      4|get-answer       |     3|exact_match|â†‘  | 0.9800|Â±  |0.0089|
| - bbh_cot_fewshot_temporal_sequences                     |      4|get-answer       |     3|exact_match|â†‘  | 0.8880|Â±  |0.0200|
| - bbh_cot_fewshot_tracking_shuffled_objects_five_objects |      4|get-answer       |     3|exact_match|â†‘  | 0.7000|Â±  |0.0290|
| - bbh_cot_fewshot_tracking_shuffled_objects_seven_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.6880|Â±  |0.0294|
| - bbh_cot_fewshot_tracking_shuffled_objects_three_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.7240|Â±  |0.0283|
| - bbh_cot_fewshot_web_of_lies                            |      4|get-answer       |     3|exact_match|â†‘  | 1.0000|Â±  |0.0000|
| - bbh_cot_fewshot_word_sorting                           |      4|get-answer       |     3|exact_match|â†‘  | 0.4040|Â±  |0.0311|
|boolq                                                     |      2|none             |     0|acc        |â†‘  | 0.8581|Â±  |0.0061|
|drop                                                      |      3|none             |     0|em         |â†‘  | 0.4148|Â±  |0.0050|
|                                                          |       |none             |     0|f1         |â†‘  | 0.4457|Â±  |0.0049|
|gpqa_diamond_cot_n_shot                                   |      2|flexible-extract |     0|exact_match|â†‘  | 0.1263|Â±  |0.0237|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.1010|Â±  |0.0215|
|gpqa_diamond_cot_zeroshot                                 |      1|flexible-extract |     0|exact_match|â†‘  | 0.1667|Â±  |0.0266|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0556|Â±  |0.0163|
|gpqa_diamond_generative_n_shot                            |      2|flexible-extract |     0|exact_match|â†‘  | 0.3283|Â±  |0.0335|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0202|Â±  |0.0100|
|gpqa_diamond_n_shot                                       |      2|none             |     0|acc        |â†‘  | 0.3434|Â±  |0.0338|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3434|Â±  |0.0338|
|gpqa_diamond_zeroshot                                     |      1|none             |     0|acc        |â†‘  | 0.3384|Â±  |0.0337|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3384|Â±  |0.0337|
|gpqa_extended_cot_n_shot                                  |      2|flexible-extract |     0|exact_match|â†‘  | 0.1667|Â±  |0.0160|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0916|Â±  |0.0124|
|gpqa_extended_cot_zeroshot                                |      1|flexible-extract |     0|exact_match|â†‘  | 0.1703|Â±  |0.0161|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0440|Â±  |0.0088|
|gpqa_extended_generative_n_shot                           |      2|flexible-extract |     0|exact_match|â†‘  | 0.2637|Â±  |0.0189|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0183|Â±  |0.0057|
|gpqa_extended_n_shot                                      |      2|none             |     0|acc        |â†‘  | 0.3187|Â±  |0.0200|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3187|Â±  |0.0200|
|gpqa_extended_zeroshot                                    |      1|none             |     0|acc        |â†‘  | 0.3535|Â±  |0.0205|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3535|Â±  |0.0205|
|gpqa_main_cot_n_shot                                      |      2|flexible-extract |     0|exact_match|â†‘  | 0.1562|Â±  |0.0172|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0893|Â±  |0.0135|
|gpqa_main_cot_zeroshot                                    |      1|flexible-extract |     0|exact_match|â†‘  | 0.1875|Â±  |0.0185|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0402|Â±  |0.0093|
|gpqa_main_generative_n_shot                               |      2|flexible-extract |     0|exact_match|â†‘  | 0.2790|Â±  |0.0212|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0201|Â±  |0.0066|
|gpqa_main_n_shot                                          |      2|none             |     0|acc        |â†‘  | 0.3594|Â±  |0.0227|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3594|Â±  |0.0227|
|gpqa_main_zeroshot                                        |      1|none             |     0|acc        |â†‘  | 0.2946|Â±  |0.0216|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2946|Â±  |0.0216|
|gsm8k                                                     |      3|flexible-extract |     5|exact_match|â†‘  | 0.6558|Â±  |0.0131|
|                                                          |       |strict-match     |     5|exact_match|â†‘  | 0.6391|Â±  |0.0132|
|hellaswag                                                 |      1|none             |     0|acc        |â†‘  | 0.5922|Â±  |0.0049|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.7789|Â±  |0.0041|
|mmlu                                                      |      2|none             |      |acc        |â†‘  | 0.6893|Â±  |0.0037|
| - humanities                                             |      2|none             |      |acc        |â†‘  | 0.6142|Â±  |0.0066|
|  - formal_logic                                          |      1|none             |     0|acc        |â†‘  | 0.5873|Â±  |0.0440|
|  - high_school_european_history                          |      1|none             |     0|acc        |â†‘  | 0.8242|Â±  |0.0297|
|  - high_school_us_history                                |      1|none             |     0|acc        |â†‘  | 0.8186|Â±  |0.0270|
|  - high_school_world_history                             |      1|none             |     0|acc        |â†‘  | 0.8650|Â±  |0.0222|
|  - international_law                                     |      1|none             |     0|acc        |â†‘  | 0.8099|Â±  |0.0358|
|  - jurisprudence                                         |      1|none             |     0|acc        |â†‘  | 0.8056|Â±  |0.0383|
|  - logical_fallacies                                     |      1|none             |     0|acc        |â†‘  | 0.8221|Â±  |0.0300|
|  - moral_disputes                                        |      1|none             |     0|acc        |â†‘  | 0.7399|Â±  |0.0236|
|  - moral_scenarios                                       |      1|none             |     0|acc        |â†‘  | 0.3419|Â±  |0.0159|
|  - philosophy                                            |      1|none             |     0|acc        |â†‘  | 0.7814|Â±  |0.0235|
|  - prehistory                                            |      1|none             |     0|acc        |â†‘  | 0.7716|Â±  |0.0234|
|  - professional_law                                      |      1|none             |     0|acc        |â†‘  | 0.5156|Â±  |0.0128|
|  - world_religions                                       |      1|none             |     0|acc        |â†‘  | 0.8363|Â±  |0.0284|
| - other                                                  |      2|none             |      |acc        |â†‘  | 0.7451|Â±  |0.0075|
|  - business_ethics                                       |      1|none             |     0|acc        |â†‘  | 0.7700|Â±  |0.0423|
|  - clinical_knowledge                                    |      1|none             |     0|acc        |â†‘  | 0.7358|Â±  |0.0271|
|  - college_medicine                                      |      1|none             |     0|acc        |â†‘  | 0.6821|Â±  |0.0355|
|  - global_facts                                          |      1|none             |     0|acc        |â†‘  | 0.3900|Â±  |0.0490|
|  - human_aging                                           |      1|none             |     0|acc        |â†‘  | 0.7489|Â±  |0.0291|
|  - management                                            |      1|none             |     0|acc        |â†‘  | 0.8155|Â±  |0.0384|
|  - marketing                                             |      1|none             |     0|acc        |â†‘  | 0.9231|Â±  |0.0175|
|  - medical_genetics                                      |      1|none             |     0|acc        |â†‘  | 0.7900|Â±  |0.0409|
|  - miscellaneous                                         |      1|none             |     0|acc        |â†‘  | 0.8506|Â±  |0.0127|
|  - nutrition                                             |      1|none             |     0|acc        |â†‘  | 0.7680|Â±  |0.0242|
|  - professional_accounting                               |      1|none             |     0|acc        |â†‘  | 0.5603|Â±  |0.0296|
|  - professional_medicine                                 |      1|none             |     0|acc        |â†‘  | 0.7243|Â±  |0.0271|
|  - virology                                              |      1|none             |     0|acc        |â†‘  | 0.5060|Â±  |0.0389|
| - social sciences                                        |      2|none             |      |acc        |â†‘  | 0.7956|Â±  |0.0071|
|  - econometrics                                          |      1|none             |     0|acc        |â†‘  | 0.5526|Â±  |0.0468|
|  - high_school_geography                                 |      1|none             |     0|acc        |â†‘  | 0.8485|Â±  |0.0255|
|  - high_school_government_and_politics                   |      1|none             |     0|acc        |â†‘  | 0.9119|Â±  |0.0205|
|  - high_school_macroeconomics                            |      1|none             |     0|acc        |â†‘  | 0.7538|Â±  |0.0218|
|  - high_school_microeconomics                            |      1|none             |     0|acc        |â†‘  | 0.8529|Â±  |0.0230|
|  - high_school_psychology                                |      1|none             |     0|acc        |â†‘  | 0.8716|Â±  |0.0143|
|  - human_sexuality                                       |      1|none             |     0|acc        |â†‘  | 0.7786|Â±  |0.0364|
|  - professional_psychology                               |      1|none             |     0|acc        |â†‘  | 0.7288|Â±  |0.0180|
|  - public_relations                                      |      1|none             |     0|acc        |â†‘  | 0.6818|Â±  |0.0446|
|  - security_studies                                      |      1|none             |     0|acc        |â†‘  | 0.7633|Â±  |0.0272|
|  - sociology                                             |      1|none             |     0|acc        |â†‘  | 0.8308|Â±  |0.0265|
|  - us_foreign_policy                                     |      1|none             |     0|acc        |â†‘  | 0.9200|Â±  |0.0273|
| - stem                                                   |      2|none             |      |acc        |â†‘  | 0.6426|Â±  |0.0082|
|  - abstract_algebra                                      |      1|none             |     0|acc        |â†‘  | 0.5000|Â±  |0.0503|
|  - anatomy                                               |      1|none             |     0|acc        |â†‘  | 0.6889|Â±  |0.0400|
|  - astronomy                                             |      1|none             |     0|acc        |â†‘  | 0.7632|Â±  |0.0346|
|  - college_biology                                       |      1|none             |     0|acc        |â†‘  | 0.8264|Â±  |0.0317|
|  - college_chemistry                                     |      1|none             |     0|acc        |â†‘  | 0.5200|Â±  |0.0502|
|  - college_computer_science                              |      1|none             |     0|acc        |â†‘  | 0.6900|Â±  |0.0465|
|  - college_mathematics                                   |      1|none             |     0|acc        |â†‘  | 0.5500|Â±  |0.0500|
|  - college_physics                                       |      1|none             |     0|acc        |â†‘  | 0.4706|Â±  |0.0497|
|  - computer_security                                     |      1|none             |     0|acc        |â†‘  | 0.7900|Â±  |0.0409|
|  - conceptual_physics                                    |      1|none             |     0|acc        |â†‘  | 0.7447|Â±  |0.0285|
|  - electrical_engineering                                |      1|none             |     0|acc        |â†‘  | 0.6552|Â±  |0.0396|
|  - elementary_mathematics                                |      1|none             |     0|acc        |â†‘  | 0.6270|Â±  |0.0249|
|  - high_school_biology                                   |      1|none             |     0|acc        |â†‘  | 0.8355|Â±  |0.0211|
|  - high_school_chemistry                                 |      1|none             |     0|acc        |â†‘  | 0.6601|Â±  |0.0333|
|  - high_school_computer_science                          |      1|none             |     0|acc        |â†‘  | 0.7800|Â±  |0.0416|
|  - high_school_mathematics                               |      1|none             |     0|acc        |â†‘  | 0.4185|Â±  |0.0301|
|  - high_school_physics                                   |      1|none             |     0|acc        |â†‘  | 0.4503|Â±  |0.0406|
|  - high_school_statistics                                |      1|none             |     0|acc        |â†‘  | 0.6019|Â±  |0.0334|
|  - machine_learning                                      |      1|none             |     0|acc        |â†‘  | 0.5000|Â±  |0.0475|
|nq_open                                                   |      4|remove_whitespace|     0|exact_match|â†‘  | 0.1532|Â±  |0.0060|
|openbookqa                                                |      1|none             |     0|acc        |â†‘  | 0.3580|Â±  |0.0215|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.4560|Â±  |0.0223|
|piqa                                                      |      1|none             |     0|acc        |â†‘  | 0.7943|Â±  |0.0094|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.8063|Â±  |0.0092|
|qnli                                                      |      1|none             |     0|acc        |â†‘  | 0.5087|Â±  |0.0068|
|sciq                                                      |      1|none             |     0|acc        |â†‘  | 0.9580|Â±  |0.0063|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.9520|Â±  |0.0068|
|triviaqa                                                  |      3|remove_whitespace|     0|exact_match|â†‘  | 0.5438|Â±  |0.0037|
|truthfulqa_gen                                            |      3|none             |     0|bleu_acc   |â†‘  | 0.4296|Â±  |0.0173|
|                                                          |       |none             |     0|bleu_diff  |â†‘  | 0.5808|Â±  |0.8548|
|                                                          |       |none             |     0|bleu_max   |â†‘  |27.3910|Â±  |0.8218|
|                                                          |       |none             |     0|rouge1_acc |â†‘  | 0.4198|Â±  |0.0173|
|                                                          |       |none             |     0|rouge1_diff|â†‘  | 0.7303|Â±  |1.0868|
|                                                          |       |none             |     0|rouge1_max |â†‘  |52.5810|Â±  |0.9006|
|                                                          |       |none             |     0|rouge2_acc |â†‘  | 0.3635|Â±  |0.0168|
|                                                          |       |none             |     0|rouge2_diff|â†‘  |-0.1698|Â±  |1.2219|
|                                                          |       |none             |     0|rouge2_max |â†‘  |37.0078|Â±  |1.0709|
|                                                          |       |none             |     0|rougeL_acc |â†‘  | 0.4186|Â±  |0.0173|
|                                                          |       |none             |     0|rougeL_diff|â†‘  | 0.4753|Â±  |1.0982|
|                                                          |       |none             |     0|rougeL_max |â†‘  |49.7474|Â±  |0.9202|
|truthfulqa_mc1                                            |      2|none             |     0|acc        |â†‘  | 0.3219|Â±  |0.0164|
|truthfulqa_mc2                                            |      3|none             |     0|acc        |â†‘  | 0.4676|Â±  |0.0149|
|winogrande                                                |      1|none             |     0|acc        |â†‘  | 0.7261|Â±  |0.0125|

|      Groups      |Version|  Filter  |n-shot|  Metric   |   |Value |   |Stderr|
|------------------|------:|----------|------|-----------|---|-----:|---|-----:|
|bbh               |      3|get-answer|      |exact_match|â†‘  |0.7120|Â±  |0.0052|
|mmlu              |      2|none      |      |acc        |â†‘  |0.6893|Â±  |0.0037|
| - humanities     |      2|none      |      |acc        |â†‘  |0.6142|Â±  |0.0066|
| - other          |      2|none      |      |acc        |â†‘  |0.7451|Â±  |0.0075|
| - social sciences|      2|none      |      |acc        |â†‘  |0.7956|Â±  |0.0071|
| - stem           |      2|none      |      |acc        |â†‘  |0.6426|Â±  |0.0082|

01-ai_Yi-1.5-9B: 11h 43m 41s
âœ… Benchmark completed for 01-ai_Yi-1.5-9B

ðŸ”¥ Starting benchmark for 01-ai_Yi-1.5-6B,
Passed argument batch_size = auto:1. Detecting largest batch size
Determined largest batch size: 8
Passed argument batch_size = auto. Detecting largest batch size
Determined Largest batch size: 9
hf (pretrained=/mnt/data8tb/Documents/llm/llm_models/01-ai_Yi-1.5-6B,), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto (8)
|                          Tasks                           |Version|     Filter      |n-shot|  Metric   |   | Value |   |Stderr|
|----------------------------------------------------------|------:|-----------------|-----:|-----------|---|------:|---|-----:|
|anli_r1                                                   |      1|none             |     0|acc        |â†‘  | 0.4480|Â±  |0.0157|
|anli_r2                                                   |      1|none             |     0|acc        |â†‘  | 0.4070|Â±  |0.0155|
|anli_r3                                                   |      1|none             |     0|acc        |â†‘  | 0.4067|Â±  |0.0142|
|arc_challenge                                             |      1|none             |     0|acc        |â†‘  | 0.4667|Â±  |0.0146|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.4966|Â±  |0.0146|
|bbh                                                       |      3|get-answer       |      |exact_match|â†‘  | 0.5755|Â±  |0.0055|
| - bbh_cot_fewshot_boolean_expressions                    |      4|get-answer       |     3|exact_match|â†‘  | 0.9200|Â±  |0.0172|
| - bbh_cot_fewshot_causal_judgement                       |      4|get-answer       |     3|exact_match|â†‘  | 0.5936|Â±  |0.0360|
| - bbh_cot_fewshot_date_understanding                     |      4|get-answer       |     3|exact_match|â†‘  | 0.7320|Â±  |0.0281|
| - bbh_cot_fewshot_disambiguation_qa                      |      4|get-answer       |     3|exact_match|â†‘  | 0.6040|Â±  |0.0310|
| - bbh_cot_fewshot_dyck_languages                         |      4|get-answer       |     3|exact_match|â†‘  | 0.1000|Â±  |0.0190|
| - bbh_cot_fewshot_formal_fallacies                       |      4|get-answer       |     3|exact_match|â†‘  | 0.4880|Â±  |0.0317|
| - bbh_cot_fewshot_geometric_shapes                       |      4|get-answer       |     3|exact_match|â†‘  | 0.3720|Â±  |0.0306|
| - bbh_cot_fewshot_hyperbaton                             |      4|get-answer       |     3|exact_match|â†‘  | 0.9040|Â±  |0.0187|
| - bbh_cot_fewshot_logical_deduction_five_objects         |      4|get-answer       |     3|exact_match|â†‘  | 0.4520|Â±  |0.0315|
| - bbh_cot_fewshot_logical_deduction_seven_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.3400|Â±  |0.0300|
| - bbh_cot_fewshot_logical_deduction_three_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.7160|Â±  |0.0286|
| - bbh_cot_fewshot_movie_recommendation                   |      4|get-answer       |     3|exact_match|â†‘  | 0.7600|Â±  |0.0271|
| - bbh_cot_fewshot_multistep_arithmetic_two               |      4|get-answer       |     3|exact_match|â†‘  | 0.6280|Â±  |0.0306|
| - bbh_cot_fewshot_navigate                               |      4|get-answer       |     3|exact_match|â†‘  | 0.8360|Â±  |0.0235|
| - bbh_cot_fewshot_object_counting                        |      4|get-answer       |     3|exact_match|â†‘  | 0.6800|Â±  |0.0296|
| - bbh_cot_fewshot_penguins_in_a_table                    |      4|get-answer       |     3|exact_match|â†‘  | 0.5890|Â±  |0.0409|
| - bbh_cot_fewshot_reasoning_about_colored_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.5640|Â±  |0.0314|
| - bbh_cot_fewshot_ruin_names                             |      4|get-answer       |     3|exact_match|â†‘  | 0.4280|Â±  |0.0314|
| - bbh_cot_fewshot_salient_translation_error_detection    |      4|get-answer       |     3|exact_match|â†‘  | 0.4400|Â±  |0.0315|
| - bbh_cot_fewshot_snarks                                 |      4|get-answer       |     3|exact_match|â†‘  | 0.6236|Â±  |0.0364|
| - bbh_cot_fewshot_sports_understanding                   |      4|get-answer       |     3|exact_match|â†‘  | 0.8920|Â±  |0.0197|
| - bbh_cot_fewshot_temporal_sequences                     |      4|get-answer       |     3|exact_match|â†‘  | 0.5200|Â±  |0.0317|
| - bbh_cot_fewshot_tracking_shuffled_objects_five_objects |      4|get-answer       |     3|exact_match|â†‘  | 0.4320|Â±  |0.0314|
| - bbh_cot_fewshot_tracking_shuffled_objects_seven_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.4040|Â±  |0.0311|
| - bbh_cot_fewshot_tracking_shuffled_objects_three_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.3880|Â±  |0.0309|
| - bbh_cot_fewshot_web_of_lies                            |      4|get-answer       |     3|exact_match|â†‘  | 0.9080|Â±  |0.0183|
| - bbh_cot_fewshot_word_sorting                           |      4|get-answer       |     3|exact_match|â†‘  | 0.2480|Â±  |0.0274|
|boolq                                                     |      2|none             |     0|acc        |â†‘  | 0.8015|Â±  |0.0070|
|drop                                                      |      3|none             |     0|em         |â†‘  | 0.3668|Â±  |0.0049|
|                                                          |       |none             |     0|f1         |â†‘  | 0.3995|Â±  |0.0049|
|gpqa_diamond_cot_n_shot                                   |      2|flexible-extract |     0|exact_match|â†‘  | 0.0859|Â±  |0.0200|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0404|Â±  |0.0140|
|gpqa_diamond_cot_zeroshot                                 |      1|flexible-extract |     0|exact_match|â†‘  | 0.1212|Â±  |0.0233|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0354|Â±  |0.0132|
|gpqa_diamond_generative_n_shot                            |      2|flexible-extract |     0|exact_match|â†‘  | 0.2323|Â±  |0.0301|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0051|Â±  |0.0051|
|gpqa_diamond_n_shot                                       |      2|none             |     0|acc        |â†‘  | 0.3586|Â±  |0.0342|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3586|Â±  |0.0342|
|gpqa_diamond_zeroshot                                     |      1|none             |     0|acc        |â†‘  | 0.3232|Â±  |0.0333|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3232|Â±  |0.0333|
|gpqa_extended_cot_n_shot                                  |      2|flexible-extract |     0|exact_match|â†‘  | 0.1886|Â±  |0.0168|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0714|Â±  |0.0110|
|gpqa_extended_cot_zeroshot                                |      1|flexible-extract |     0|exact_match|â†‘  | 0.2198|Â±  |0.0177|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.1062|Â±  |0.0132|
|gpqa_extended_generative_n_shot                           |      2|flexible-extract |     0|exact_match|â†‘  | 0.2601|Â±  |0.0188|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0055|Â±  |0.0032|
|gpqa_extended_n_shot                                      |      2|none             |     0|acc        |â†‘  | 0.3315|Â±  |0.0202|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3315|Â±  |0.0202|
|gpqa_extended_zeroshot                                    |      1|none             |     0|acc        |â†‘  | 0.2985|Â±  |0.0196|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2985|Â±  |0.0196|
|gpqa_main_cot_n_shot                                      |      2|flexible-extract |     0|exact_match|â†‘  | 0.1473|Â±  |0.0168|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0714|Â±  |0.0122|
|gpqa_main_cot_zeroshot                                    |      1|flexible-extract |     0|exact_match|â†‘  | 0.1741|Â±  |0.0179|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0871|Â±  |0.0133|
|gpqa_main_generative_n_shot                               |      2|flexible-extract |     0|exact_match|â†‘  | 0.2277|Â±  |0.0198|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_n_shot                                          |      2|none             |     0|acc        |â†‘  | 0.3147|Â±  |0.0220|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3147|Â±  |0.0220|
|gpqa_main_zeroshot                                        |      1|none             |     0|acc        |â†‘  | 0.2902|Â±  |0.0215|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2902|Â±  |0.0215|
|gsm8k                                                     |      3|flexible-extract |     5|exact_match|â†‘  | 0.5262|Â±  |0.0138|
|                                                          |       |strict-match     |     5|exact_match|â†‘  | 0.5224|Â±  |0.0138|
|hellaswag                                                 |      1|none             |     0|acc        |â†‘  | 0.5668|Â±  |0.0049|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.7541|Â±  |0.0043|
|mmlu                                                      |      2|none             |      |acc        |â†‘  | 0.6243|Â±  |0.0038|
| - humanities                                             |      2|none             |      |acc        |â†‘  | 0.5528|Â±  |0.0067|
|  - formal_logic                                          |      1|none             |     0|acc        |â†‘  | 0.4841|Â±  |0.0447|
|  - high_school_european_history                          |      1|none             |     0|acc        |â†‘  | 0.7576|Â±  |0.0335|
|  - high_school_us_history                                |      1|none             |     0|acc        |â†‘  | 0.7990|Â±  |0.0281|
|  - high_school_world_history                             |      1|none             |     0|acc        |â†‘  | 0.7806|Â±  |0.0269|
|  - international_law                                     |      1|none             |     0|acc        |â†‘  | 0.7851|Â±  |0.0375|
|  - jurisprudence                                         |      1|none             |     0|acc        |â†‘  | 0.7222|Â±  |0.0433|
|  - logical_fallacies                                     |      1|none             |     0|acc        |â†‘  | 0.7423|Â±  |0.0344|
|  - moral_disputes                                        |      1|none             |     0|acc        |â†‘  | 0.6792|Â±  |0.0251|
|  - moral_scenarios                                       |      1|none             |     0|acc        |â†‘  | 0.2425|Â±  |0.0143|
|  - philosophy                                            |      1|none             |     0|acc        |â†‘  | 0.7203|Â±  |0.0255|
|  - prehistory                                            |      1|none             |     0|acc        |â†‘  | 0.6883|Â±  |0.0258|
|  - professional_law                                      |      1|none             |     0|acc        |â†‘  | 0.4824|Â±  |0.0128|
|  - world_religions                                       |      1|none             |     0|acc        |â†‘  | 0.7836|Â±  |0.0316|
| - other                                                  |      2|none             |      |acc        |â†‘  | 0.6849|Â±  |0.0081|
|  - business_ethics                                       |      1|none             |     0|acc        |â†‘  | 0.7000|Â±  |0.0461|
|  - clinical_knowledge                                    |      1|none             |     0|acc        |â†‘  | 0.6717|Â±  |0.0289|
|  - college_medicine                                      |      1|none             |     0|acc        |â†‘  | 0.6358|Â±  |0.0367|
|  - global_facts                                          |      1|none             |     0|acc        |â†‘  | 0.3800|Â±  |0.0488|
|  - human_aging                                           |      1|none             |     0|acc        |â†‘  | 0.6502|Â±  |0.0320|
|  - management                                            |      1|none             |     0|acc        |â†‘  | 0.8058|Â±  |0.0392|
|  - marketing                                             |      1|none             |     0|acc        |â†‘  | 0.8675|Â±  |0.0222|
|  - medical_genetics                                      |      1|none             |     0|acc        |â†‘  | 0.6600|Â±  |0.0476|
|  - miscellaneous                                         |      1|none             |     0|acc        |â†‘  | 0.8008|Â±  |0.0143|
|  - nutrition                                             |      1|none             |     0|acc        |â†‘  | 0.6895|Â±  |0.0265|
|  - professional_accounting                               |      1|none             |     0|acc        |â†‘  | 0.4965|Â±  |0.0298|
|  - professional_medicine                                 |      1|none             |     0|acc        |â†‘  | 0.6287|Â±  |0.0293|
|  - virology                                              |      1|none             |     0|acc        |â†‘  | 0.5181|Â±  |0.0389|
| - social sciences                                        |      2|none             |      |acc        |â†‘  | 0.7407|Â±  |0.0077|
|  - econometrics                                          |      1|none             |     0|acc        |â†‘  | 0.4386|Â±  |0.0467|
|  - high_school_geography                                 |      1|none             |     0|acc        |â†‘  | 0.7929|Â±  |0.0289|
|  - high_school_government_and_politics                   |      1|none             |     0|acc        |â†‘  | 0.8601|Â±  |0.0250|
|  - high_school_macroeconomics                            |      1|none             |     0|acc        |â†‘  | 0.7026|Â±  |0.0232|
|  - high_school_microeconomics                            |      1|none             |     0|acc        |â†‘  | 0.8025|Â±  |0.0259|
|  - high_school_psychology                                |      1|none             |     0|acc        |â†‘  | 0.8367|Â±  |0.0158|
|  - human_sexuality                                       |      1|none             |     0|acc        |â†‘  | 0.6794|Â±  |0.0409|
|  - professional_psychology                               |      1|none             |     0|acc        |â†‘  | 0.6552|Â±  |0.0192|
|  - public_relations                                      |      1|none             |     0|acc        |â†‘  | 0.6545|Â±  |0.0455|
|  - security_studies                                      |      1|none             |     0|acc        |â†‘  | 0.7061|Â±  |0.0292|
|  - sociology                                             |      1|none             |     0|acc        |â†‘  | 0.8159|Â±  |0.0274|
|  - us_foreign_policy                                     |      1|none             |     0|acc        |â†‘  | 0.8600|Â±  |0.0349|
| - stem                                                   |      2|none             |      |acc        |â†‘  | 0.5576|Â±  |0.0085|
|  - abstract_algebra                                      |      1|none             |     0|acc        |â†‘  | 0.3500|Â±  |0.0479|
|  - anatomy                                               |      1|none             |     0|acc        |â†‘  | 0.5852|Â±  |0.0426|
|  - astronomy                                             |      1|none             |     0|acc        |â†‘  | 0.6382|Â±  |0.0391|
|  - college_biology                                       |      1|none             |     0|acc        |â†‘  | 0.7222|Â±  |0.0375|
|  - college_chemistry                                     |      1|none             |     0|acc        |â†‘  | 0.4700|Â±  |0.0502|
|  - college_computer_science                              |      1|none             |     0|acc        |â†‘  | 0.5400|Â±  |0.0501|
|  - college_mathematics                                   |      1|none             |     0|acc        |â†‘  | 0.4500|Â±  |0.0500|
|  - college_physics                                       |      1|none             |     0|acc        |â†‘  | 0.4020|Â±  |0.0488|
|  - computer_security                                     |      1|none             |     0|acc        |â†‘  | 0.7700|Â±  |0.0423|
|  - conceptual_physics                                    |      1|none             |     0|acc        |â†‘  | 0.6596|Â±  |0.0310|
|  - electrical_engineering                                |      1|none             |     0|acc        |â†‘  | 0.6414|Â±  |0.0400|
|  - elementary_mathematics                                |      1|none             |     0|acc        |â†‘  | 0.5159|Â±  |0.0257|
|  - high_school_biology                                   |      1|none             |     0|acc        |â†‘  | 0.7935|Â±  |0.0230|
|  - high_school_chemistry                                 |      1|none             |     0|acc        |â†‘  | 0.5567|Â±  |0.0350|
|  - high_school_computer_science                          |      1|none             |     0|acc        |â†‘  | 0.7100|Â±  |0.0456|
|  - high_school_mathematics                               |      1|none             |     0|acc        |â†‘  | 0.3481|Â±  |0.0290|
|  - high_school_physics                                   |      1|none             |     0|acc        |â†‘  | 0.4238|Â±  |0.0403|
|  - high_school_statistics                                |      1|none             |     0|acc        |â†‘  | 0.4583|Â±  |0.0340|
|  - machine_learning                                      |      1|none             |     0|acc        |â†‘  | 0.4375|Â±  |0.0471|
|nq_open                                                   |      4|remove_whitespace|     0|exact_match|â†‘  | 0.1781|Â±  |0.0064|
|openbookqa                                                |      1|none             |     0|acc        |â†‘  | 0.3200|Â±  |0.0209|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.4220|Â±  |0.0221|
|piqa                                                      |      1|none             |     0|acc        |â†‘  | 0.7856|Â±  |0.0096|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.8014|Â±  |0.0093|
|qnli                                                      |      1|none             |     0|acc        |â†‘  | 0.5986|Â±  |0.0066|
|sciq                                                      |      1|none             |     0|acc        |â†‘  | 0.9540|Â±  |0.0066|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.9410|Â±  |0.0075|
|triviaqa                                                  |      3|remove_whitespace|     0|exact_match|â†‘  | 0.4952|Â±  |0.0037|
|truthfulqa_gen                                            |      3|none             |     0|bleu_acc   |â†‘  | 0.5129|Â±  |0.0175|
|                                                          |       |none             |     0|bleu_diff  |â†‘  | 8.5287|Â±  |1.1119|
|                                                          |       |none             |     0|bleu_max   |â†‘  |33.0037|Â±  |0.8799|
|                                                          |       |none             |     0|rouge1_acc |â†‘  | 0.4835|Â±  |0.0175|
|                                                          |       |none             |     0|rouge1_diff|â†‘  |12.2235|Â±  |1.5196|
|                                                          |       |none             |     0|rouge1_max |â†‘  |57.2896|Â±  |1.0008|
|                                                          |       |none             |     0|rouge2_acc |â†‘  | 0.4370|Â±  |0.0174|
|                                                          |       |none             |     0|rouge2_diff|â†‘  |12.2809|Â±  |1.6323|
|                                                          |       |none             |     0|rouge2_max |â†‘  |44.2123|Â±  |1.2026|
|                                                          |       |none             |     0|rougeL_acc |â†‘  | 0.4663|Â±  |0.0175|
|                                                          |       |none             |     0|rougeL_diff|â†‘  |12.0600|Â±  |1.5359|
|                                                          |       |none             |     0|rougeL_max |â†‘  |55.2797|Â±  |1.0310|
|truthfulqa_mc1                                            |      2|none             |     0|acc        |â†‘  | 0.2999|Â±  |0.0160|
|truthfulqa_mc2                                            |      3|none             |     0|acc        |â†‘  | 0.4408|Â±  |0.0148|
|winogrande                                                |      1|none             |     0|acc        |â†‘  | 0.7206|Â±  |0.0126|

|      Groups      |Version|  Filter  |n-shot|  Metric   |   |Value |   |Stderr|
|------------------|------:|----------|------|-----------|---|-----:|---|-----:|
|bbh               |      3|get-answer|      |exact_match|â†‘  |0.5755|Â±  |0.0055|
|mmlu              |      2|none      |      |acc        |â†‘  |0.6243|Â±  |0.0038|
| - humanities     |      2|none      |      |acc        |â†‘  |0.5528|Â±  |0.0067|
| - other          |      2|none      |      |acc        |â†‘  |0.6849|Â±  |0.0081|
| - social sciences|      2|none      |      |acc        |â†‘  |0.7407|Â±  |0.0077|
| - stem           |      2|none      |      |acc        |â†‘  |0.5576|Â±  |0.0085|

01-ai_Yi-1.5-6B,: 4h 28m 24s
âœ… Benchmark completed for 01-ai_Yi-1.5-6B,

ðŸ”¥ Starting benchmark for Qwen_Qwen2.5-7B-Instruct-1M,
Passed argument batch_size = auto:1. Detecting largest batch size
Determined largest batch size: 1
Passed argument batch_size = auto. Detecting largest batch size
Determined Largest batch size: 1
hf (pretrained=/mnt/data8tb/Documents/llm/llm_models/Qwen_Qwen2.5-7B-Instruct-1M,), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto (1)
|                          Tasks                           |Version|     Filter      |n-shot|  Metric   |   | Value |   |Stderr|
|----------------------------------------------------------|------:|-----------------|-----:|-----------|---|------:|---|-----:|
|anli_r1                                                   |      1|none             |     0|acc        |â†‘  | 0.5850|Â±  |0.0156|
|anli_r2                                                   |      1|none             |     0|acc        |â†‘  | 0.5330|Â±  |0.0158|
|anli_r3                                                   |      1|none             |     0|acc        |â†‘  | 0.5567|Â±  |0.0143|
|arc_challenge                                             |      1|none             |     0|acc        |â†‘  | 0.5503|Â±  |0.0145|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.5853|Â±  |0.0144|
|bbh                                                       |      3|get-answer       |      |exact_match|â†‘  | 0.2772|Â±  |0.0043|
| - bbh_cot_fewshot_boolean_expressions                    |      4|get-answer       |     3|exact_match|â†‘  | 0.0000|Â±  |0.0000|
| - bbh_cot_fewshot_causal_judgement                       |      4|get-answer       |     3|exact_match|â†‘  | 0.5668|Â±  |0.0363|
| - bbh_cot_fewshot_date_understanding                     |      4|get-answer       |     3|exact_match|â†‘  | 0.7200|Â±  |0.0285|
| - bbh_cot_fewshot_disambiguation_qa                      |      4|get-answer       |     3|exact_match|â†‘  | 0.0000|Â±  |0.0000|
| - bbh_cot_fewshot_dyck_languages                         |      4|get-answer       |     3|exact_match|â†‘  | 0.0440|Â±  |0.0130|
| - bbh_cot_fewshot_formal_fallacies                       |      4|get-answer       |     3|exact_match|â†‘  | 0.0480|Â±  |0.0135|
| - bbh_cot_fewshot_geometric_shapes                       |      4|get-answer       |     3|exact_match|â†‘  | 0.1480|Â±  |0.0225|
| - bbh_cot_fewshot_hyperbaton                             |      4|get-answer       |     3|exact_match|â†‘  | 0.0040|Â±  |0.0040|
| - bbh_cot_fewshot_logical_deduction_five_objects         |      4|get-answer       |     3|exact_match|â†‘  | 0.4240|Â±  |0.0313|
| - bbh_cot_fewshot_logical_deduction_seven_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.3320|Â±  |0.0298|
| - bbh_cot_fewshot_logical_deduction_three_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.8440|Â±  |0.0230|
| - bbh_cot_fewshot_movie_recommendation                   |      4|get-answer       |     3|exact_match|â†‘  | 0.5440|Â±  |0.0316|
| - bbh_cot_fewshot_multistep_arithmetic_two               |      4|get-answer       |     3|exact_match|â†‘  | 0.0040|Â±  |0.0040|
| - bbh_cot_fewshot_navigate                               |      4|get-answer       |     3|exact_match|â†‘  | 0.0000|Â±  |0.0000|
| - bbh_cot_fewshot_object_counting                        |      4|get-answer       |     3|exact_match|â†‘  | 0.3280|Â±  |0.0298|
| - bbh_cot_fewshot_penguins_in_a_table                    |      4|get-answer       |     3|exact_match|â†‘  | 0.5616|Â±  |0.0412|
| - bbh_cot_fewshot_reasoning_about_colored_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.0120|Â±  |0.0069|
| - bbh_cot_fewshot_ruin_names                             |      4|get-answer       |     3|exact_match|â†‘  | 0.2920|Â±  |0.0288|
| - bbh_cot_fewshot_salient_translation_error_detection    |      4|get-answer       |     3|exact_match|â†‘  | 0.5120|Â±  |0.0317|
| - bbh_cot_fewshot_snarks                                 |      4|get-answer       |     3|exact_match|â†‘  | 0.7247|Â±  |0.0336|
| - bbh_cot_fewshot_sports_understanding                   |      4|get-answer       |     3|exact_match|â†‘  | 0.8800|Â±  |0.0206|
| - bbh_cot_fewshot_temporal_sequences                     |      4|get-answer       |     3|exact_match|â†‘  | 0.0000|Â±  |0.0000|
| - bbh_cot_fewshot_tracking_shuffled_objects_five_objects |      4|get-answer       |     3|exact_match|â†‘  | 0.2200|Â±  |0.0263|
| - bbh_cot_fewshot_tracking_shuffled_objects_seven_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.0920|Â±  |0.0183|
| - bbh_cot_fewshot_tracking_shuffled_objects_three_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.3280|Â±  |0.0298|
| - bbh_cot_fewshot_web_of_lies                            |      4|get-answer       |     3|exact_match|â†‘  | 0.0000|Â±  |0.0000|
| - bbh_cot_fewshot_word_sorting                           |      4|get-answer       |     3|exact_match|â†‘  | 0.1760|Â±  |0.0241|
|boolq                                                     |      2|none             |     0|acc        |â†‘  | 0.8526|Â±  |0.0062|
|drop                                                      |      3|none             |     0|em         |â†‘  | 0.0023|Â±  |0.0005|
|                                                          |       |none             |     0|f1         |â†‘  | 0.0570|Â±  |0.0014|
|gpqa_diamond_cot_n_shot                                   |      2|flexible-extract |     0|exact_match|â†‘  | 0.1414|Â±  |0.0248|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0101|Â±  |0.0071|
|gpqa_diamond_cot_zeroshot                                 |      1|flexible-extract |     0|exact_match|â†‘  | 0.1111|Â±  |0.0224|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_generative_n_shot                            |      2|flexible-extract |     0|exact_match|â†‘  | 0.2374|Â±  |0.0303|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_n_shot                                       |      2|none             |     0|acc        |â†‘  | 0.3030|Â±  |0.0327|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3030|Â±  |0.0327|
|gpqa_diamond_zeroshot                                     |      1|none             |     0|acc        |â†‘  | 0.2980|Â±  |0.0326|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2980|Â±  |0.0326|
|gpqa_extended_cot_n_shot                                  |      2|flexible-extract |     0|exact_match|â†‘  | 0.1758|Â±  |0.0163|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0018|Â±  |0.0018|
|gpqa_extended_cot_zeroshot                                |      1|flexible-extract |     0|exact_match|â†‘  | 0.1190|Â±  |0.0139|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_generative_n_shot                           |      2|flexible-extract |     0|exact_match|â†‘  | 0.2143|Â±  |0.0176|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_n_shot                                      |      2|none             |     0|acc        |â†‘  | 0.3388|Â±  |0.0203|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3388|Â±  |0.0203|
|gpqa_extended_zeroshot                                    |      1|none             |     0|acc        |â†‘  | 0.3388|Â±  |0.0203|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3388|Â±  |0.0203|
|gpqa_main_cot_n_shot                                      |      2|flexible-extract |     0|exact_match|â†‘  | 0.1719|Â±  |0.0178|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0022|Â±  |0.0022|
|gpqa_main_cot_zeroshot                                    |      1|flexible-extract |     0|exact_match|â†‘  | 0.1138|Â±  |0.0150|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_generative_n_shot                               |      2|flexible-extract |     0|exact_match|â†‘  | 0.2299|Â±  |0.0199|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_n_shot                                          |      2|none             |     0|acc        |â†‘  | 0.3326|Â±  |0.0223|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3326|Â±  |0.0223|
|gpqa_main_zeroshot                                        |      1|none             |     0|acc        |â†‘  | 0.3393|Â±  |0.0224|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3393|Â±  |0.0224|
|gsm8k                                                     |      3|flexible-extract |     5|exact_match|â†‘  | 0.8234|Â±  |0.0105|
|                                                          |       |strict-match     |     5|exact_match|â†‘  | 0.7953|Â±  |0.0111|
|hellaswag                                                 |      1|none             |     0|acc        |â†‘  | 0.5987|Â±  |0.0049|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.7900|Â±  |0.0041|
|mmlu                                                      |      2|none             |      |acc        |â†‘  | 0.7166|Â±  |0.0036|
| - humanities                                             |      2|none             |      |acc        |â†‘  | 0.6361|Â±  |0.0066|
|  - formal_logic                                          |      1|none             |     0|acc        |â†‘  | 0.5000|Â±  |0.0447|
|  - high_school_european_history                          |      1|none             |     0|acc        |â†‘  | 0.8182|Â±  |0.0301|
|  - high_school_us_history                                |      1|none             |     0|acc        |â†‘  | 0.8627|Â±  |0.0242|
|  - high_school_world_history                             |      1|none             |     0|acc        |â†‘  | 0.8650|Â±  |0.0222|
|  - international_law                                     |      1|none             |     0|acc        |â†‘  | 0.8347|Â±  |0.0339|
|  - jurisprudence                                         |      1|none             |     0|acc        |â†‘  | 0.7778|Â±  |0.0402|
|  - logical_fallacies                                     |      1|none             |     0|acc        |â†‘  | 0.8344|Â±  |0.0292|
|  - moral_disputes                                        |      1|none             |     0|acc        |â†‘  | 0.7832|Â±  |0.0222|
|  - moral_scenarios                                       |      1|none             |     0|acc        |â†‘  | 0.4201|Â±  |0.0165|
|  - philosophy                                            |      1|none             |     0|acc        |â†‘  | 0.7492|Â±  |0.0246|
|  - prehistory                                            |      1|none             |     0|acc        |â†‘  | 0.8086|Â±  |0.0219|
|  - professional_law                                      |      1|none             |     0|acc        |â†‘  | 0.5248|Â±  |0.0128|
|  - world_religions                                       |      1|none             |     0|acc        |â†‘  | 0.8538|Â±  |0.0271|
| - other                                                  |      2|none             |      |acc        |â†‘  | 0.7634|Â±  |0.0074|
|  - business_ethics                                       |      1|none             |     0|acc        |â†‘  | 0.7600|Â±  |0.0429|
|  - clinical_knowledge                                    |      1|none             |     0|acc        |â†‘  | 0.7736|Â±  |0.0258|
|  - college_medicine                                      |      1|none             |     0|acc        |â†‘  | 0.7052|Â±  |0.0348|
|  - global_facts                                          |      1|none             |     0|acc        |â†‘  | 0.4700|Â±  |0.0502|
|  - human_aging                                           |      1|none             |     0|acc        |â†‘  | 0.7220|Â±  |0.0301|
|  - management                                            |      1|none             |     0|acc        |â†‘  | 0.8641|Â±  |0.0339|
|  - marketing                                             |      1|none             |     0|acc        |â†‘  | 0.9188|Â±  |0.0179|
|  - medical_genetics                                      |      1|none             |     0|acc        |â†‘  | 0.8500|Â±  |0.0359|
|  - miscellaneous                                         |      1|none             |     0|acc        |â†‘  | 0.8493|Â±  |0.0128|
|  - nutrition                                             |      1|none             |     0|acc        |â†‘  | 0.7843|Â±  |0.0236|
|  - professional_accounting                               |      1|none             |     0|acc        |â†‘  | 0.6064|Â±  |0.0291|
|  - professional_medicine                                 |      1|none             |     0|acc        |â†‘  | 0.7831|Â±  |0.0250|
|  - virology                                              |      1|none             |     0|acc        |â†‘  | 0.5000|Â±  |0.0389|
| - social sciences                                        |      2|none             |      |acc        |â†‘  | 0.8229|Â±  |0.0068|
|  - econometrics                                          |      1|none             |     0|acc        |â†‘  | 0.6754|Â±  |0.0440|
|  - high_school_geography                                 |      1|none             |     0|acc        |â†‘  | 0.8636|Â±  |0.0245|
|  - high_school_government_and_politics                   |      1|none             |     0|acc        |â†‘  | 0.9223|Â±  |0.0193|
|  - high_school_macroeconomics                            |      1|none             |     0|acc        |â†‘  | 0.7821|Â±  |0.0209|
|  - high_school_microeconomics                            |      1|none             |     0|acc        |â†‘  | 0.8655|Â±  |0.0222|
|  - high_school_psychology                                |      1|none             |     0|acc        |â†‘  | 0.8936|Â±  |0.0132|
|  - human_sexuality                                       |      1|none             |     0|acc        |â†‘  | 0.8092|Â±  |0.0345|
|  - professional_psychology                               |      1|none             |     0|acc        |â†‘  | 0.7598|Â±  |0.0173|
|  - public_relations                                      |      1|none             |     0|acc        |â†‘  | 0.7091|Â±  |0.0435|
|  - security_studies                                      |      1|none             |     0|acc        |â†‘  | 0.7796|Â±  |0.0265|
|  - sociology                                             |      1|none             |     0|acc        |â†‘  | 0.8955|Â±  |0.0216|
|  - us_foreign_policy                                     |      1|none             |     0|acc        |â†‘  | 0.8800|Â±  |0.0327|
| - stem                                                   |      2|none             |      |acc        |â†‘  | 0.6870|Â±  |0.0080|
|  - abstract_algebra                                      |      1|none             |     0|acc        |â†‘  | 0.5600|Â±  |0.0499|
|  - anatomy                                               |      1|none             |     0|acc        |â†‘  | 0.6963|Â±  |0.0397|
|  - astronomy                                             |      1|none             |     0|acc        |â†‘  | 0.8158|Â±  |0.0315|
|  - college_biology                                       |      1|none             |     0|acc        |â†‘  | 0.8542|Â±  |0.0295|
|  - college_chemistry                                     |      1|none             |     0|acc        |â†‘  | 0.5500|Â±  |0.0500|
|  - college_computer_science                              |      1|none             |     0|acc        |â†‘  | 0.7200|Â±  |0.0451|
|  - college_mathematics                                   |      1|none             |     0|acc        |â†‘  | 0.4400|Â±  |0.0499|
|  - college_physics                                       |      1|none             |     0|acc        |â†‘  | 0.5392|Â±  |0.0496|
|  - computer_security                                     |      1|none             |     0|acc        |â†‘  | 0.8000|Â±  |0.0402|
|  - conceptual_physics                                    |      1|none             |     0|acc        |â†‘  | 0.7277|Â±  |0.0291|
|  - electrical_engineering                                |      1|none             |     0|acc        |â†‘  | 0.7172|Â±  |0.0375|
|  - elementary_mathematics                                |      1|none             |     0|acc        |â†‘  | 0.6534|Â±  |0.0245|
|  - high_school_biology                                   |      1|none             |     0|acc        |â†‘  | 0.8806|Â±  |0.0184|
|  - high_school_chemistry                                 |      1|none             |     0|acc        |â†‘  | 0.6601|Â±  |0.0333|
|  - high_school_computer_science                          |      1|none             |     0|acc        |â†‘  | 0.8400|Â±  |0.0368|
|  - high_school_mathematics                               |      1|none             |     0|acc        |â†‘  | 0.5630|Â±  |0.0302|
|  - high_school_physics                                   |      1|none             |     0|acc        |â†‘  | 0.5695|Â±  |0.0404|
|  - high_school_statistics                                |      1|none             |     0|acc        |â†‘  | 0.6898|Â±  |0.0315|
|  - machine_learning                                      |      1|none             |     0|acc        |â†‘  | 0.5625|Â±  |0.0471|
|nq_open                                                   |      4|remove_whitespace|     0|exact_match|â†‘  | 0.1576|Â±  |0.0061|
|openbookqa                                                |      1|none             |     0|acc        |â†‘  | 0.3580|Â±  |0.0215|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.4800|Â±  |0.0224|
|piqa                                                      |      1|none             |     0|acc        |â†‘  | 0.8009|Â±  |0.0093|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.8161|Â±  |0.0090|
|qnli                                                      |      1|none             |     0|acc        |â†‘  | 0.6782|Â±  |0.0063|
|sciq                                                      |      1|none             |     0|acc        |â†‘  | 0.9630|Â±  |0.0060|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.9500|Â±  |0.0069|
|triviaqa                                                  |      3|remove_whitespace|     0|exact_match|â†‘  | 0.4205|Â±  |0.0037|
|truthfulqa_gen                                            |      3|none             |     0|bleu_acc   |â†‘  | 0.4908|Â±  |0.0175|
|                                                          |       |none             |     0|bleu_diff  |â†‘  | 0.0819|Â±  |0.3040|
|                                                          |       |none             |     0|bleu_max   |â†‘  |10.5933|Â±  |0.4922|
|                                                          |       |none             |     0|rouge1_acc |â†‘  | 0.5067|Â±  |0.0175|
|                                                          |       |none             |     0|rouge1_diff|â†‘  | 0.1716|Â±  |0.4296|
|                                                          |       |none             |     0|rouge1_max |â†‘  |31.0117|Â±  |0.6908|
|                                                          |       |none             |     0|rouge2_acc |â†‘  | 0.3953|Â±  |0.0171|
|                                                          |       |none             |     0|rouge2_diff|â†‘  |-0.6488|Â±  |0.4941|
|                                                          |       |none             |     0|rouge2_max |â†‘  |18.4106|Â±  |0.6992|
|                                                          |       |none             |     0|rougeL_acc |â†‘  | 0.4982|Â±  |0.0175|
|                                                          |       |none             |     0|rougeL_diff|â†‘  |-0.0253|Â±  |0.4232|
|                                                          |       |none             |     0|rougeL_max |â†‘  |27.9487|Â±  |0.6871|
|truthfulqa_mc1                                            |      2|none             |     0|acc        |â†‘  | 0.4259|Â±  |0.0173|
|truthfulqa_mc2                                            |      3|none             |     0|acc        |â†‘  | 0.6001|Â±  |0.0154|
|winogrande                                                |      1|none             |     0|acc        |â†‘  | 0.7277|Â±  |0.0125|

|      Groups      |Version|  Filter  |n-shot|  Metric   |   |Value |   |Stderr|
|------------------|------:|----------|------|-----------|---|-----:|---|-----:|
|bbh               |      3|get-answer|      |exact_match|â†‘  |0.2772|Â±  |0.0043|
|mmlu              |      2|none      |      |acc        |â†‘  |0.7166|Â±  |0.0036|
| - humanities     |      2|none      |      |acc        |â†‘  |0.6361|Â±  |0.0066|
| - other          |      2|none      |      |acc        |â†‘  |0.7634|Â±  |0.0074|
| - social sciences|      2|none      |      |acc        |â†‘  |0.8229|Â±  |0.0068|
| - stem           |      2|none      |      |acc        |â†‘  |0.6870|Â±  |0.0080|

Qwen_Qwen2.5-7B-Instruct-1M,: 11h 17m 22s
âœ… Benchmark completed for Qwen_Qwen2.5-7B-Instruct-1M,

ðŸ”¥ Starting benchmark for Qwen_Qwen3-8B,
Passed argument batch_size = auto:1. Detecting largest batch size
Determined largest batch size: 1
Passed argument batch_size = auto. Detecting largest batch size
Determined Largest batch size: 1
hf (pretrained=/mnt/data8tb/Documents/llm/llm_models/Qwen_Qwen3-8B,), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto (1)
|                          Tasks                           |Version|     Filter      |n-shot|  Metric   |   | Value |   |Stderr|
|----------------------------------------------------------|------:|-----------------|-----:|-----------|---|------:|---|-----:|
|anli_r1                                                   |      1|none             |     0|acc        |â†‘  | 0.6690|Â±  |0.0149|
|anli_r2                                                   |      1|none             |     0|acc        |â†‘  | 0.5420|Â±  |0.0158|
|anli_r3                                                   |      1|none             |     0|acc        |â†‘  | 0.5558|Â±  |0.0143|
|arc_challenge                                             |      1|none             |     0|acc        |â†‘  | 0.5546|Â±  |0.0145|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.5623|Â±  |0.0145|
|bbh                                                       |      3|get-answer       |      |exact_match|â†‘  | 0.7976|Â±  |0.0045|
| - bbh_cot_fewshot_boolean_expressions                    |      4|get-answer       |     3|exact_match|â†‘  | 0.9800|Â±  |0.0089|
| - bbh_cot_fewshot_causal_judgement                       |      4|get-answer       |     3|exact_match|â†‘  | 0.5936|Â±  |0.0360|
| - bbh_cot_fewshot_date_understanding                     |      4|get-answer       |     3|exact_match|â†‘  | 0.8520|Â±  |0.0225|
| - bbh_cot_fewshot_disambiguation_qa                      |      4|get-answer       |     3|exact_match|â†‘  | 0.5960|Â±  |0.0311|
| - bbh_cot_fewshot_dyck_languages                         |      4|get-answer       |     3|exact_match|â†‘  | 0.4440|Â±  |0.0315|
| - bbh_cot_fewshot_formal_fallacies                       |      4|get-answer       |     3|exact_match|â†‘  | 0.6800|Â±  |0.0296|
| - bbh_cot_fewshot_geometric_shapes                       |      4|get-answer       |     3|exact_match|â†‘  | 0.5920|Â±  |0.0311|
| - bbh_cot_fewshot_hyperbaton                             |      4|get-answer       |     3|exact_match|â†‘  | 0.9480|Â±  |0.0141|
| - bbh_cot_fewshot_logical_deduction_five_objects         |      4|get-answer       |     3|exact_match|â†‘  | 0.7160|Â±  |0.0286|
| - bbh_cot_fewshot_logical_deduction_seven_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.4080|Â±  |0.0311|
| - bbh_cot_fewshot_logical_deduction_three_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.9640|Â±  |0.0118|
| - bbh_cot_fewshot_movie_recommendation                   |      4|get-answer       |     3|exact_match|â†‘  | 0.7640|Â±  |0.0269|
| - bbh_cot_fewshot_multistep_arithmetic_two               |      4|get-answer       |     3|exact_match|â†‘  | 0.9840|Â±  |0.0080|
| - bbh_cot_fewshot_navigate                               |      4|get-answer       |     3|exact_match|â†‘  | 0.9600|Â±  |0.0124|
| - bbh_cot_fewshot_object_counting                        |      4|get-answer       |     3|exact_match|â†‘  | 0.9000|Â±  |0.0190|
| - bbh_cot_fewshot_penguins_in_a_table                    |      4|get-answer       |     3|exact_match|â†‘  | 0.8836|Â±  |0.0266|
| - bbh_cot_fewshot_reasoning_about_colored_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.8640|Â±  |0.0217|
| - bbh_cot_fewshot_ruin_names                             |      4|get-answer       |     3|exact_match|â†‘  | 0.7960|Â±  |0.0255|
| - bbh_cot_fewshot_salient_translation_error_detection    |      4|get-answer       |     3|exact_match|â†‘  | 0.5880|Â±  |0.0312|
| - bbh_cot_fewshot_snarks                                 |      4|get-answer       |     3|exact_match|â†‘  | 0.7079|Â±  |0.0342|
| - bbh_cot_fewshot_sports_understanding                   |      4|get-answer       |     3|exact_match|â†‘  | 0.8640|Â±  |0.0217|
| - bbh_cot_fewshot_temporal_sequences                     |      4|get-answer       |     3|exact_match|â†‘  | 0.9280|Â±  |0.0164|
| - bbh_cot_fewshot_tracking_shuffled_objects_five_objects |      4|get-answer       |     3|exact_match|â†‘  | 0.9640|Â±  |0.0118|
| - bbh_cot_fewshot_tracking_shuffled_objects_seven_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.8960|Â±  |0.0193|
| - bbh_cot_fewshot_tracking_shuffled_objects_three_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.9960|Â±  |0.0040|
| - bbh_cot_fewshot_web_of_lies                            |      4|get-answer       |     3|exact_match|â†‘  | 1.0000|Â±  |0.0000|
| - bbh_cot_fewshot_word_sorting                           |      4|get-answer       |     3|exact_match|â†‘  | 0.6240|Â±  |0.0307|
|boolq                                                     |      2|none             |     0|acc        |â†‘  | 0.8657|Â±  |0.0060|
|drop                                                      |      3|none             |     0|em         |â†‘  | 0.0034|Â±  |0.0006|
|                                                          |       |none             |     0|f1         |â†‘  | 0.1099|Â±  |0.0020|
|gpqa_diamond_cot_n_shot                                   |      2|flexible-extract |     0|exact_match|â†‘  | 0.1162|Â±  |0.0228|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0051|Â±  |0.0051|
|gpqa_diamond_cot_zeroshot                                 |      1|flexible-extract |     0|exact_match|â†‘  | 0.0354|Â±  |0.0132|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_generative_n_shot                            |      2|flexible-extract |     0|exact_match|â†‘  | 0.2828|Â±  |0.0321|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_n_shot                                       |      2|none             |     0|acc        |â†‘  | 0.3434|Â±  |0.0338|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3434|Â±  |0.0338|
|gpqa_diamond_zeroshot                                     |      1|none             |     0|acc        |â†‘  | 0.3434|Â±  |0.0338|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3434|Â±  |0.0338|
|gpqa_extended_cot_n_shot                                  |      2|flexible-extract |     0|exact_match|â†‘  | 0.1227|Â±  |0.0141|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0073|Â±  |0.0037|
|gpqa_extended_cot_zeroshot                                |      1|flexible-extract |     0|exact_match|â†‘  | 0.0733|Â±  |0.0112|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_generative_n_shot                           |      2|flexible-extract |     0|exact_match|â†‘  | 0.2692|Â±  |0.0190|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0037|Â±  |0.0026|
|gpqa_extended_n_shot                                      |      2|none             |     0|acc        |â†‘  | 0.3608|Â±  |0.0206|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3608|Â±  |0.0206|
|gpqa_extended_zeroshot                                    |      1|none             |     0|acc        |â†‘  | 0.3828|Â±  |0.0208|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3828|Â±  |0.0208|
|gpqa_main_cot_n_shot                                      |      2|flexible-extract |     0|exact_match|â†‘  | 0.1295|Â±  |0.0159|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0134|Â±  |0.0054|
|gpqa_main_cot_zeroshot                                    |      1|flexible-extract |     0|exact_match|â†‘  | 0.0647|Â±  |0.0116|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_generative_n_shot                               |      2|flexible-extract |     0|exact_match|â†‘  | 0.2634|Â±  |0.0208|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0022|Â±  |0.0022|
|gpqa_main_n_shot                                          |      2|none             |     0|acc        |â†‘  | 0.3884|Â±  |0.0231|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3884|Â±  |0.0231|
|gpqa_main_zeroshot                                        |      1|none             |     0|acc        |â†‘  | 0.3504|Â±  |0.0226|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3504|Â±  |0.0226|
|gsm8k                                                     |      3|flexible-extract |     5|exact_match|â†‘  | 0.8787|Â±  |0.0090|
|                                                          |       |strict-match     |     5|exact_match|â†‘  | 0.8726|Â±  |0.0092|
|hellaswag                                                 |      1|none             |     0|acc        |â†‘  | 0.5711|Â±  |0.0049|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.7487|Â±  |0.0043|
|mmlu                                                      |      2|none             |      |acc        |â†‘  | 0.7290|Â±  |0.0035|
| - humanities                                             |      2|none             |      |acc        |â†‘  | 0.6383|Â±  |0.0065|
|  - formal_logic                                          |      1|none             |     0|acc        |â†‘  | 0.6032|Â±  |0.0438|
|  - high_school_european_history                          |      1|none             |     0|acc        |â†‘  | 0.8788|Â±  |0.0255|
|  - high_school_us_history                                |      1|none             |     0|acc        |â†‘  | 0.8824|Â±  |0.0226|
|  - high_school_world_history                             |      1|none             |     0|acc        |â†‘  | 0.8734|Â±  |0.0216|
|  - international_law                                     |      1|none             |     0|acc        |â†‘  | 0.8182|Â±  |0.0352|
|  - jurisprudence                                         |      1|none             |     0|acc        |â†‘  | 0.8056|Â±  |0.0383|
|  - logical_fallacies                                     |      1|none             |     0|acc        |â†‘  | 0.8405|Â±  |0.0288|
|  - moral_disputes                                        |      1|none             |     0|acc        |â†‘  | 0.7399|Â±  |0.0236|
|  - moral_scenarios                                       |      1|none             |     0|acc        |â†‘  | 0.4101|Â±  |0.0164|
|  - philosophy                                            |      1|none             |     0|acc        |â†‘  | 0.7878|Â±  |0.0232|
|  - prehistory                                            |      1|none             |     0|acc        |â†‘  | 0.8395|Â±  |0.0204|
|  - professional_law                                      |      1|none             |     0|acc        |â†‘  | 0.5111|Â±  |0.0128|
|  - world_religions                                       |      1|none             |     0|acc        |â†‘  | 0.8655|Â±  |0.0262|
| - other                                                  |      2|none             |      |acc        |â†‘  | 0.7702|Â±  |0.0072|
|  - business_ethics                                       |      1|none             |     0|acc        |â†‘  | 0.7500|Â±  |0.0435|
|  - clinical_knowledge                                    |      1|none             |     0|acc        |â†‘  | 0.7925|Â±  |0.0250|
|  - college_medicine                                      |      1|none             |     0|acc        |â†‘  | 0.7572|Â±  |0.0327|
|  - global_facts                                          |      1|none             |     0|acc        |â†‘  | 0.4100|Â±  |0.0494|
|  - human_aging                                           |      1|none             |     0|acc        |â†‘  | 0.7309|Â±  |0.0298|
|  - management                                            |      1|none             |     0|acc        |â†‘  | 0.8835|Â±  |0.0318|
|  - marketing                                             |      1|none             |     0|acc        |â†‘  | 0.9274|Â±  |0.0170|
|  - medical_genetics                                      |      1|none             |     0|acc        |â†‘  | 0.8100|Â±  |0.0394|
|  - miscellaneous                                         |      1|none             |     0|acc        |â†‘  | 0.8557|Â±  |0.0126|
|  - nutrition                                             |      1|none             |     0|acc        |â†‘  | 0.7810|Â±  |0.0237|
|  - professional_accounting                               |      1|none             |     0|acc        |â†‘  | 0.5745|Â±  |0.0295|
|  - professional_medicine                                 |      1|none             |     0|acc        |â†‘  | 0.8199|Â±  |0.0233|
|  - virology                                              |      1|none             |     0|acc        |â†‘  | 0.5422|Â±  |0.0388|
| - social sciences                                        |      2|none             |      |acc        |â†‘  | 0.8294|Â±  |0.0067|
|  - econometrics                                          |      1|none             |     0|acc        |â†‘  | 0.6754|Â±  |0.0440|
|  - high_school_geography                                 |      1|none             |     0|acc        |â†‘  | 0.8535|Â±  |0.0252|
|  - high_school_government_and_politics                   |      1|none             |     0|acc        |â†‘  | 0.9326|Â±  |0.0181|
|  - high_school_macroeconomics                            |      1|none             |     0|acc        |â†‘  | 0.7949|Â±  |0.0205|
|  - high_school_microeconomics                            |      1|none             |     0|acc        |â†‘  | 0.9160|Â±  |0.0180|
|  - high_school_psychology                                |      1|none             |     0|acc        |â†‘  | 0.9064|Â±  |0.0125|
|  - human_sexuality                                       |      1|none             |     0|acc        |â†‘  | 0.8473|Â±  |0.0315|
|  - professional_psychology                               |      1|none             |     0|acc        |â†‘  | 0.7533|Â±  |0.0174|
|  - public_relations                                      |      1|none             |     0|acc        |â†‘  | 0.7091|Â±  |0.0435|
|  - security_studies                                      |      1|none             |     0|acc        |â†‘  | 0.7755|Â±  |0.0267|
|  - sociology                                             |      1|none             |     0|acc        |â†‘  | 0.8856|Â±  |0.0225|
|  - us_foreign_policy                                     |      1|none             |     0|acc        |â†‘  | 0.8600|Â±  |0.0349|
| - stem                                                   |      2|none             |      |acc        |â†‘  | 0.7257|Â±  |0.0077|
|  - abstract_algebra                                      |      1|none             |     0|acc        |â†‘  | 0.5700|Â±  |0.0498|
|  - anatomy                                               |      1|none             |     0|acc        |â†‘  | 0.7037|Â±  |0.0394|
|  - astronomy                                             |      1|none             |     0|acc        |â†‘  | 0.8684|Â±  |0.0275|
|  - college_biology                                       |      1|none             |     0|acc        |â†‘  | 0.8542|Â±  |0.0295|
|  - college_chemistry                                     |      1|none             |     0|acc        |â†‘  | 0.5900|Â±  |0.0494|
|  - college_computer_science                              |      1|none             |     0|acc        |â†‘  | 0.7300|Â±  |0.0446|
|  - college_mathematics                                   |      1|none             |     0|acc        |â†‘  | 0.5900|Â±  |0.0494|
|  - college_physics                                       |      1|none             |     0|acc        |â†‘  | 0.5686|Â±  |0.0493|
|  - computer_security                                     |      1|none             |     0|acc        |â†‘  | 0.8200|Â±  |0.0386|
|  - conceptual_physics                                    |      1|none             |     0|acc        |â†‘  | 0.8255|Â±  |0.0248|
|  - electrical_engineering                                |      1|none             |     0|acc        |â†‘  | 0.7310|Â±  |0.0370|
|  - elementary_mathematics                                |      1|none             |     0|acc        |â†‘  | 0.7011|Â±  |0.0236|
|  - high_school_biology                                   |      1|none             |     0|acc        |â†‘  | 0.9129|Â±  |0.0160|
|  - high_school_chemistry                                 |      1|none             |     0|acc        |â†‘  | 0.7241|Â±  |0.0314|
|  - high_school_computer_science                          |      1|none             |     0|acc        |â†‘  | 0.8800|Â±  |0.0327|
|  - high_school_mathematics                               |      1|none             |     0|acc        |â†‘  | 0.5074|Â±  |0.0305|
|  - high_school_physics                                   |      1|none             |     0|acc        |â†‘  | 0.7020|Â±  |0.0373|
|  - high_school_statistics                                |      1|none             |     0|acc        |â†‘  | 0.7315|Â±  |0.0302|
|  - machine_learning                                      |      1|none             |     0|acc        |â†‘  | 0.5893|Â±  |0.0467|
|nq_open                                                   |      4|remove_whitespace|     0|exact_match|â†‘  | 0.0737|Â±  |0.0043|
|openbookqa                                                |      1|none             |     0|acc        |â†‘  | 0.3160|Â±  |0.0208|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.4180|Â±  |0.0221|
|piqa                                                      |      1|none             |     0|acc        |â†‘  | 0.7644|Â±  |0.0099|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.7753|Â±  |0.0097|
|qnli                                                      |      1|none             |     0|acc        |â†‘  | 0.7818|Â±  |0.0056|
|sciq                                                      |      1|none             |     0|acc        |â†‘  | 0.9670|Â±  |0.0057|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.9580|Â±  |0.0063|
|triviaqa                                                  |      3|remove_whitespace|     0|exact_match|â†‘  | 0.3206|Â±  |0.0035|
|truthfulqa_gen                                            |      3|none             |     0|bleu_acc   |â†‘  | 0.6022|Â±  |0.0171|
|                                                          |       |none             |     0|bleu_diff  |â†‘  |16.3978|Â±  |1.0966|
|                                                          |       |none             |     0|bleu_max   |â†‘  |35.5543|Â±  |0.8937|
|                                                          |       |none             |     0|rouge1_acc |â†‘  | 0.6083|Â±  |0.0171|
|                                                          |       |none             |     0|rouge1_diff|â†‘  |23.2877|Â±  |1.5315|
|                                                          |       |none             |     0|rouge1_max |â†‘  |62.0759|Â±  |0.9426|
|                                                          |       |none             |     0|rouge2_acc |â†‘  | 0.5704|Â±  |0.0173|
|                                                          |       |none             |     0|rouge2_diff|â†‘  |23.9536|Â±  |1.6740|
|                                                          |       |none             |     0|rouge2_max |â†‘  |50.5507|Â±  |1.1698|
|                                                          |       |none             |     0|rougeL_acc |â†‘  | 0.6120|Â±  |0.0171|
|                                                          |       |none             |     0|rougeL_diff|â†‘  |23.4988|Â±  |1.5413|
|                                                          |       |none             |     0|rougeL_max |â†‘  |59.9677|Â±  |0.9917|
|truthfulqa_mc1                                            |      2|none             |     0|acc        |â†‘  | 0.3635|Â±  |0.0168|
|truthfulqa_mc2                                            |      3|none             |     0|acc        |â†‘  | 0.5431|Â±  |0.0158|
|winogrande                                                |      1|none             |     0|acc        |â†‘  | 0.6803|Â±  |0.0131|

|      Groups      |Version|  Filter  |n-shot|  Metric   |   |Value |   |Stderr|
|------------------|------:|----------|------|-----------|---|-----:|---|-----:|
|bbh               |      3|get-answer|      |exact_match|â†‘  |0.7976|Â±  |0.0045|
|mmlu              |      2|none      |      |acc        |â†‘  |0.7290|Â±  |0.0035|
| - humanities     |      2|none      |      |acc        |â†‘  |0.6383|Â±  |0.0065|
| - other          |      2|none      |      |acc        |â†‘  |0.7702|Â±  |0.0072|
| - social sciences|      2|none      |      |acc        |â†‘  |0.8294|Â±  |0.0067|
| - stem           |      2|none      |      |acc        |â†‘  |0.7257|Â±  |0.0077|

Qwen_Qwen3-8B,: 15h 32m 7s
âœ… Benchmark completed for Qwen_Qwen3-8B,

ðŸ”¥ Starting benchmark for Qwen_Qwen3-8B-FP8,
Passed argument batch_size = auto:1. Detecting largest batch size
Qwen_Qwen3-8B-FP8,: 0h 5m 29s
âœ… Benchmark completed for Qwen_Qwen3-8B-FP8,

ðŸ”¥ Starting benchmark for Qwen_Qwen2.5-Math-7B-Instruct,
Passed argument batch_size = auto:1. Detecting largest batch size
Determined largest batch size: 4
Passed argument batch_size = auto. Detecting largest batch size
Determined Largest batch size: 4
hf (pretrained=/mnt/data8tb/Documents/llm/llm_models/Qwen_Qwen2.5-Math-7B-Instruct,), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto (4)
|                          Tasks                           |Version|     Filter      |n-shot|  Metric   |   | Value |   |Stderr|
|----------------------------------------------------------|------:|-----------------|-----:|-----------|---|------:|---|-----:|
|anli_r1                                                   |      1|none             |     0|acc        |â†‘  | 0.4310|Â±  |0.0157|
|anli_r2                                                   |      1|none             |     0|acc        |â†‘  | 0.4150|Â±  |0.0156|
|anli_r3                                                   |      1|none             |     0|acc        |â†‘  | 0.4292|Â±  |0.0143|
|arc_challenge                                             |      1|none             |     0|acc        |â†‘  | 0.4061|Â±  |0.0144|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.4309|Â±  |0.0145|
|bbh                                                       |      3|get-answer       |      |exact_match|â†‘  | 0.6140|Â±  |0.0051|
| - bbh_cot_fewshot_boolean_expressions                    |      4|get-answer       |     3|exact_match|â†‘  | 0.9040|Â±  |0.0187|
| - bbh_cot_fewshot_causal_judgement                       |      4|get-answer       |     3|exact_match|â†‘  | 0.4545|Â±  |0.0365|
| - bbh_cot_fewshot_date_understanding                     |      4|get-answer       |     3|exact_match|â†‘  | 0.6680|Â±  |0.0298|
| - bbh_cot_fewshot_disambiguation_qa                      |      4|get-answer       |     3|exact_match|â†‘  | 0.5800|Â±  |0.0313|
| - bbh_cot_fewshot_dyck_languages                         |      4|get-answer       |     3|exact_match|â†‘  | 0.0480|Â±  |0.0135|
| - bbh_cot_fewshot_formal_fallacies                       |      4|get-answer       |     3|exact_match|â†‘  | 0.5840|Â±  |0.0312|
| - bbh_cot_fewshot_geometric_shapes                       |      4|get-answer       |     3|exact_match|â†‘  | 0.4480|Â±  |0.0315|
| - bbh_cot_fewshot_hyperbaton                             |      4|get-answer       |     3|exact_match|â†‘  | 0.6520|Â±  |0.0302|
| - bbh_cot_fewshot_logical_deduction_five_objects         |      4|get-answer       |     3|exact_match|â†‘  | 0.5440|Â±  |0.0316|
| - bbh_cot_fewshot_logical_deduction_seven_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.3960|Â±  |0.0310|
| - bbh_cot_fewshot_logical_deduction_three_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.9160|Â±  |0.0176|
| - bbh_cot_fewshot_movie_recommendation                   |      4|get-answer       |     3|exact_match|â†‘  | 0.5360|Â±  |0.0316|
| - bbh_cot_fewshot_multistep_arithmetic_two               |      4|get-answer       |     3|exact_match|â†‘  | 0.9480|Â±  |0.0141|
| - bbh_cot_fewshot_navigate                               |      4|get-answer       |     3|exact_match|â†‘  | 0.9320|Â±  |0.0160|
| - bbh_cot_fewshot_object_counting                        |      4|get-answer       |     3|exact_match|â†‘  | 0.7960|Â±  |0.0255|
| - bbh_cot_fewshot_penguins_in_a_table                    |      4|get-answer       |     3|exact_match|â†‘  | 0.7671|Â±  |0.0351|
| - bbh_cot_fewshot_reasoning_about_colored_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.8200|Â±  |0.0243|
| - bbh_cot_fewshot_ruin_names                             |      4|get-answer       |     3|exact_match|â†‘  | 0.3800|Â±  |0.0308|
| - bbh_cot_fewshot_salient_translation_error_detection    |      4|get-answer       |     3|exact_match|â†‘  | 0.3240|Â±  |0.0297|
| - bbh_cot_fewshot_snarks                                 |      4|get-answer       |     3|exact_match|â†‘  | 0.5674|Â±  |0.0372|
| - bbh_cot_fewshot_sports_understanding                   |      4|get-answer       |     3|exact_match|â†‘  | 0.6360|Â±  |0.0305|
| - bbh_cot_fewshot_temporal_sequences                     |      4|get-answer       |     3|exact_match|â†‘  | 0.2560|Â±  |0.0277|
| - bbh_cot_fewshot_tracking_shuffled_objects_five_objects |      4|get-answer       |     3|exact_match|â†‘  | 0.7640|Â±  |0.0269|
| - bbh_cot_fewshot_tracking_shuffled_objects_seven_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.6600|Â±  |0.0300|
| - bbh_cot_fewshot_tracking_shuffled_objects_three_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.9280|Â±  |0.0164|
| - bbh_cot_fewshot_web_of_lies                            |      4|get-answer       |     3|exact_match|â†‘  | 0.9960|Â±  |0.0040|
| - bbh_cot_fewshot_word_sorting                           |      4|get-answer       |     3|exact_match|â†‘  | 0.0840|Â±  |0.0176|
|boolq                                                     |      2|none             |     0|acc        |â†‘  | 0.6061|Â±  |0.0085|
|drop                                                      |      3|none             |     0|em         |â†‘  | 0.0001|Â±  |0.0001|
|                                                          |       |none             |     0|f1         |â†‘  | 0.0273|Â±  |0.0008|
|gpqa_diamond_cot_n_shot                                   |      2|flexible-extract |     0|exact_match|â†‘  | 0.1010|Â±  |0.0215|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_cot_zeroshot                                 |      1|flexible-extract |     0|exact_match|â†‘  | 0.0909|Â±  |0.0205|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_generative_n_shot                            |      2|flexible-extract |     0|exact_match|â†‘  | 0.0960|Â±  |0.0210|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_n_shot                                       |      2|none             |     0|acc        |â†‘  | 0.3182|Â±  |0.0332|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3182|Â±  |0.0332|
|gpqa_diamond_zeroshot                                     |      1|none             |     0|acc        |â†‘  | 0.3182|Â±  |0.0332|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3182|Â±  |0.0332|
|gpqa_extended_cot_n_shot                                  |      2|flexible-extract |     0|exact_match|â†‘  | 0.1154|Â±  |0.0137|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_cot_zeroshot                                |      1|flexible-extract |     0|exact_match|â†‘  | 0.1227|Â±  |0.0141|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_generative_n_shot                           |      2|flexible-extract |     0|exact_match|â†‘  | 0.1136|Â±  |0.0136|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_n_shot                                      |      2|none             |     0|acc        |â†‘  | 0.3278|Â±  |0.0201|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3278|Â±  |0.0201|
|gpqa_extended_zeroshot                                    |      1|none             |     0|acc        |â†‘  | 0.3498|Â±  |0.0204|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3498|Â±  |0.0204|
|gpqa_main_cot_n_shot                                      |      2|flexible-extract |     0|exact_match|â†‘  | 0.1116|Â±  |0.0149|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_cot_zeroshot                                    |      1|flexible-extract |     0|exact_match|â†‘  | 0.1205|Â±  |0.0154|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_generative_n_shot                               |      2|flexible-extract |     0|exact_match|â†‘  | 0.1295|Â±  |0.0159|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_n_shot                                          |      2|none             |     0|acc        |â†‘  | 0.3147|Â±  |0.0220|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3147|Â±  |0.0220|
|gpqa_main_zeroshot                                        |      1|none             |     0|acc        |â†‘  | 0.2879|Â±  |0.0214|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2879|Â±  |0.0214|
|gsm8k                                                     |      3|flexible-extract |     5|exact_match|â†‘  | 0.8931|Â±  |0.0085|
|                                                          |       |strict-match     |     5|exact_match|â†‘  | 0.8901|Â±  |0.0086|
|hellaswag                                                 |      1|none             |     0|acc        |â†‘  | 0.4395|Â±  |0.0050|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.5881|Â±  |0.0049|
|mmlu                                                      |      2|none             |      |acc        |â†‘  | 0.5372|Â±  |0.0041|
| - humanities                                             |      2|none             |      |acc        |â†‘  | 0.4389|Â±  |0.0070|
|  - formal_logic                                          |      1|none             |     0|acc        |â†‘  | 0.4921|Â±  |0.0447|
|  - high_school_european_history                          |      1|none             |     0|acc        |â†‘  | 0.5818|Â±  |0.0385|
|  - high_school_us_history                                |      1|none             |     0|acc        |â†‘  | 0.5343|Â±  |0.0350|
|  - high_school_world_history                             |      1|none             |     0|acc        |â†‘  | 0.6160|Â±  |0.0317|
|  - international_law                                     |      1|none             |     0|acc        |â†‘  | 0.6529|Â±  |0.0435|
|  - jurisprudence                                         |      1|none             |     0|acc        |â†‘  | 0.5833|Â±  |0.0477|
|  - logical_fallacies                                     |      1|none             |     0|acc        |â†‘  | 0.6196|Â±  |0.0381|
|  - moral_disputes                                        |      1|none             |     0|acc        |â†‘  | 0.5318|Â±  |0.0269|
|  - moral_scenarios                                       |      1|none             |     0|acc        |â†‘  | 0.2771|Â±  |0.0150|
|  - philosophy                                            |      1|none             |     0|acc        |â†‘  | 0.5563|Â±  |0.0282|
|  - prehistory                                            |      1|none             |     0|acc        |â†‘  | 0.5031|Â±  |0.0278|
|  - professional_law                                      |      1|none             |     0|acc        |â†‘  | 0.3677|Â±  |0.0123|
|  - world_religions                                       |      1|none             |     0|acc        |â†‘  | 0.4503|Â±  |0.0382|
| - other                                                  |      2|none             |      |acc        |â†‘  | 0.5340|Â±  |0.0087|
|  - business_ethics                                       |      1|none             |     0|acc        |â†‘  | 0.6000|Â±  |0.0492|
|  - clinical_knowledge                                    |      1|none             |     0|acc        |â†‘  | 0.5132|Â±  |0.0308|
|  - college_medicine                                      |      1|none             |     0|acc        |â†‘  | 0.4913|Â±  |0.0381|
|  - global_facts                                          |      1|none             |     0|acc        |â†‘  | 0.2400|Â±  |0.0429|
|  - human_aging                                           |      1|none             |     0|acc        |â†‘  | 0.5471|Â±  |0.0334|
|  - management                                            |      1|none             |     0|acc        |â†‘  | 0.7379|Â±  |0.0435|
|  - marketing                                             |      1|none             |     0|acc        |â†‘  | 0.7607|Â±  |0.0280|
|  - medical_genetics                                      |      1|none             |     0|acc        |â†‘  | 0.5000|Â±  |0.0503|
|  - miscellaneous                                         |      1|none             |     0|acc        |â†‘  | 0.6054|Â±  |0.0175|
|  - nutrition                                             |      1|none             |     0|acc        |â†‘  | 0.5065|Â±  |0.0286|
|  - professional_accounting                               |      1|none             |     0|acc        |â†‘  | 0.4113|Â±  |0.0294|
|  - professional_medicine                                 |      1|none             |     0|acc        |â†‘  | 0.3934|Â±  |0.0297|
|  - virology                                              |      1|none             |     0|acc        |â†‘  | 0.4578|Â±  |0.0388|
| - social sciences                                        |      2|none             |      |acc        |â†‘  | 0.6233|Â±  |0.0086|
|  - econometrics                                          |      1|none             |     0|acc        |â†‘  | 0.5263|Â±  |0.0470|
|  - high_school_geography                                 |      1|none             |     0|acc        |â†‘  | 0.6111|Â±  |0.0347|
|  - high_school_government_and_politics                   |      1|none             |     0|acc        |â†‘  | 0.6114|Â±  |0.0352|
|  - high_school_macroeconomics                            |      1|none             |     0|acc        |â†‘  | 0.6410|Â±  |0.0243|
|  - high_school_microeconomics                            |      1|none             |     0|acc        |â†‘  | 0.7479|Â±  |0.0282|
|  - high_school_psychology                                |      1|none             |     0|acc        |â†‘  | 0.7193|Â±  |0.0193|
|  - human_sexuality                                       |      1|none             |     0|acc        |â†‘  | 0.4962|Â±  |0.0439|
|  - professional_psychology                               |      1|none             |     0|acc        |â†‘  | 0.5049|Â±  |0.0202|
|  - public_relations                                      |      1|none             |     0|acc        |â†‘  | 0.5818|Â±  |0.0472|
|  - security_studies                                      |      1|none             |     0|acc        |â†‘  | 0.6286|Â±  |0.0309|
|  - sociology                                             |      1|none             |     0|acc        |â†‘  | 0.6866|Â±  |0.0328|
|  - us_foreign_policy                                     |      1|none             |     0|acc        |â†‘  | 0.6900|Â±  |0.0465|
| - stem                                                   |      2|none             |      |acc        |â†‘  | 0.6032|Â±  |0.0086|
|  - abstract_algebra                                      |      1|none             |     0|acc        |â†‘  | 0.4800|Â±  |0.0502|
|  - anatomy                                               |      1|none             |     0|acc        |â†‘  | 0.4519|Â±  |0.0430|
|  - astronomy                                             |      1|none             |     0|acc        |â†‘  | 0.6382|Â±  |0.0391|
|  - college_biology                                       |      1|none             |     0|acc        |â†‘  | 0.5000|Â±  |0.0418|
|  - college_chemistry                                     |      1|none             |     0|acc        |â†‘  | 0.5200|Â±  |0.0502|
|  - college_computer_science                              |      1|none             |     0|acc        |â†‘  | 0.5700|Â±  |0.0498|
|  - college_mathematics                                   |      1|none             |     0|acc        |â†‘  | 0.4900|Â±  |0.0502|
|  - college_physics                                       |      1|none             |     0|acc        |â†‘  | 0.4412|Â±  |0.0494|
|  - computer_security                                     |      1|none             |     0|acc        |â†‘  | 0.6500|Â±  |0.0479|
|  - conceptual_physics                                    |      1|none             |     0|acc        |â†‘  | 0.7234|Â±  |0.0292|
|  - electrical_engineering                                |      1|none             |     0|acc        |â†‘  | 0.6276|Â±  |0.0403|
|  - elementary_mathematics                                |      1|none             |     0|acc        |â†‘  | 0.6958|Â±  |0.0237|
|  - high_school_biology                                   |      1|none             |     0|acc        |â†‘  | 0.6903|Â±  |0.0263|
|  - high_school_chemistry                                 |      1|none             |     0|acc        |â†‘  | 0.6059|Â±  |0.0344|
|  - high_school_computer_science                          |      1|none             |     0|acc        |â†‘  | 0.6700|Â±  |0.0473|
|  - high_school_mathematics                               |      1|none             |     0|acc        |â†‘  | 0.5444|Â±  |0.0304|
|  - high_school_physics                                   |      1|none             |     0|acc        |â†‘  | 0.5497|Â±  |0.0406|
|  - high_school_statistics                                |      1|none             |     0|acc        |â†‘  | 0.6852|Â±  |0.0317|
|  - machine_learning                                      |      1|none             |     0|acc        |â†‘  | 0.4464|Â±  |0.0472|
|nq_open                                                   |      4|remove_whitespace|     0|exact_match|â†‘  | 0.0199|Â±  |0.0023|
|openbookqa                                                |      1|none             |     0|acc        |â†‘  | 0.2380|Â±  |0.0191|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3340|Â±  |0.0211|
|piqa                                                      |      1|none             |     0|acc        |â†‘  | 0.6850|Â±  |0.0108|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.6855|Â±  |0.0108|
|qnli                                                      |      1|none             |     0|acc        |â†‘  | 0.6775|Â±  |0.0063|
|sciq                                                      |      1|none             |     0|acc        |â†‘  | 0.9110|Â±  |0.0090|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.8580|Â±  |0.0110|
|triviaqa                                                  |      3|remove_whitespace|     0|exact_match|â†‘  | 0.0075|Â±  |0.0006|
|truthfulqa_gen                                            |      3|none             |     0|bleu_acc   |â†‘  | 0.3978|Â±  |0.0171|
|                                                          |       |none             |     0|bleu_diff  |â†‘  | 0.7878|Â±  |0.5709|
|                                                          |       |none             |     0|bleu_max   |â†‘  |17.7316|Â±  |0.6218|
|                                                          |       |none             |     0|rouge1_acc |â†‘  | 0.4211|Â±  |0.0173|
|                                                          |       |none             |     0|rouge1_diff|â†‘  | 1.8283|Â±  |0.8791|
|                                                          |       |none             |     0|rouge1_max |â†‘  |42.6331|Â±  |0.8380|
|                                                          |       |none             |     0|rouge2_acc |â†‘  | 0.3537|Â±  |0.0167|
|                                                          |       |none             |     0|rouge2_diff|â†‘  | 1.4478|Â±  |0.9688|
|                                                          |       |none             |     0|rouge2_max |â†‘  |29.6721|Â±  |0.9118|
|                                                          |       |none             |     0|rougeL_acc |â†‘  | 0.4039|Â±  |0.0172|
|                                                          |       |none             |     0|rougeL_diff|â†‘  | 1.5558|Â±  |0.8801|
|                                                          |       |none             |     0|rougeL_max |â†‘  |40.0785|Â±  |0.8407|
|truthfulqa_mc1                                            |      2|none             |     0|acc        |â†‘  | 0.2987|Â±  |0.0160|
|truthfulqa_mc2                                            |      3|none             |     0|acc        |â†‘  | 0.4750|Â±  |0.0160|
|winogrande                                                |      1|none             |     0|acc        |â†‘  | 0.5793|Â±  |0.0139|

|      Groups      |Version|  Filter  |n-shot|  Metric   |   |Value |   |Stderr|
|------------------|------:|----------|------|-----------|---|-----:|---|-----:|
|bbh               |      3|get-answer|      |exact_match|â†‘  |0.6140|Â±  |0.0051|
|mmlu              |      2|none      |      |acc        |â†‘  |0.5372|Â±  |0.0041|
| - humanities     |      2|none      |      |acc        |â†‘  |0.4389|Â±  |0.0070|
| - other          |      2|none      |      |acc        |â†‘  |0.5340|Â±  |0.0087|
| - social sciences|      2|none      |      |acc        |â†‘  |0.6233|Â±  |0.0086|
| - stem           |      2|none      |      |acc        |â†‘  |0.6032|Â±  |0.0086|

Qwen_Qwen2.5-Math-7B-Instruct,: 5h 37m 20s
âœ… Benchmark completed for Qwen_Qwen2.5-Math-7B-Instruct,

ðŸ”¥ Starting benchmark for Qwen_Qwen2.5-Math-7B,
Passed argument batch_size = auto:1. Detecting largest batch size
Determined largest batch size: 4
Passed argument batch_size = auto. Detecting largest batch size
Determined Largest batch size: 4
hf (pretrained=/mnt/data8tb/Documents/llm/llm_models/Qwen_Qwen2.5-Math-7B,), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto (4)
|                          Tasks                           |Version|     Filter      |n-shot|  Metric   |   | Value |   |Stderr|
|----------------------------------------------------------|------:|-----------------|-----:|-----------|---|------:|---|-----:|
|anli_r1                                                   |      1|none             |     0|acc        |â†‘  | 0.3870|Â±  |0.0154|
|anli_r2                                                   |      1|none             |     0|acc        |â†‘  | 0.4070|Â±  |0.0155|
|anli_r3                                                   |      1|none             |     0|acc        |â†‘  | 0.3825|Â±  |0.0140|
|arc_challenge                                             |      1|none             |     0|acc        |â†‘  | 0.4855|Â±  |0.0146|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.5026|Â±  |0.0146|
|bbh                                                       |      3|get-answer       |      |exact_match|â†‘  | 0.6724|Â±  |0.0050|
| - bbh_cot_fewshot_boolean_expressions                    |      4|get-answer       |     3|exact_match|â†‘  | 0.9120|Â±  |0.0180|
| - bbh_cot_fewshot_causal_judgement                       |      4|get-answer       |     3|exact_match|â†‘  | 0.5027|Â±  |0.0367|
| - bbh_cot_fewshot_date_understanding                     |      4|get-answer       |     3|exact_match|â†‘  | 0.7640|Â±  |0.0269|
| - bbh_cot_fewshot_disambiguation_qa                      |      4|get-answer       |     3|exact_match|â†‘  | 0.5080|Â±  |0.0317|
| - bbh_cot_fewshot_dyck_languages                         |      4|get-answer       |     3|exact_match|â†‘  | 0.1680|Â±  |0.0237|
| - bbh_cot_fewshot_formal_fallacies                       |      4|get-answer       |     3|exact_match|â†‘  | 0.5520|Â±  |0.0315|
| - bbh_cot_fewshot_geometric_shapes                       |      4|get-answer       |     3|exact_match|â†‘  | 0.4440|Â±  |0.0315|
| - bbh_cot_fewshot_hyperbaton                             |      4|get-answer       |     3|exact_match|â†‘  | 0.9080|Â±  |0.0183|
| - bbh_cot_fewshot_logical_deduction_five_objects         |      4|get-answer       |     3|exact_match|â†‘  | 0.5640|Â±  |0.0314|
| - bbh_cot_fewshot_logical_deduction_seven_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.3800|Â±  |0.0308|
| - bbh_cot_fewshot_logical_deduction_three_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.9080|Â±  |0.0183|
| - bbh_cot_fewshot_movie_recommendation                   |      4|get-answer       |     3|exact_match|â†‘  | 0.6160|Â±  |0.0308|
| - bbh_cot_fewshot_multistep_arithmetic_two               |      4|get-answer       |     3|exact_match|â†‘  | 0.9480|Â±  |0.0141|
| - bbh_cot_fewshot_navigate                               |      4|get-answer       |     3|exact_match|â†‘  | 0.9600|Â±  |0.0124|
| - bbh_cot_fewshot_object_counting                        |      4|get-answer       |     3|exact_match|â†‘  | 0.8680|Â±  |0.0215|
| - bbh_cot_fewshot_penguins_in_a_table                    |      4|get-answer       |     3|exact_match|â†‘  | 0.7808|Â±  |0.0344|
| - bbh_cot_fewshot_reasoning_about_colored_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.8840|Â±  |0.0203|
| - bbh_cot_fewshot_ruin_names                             |      4|get-answer       |     3|exact_match|â†‘  | 0.5040|Â±  |0.0317|
| - bbh_cot_fewshot_salient_translation_error_detection    |      4|get-answer       |     3|exact_match|â†‘  | 0.3480|Â±  |0.0302|
| - bbh_cot_fewshot_snarks                                 |      4|get-answer       |     3|exact_match|â†‘  | 0.5506|Â±  |0.0374|
| - bbh_cot_fewshot_sports_understanding                   |      4|get-answer       |     3|exact_match|â†‘  | 0.7400|Â±  |0.0278|
| - bbh_cot_fewshot_temporal_sequences                     |      4|get-answer       |     3|exact_match|â†‘  | 0.3080|Â±  |0.0293|
| - bbh_cot_fewshot_tracking_shuffled_objects_five_objects |      4|get-answer       |     3|exact_match|â†‘  | 0.8960|Â±  |0.0193|
| - bbh_cot_fewshot_tracking_shuffled_objects_seven_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.8520|Â±  |0.0225|
| - bbh_cot_fewshot_tracking_shuffled_objects_three_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.9600|Â±  |0.0124|
| - bbh_cot_fewshot_web_of_lies                            |      4|get-answer       |     3|exact_match|â†‘  | 1.0000|Â±  |0.0000|
| - bbh_cot_fewshot_word_sorting                           |      4|get-answer       |     3|exact_match|â†‘  | 0.2960|Â±  |0.0289|
|boolq                                                     |      2|none             |     0|acc        |â†‘  | 0.7456|Â±  |0.0076|
|drop                                                      |      3|none             |     0|em         |â†‘  | 0.0012|Â±  |0.0003|
|                                                          |       |none             |     0|f1         |â†‘  | 0.0432|Â±  |0.0011|
|gpqa_diamond_cot_n_shot                                   |      2|flexible-extract |     0|exact_match|â†‘  | 0.2222|Â±  |0.0296|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_cot_zeroshot                                 |      1|flexible-extract |     0|exact_match|â†‘  | 0.2121|Â±  |0.0291|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0051|Â±  |0.0051|
|gpqa_diamond_generative_n_shot                            |      2|flexible-extract |     0|exact_match|â†‘  | 0.2778|Â±  |0.0319|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_n_shot                                       |      2|none             |     0|acc        |â†‘  | 0.3333|Â±  |0.0336|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3333|Â±  |0.0336|
|gpqa_diamond_zeroshot                                     |      1|none             |     0|acc        |â†‘  | 0.3182|Â±  |0.0332|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3182|Â±  |0.0332|
|gpqa_extended_cot_n_shot                                  |      2|flexible-extract |     0|exact_match|â†‘  | 0.2161|Â±  |0.0176|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0055|Â±  |0.0032|
|gpqa_extended_cot_zeroshot                                |      1|flexible-extract |     0|exact_match|â†‘  | 0.2692|Â±  |0.0190|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0018|Â±  |0.0018|
|gpqa_extended_generative_n_shot                           |      2|flexible-extract |     0|exact_match|â†‘  | 0.3168|Â±  |0.0199|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_n_shot                                      |      2|none             |     0|acc        |â†‘  | 0.3040|Â±  |0.0197|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3040|Â±  |0.0197|
|gpqa_extended_zeroshot                                    |      1|none             |     0|acc        |â†‘  | 0.3297|Â±  |0.0201|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3297|Â±  |0.0201|
|gpqa_main_cot_n_shot                                      |      2|flexible-extract |     0|exact_match|â†‘  | 0.2188|Â±  |0.0196|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0022|Â±  |0.0022|
|gpqa_main_cot_zeroshot                                    |      1|flexible-extract |     0|exact_match|â†‘  | 0.2455|Â±  |0.0204|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0022|Â±  |0.0022|
|gpqa_main_generative_n_shot                               |      2|flexible-extract |     0|exact_match|â†‘  | 0.3237|Â±  |0.0221|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_n_shot                                          |      2|none             |     0|acc        |â†‘  | 0.3036|Â±  |0.0217|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3036|Â±  |0.0217|
|gpqa_main_zeroshot                                        |      1|none             |     0|acc        |â†‘  | 0.3080|Â±  |0.0218|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3080|Â±  |0.0218|
|gsm8k                                                     |      3|flexible-extract |     5|exact_match|â†‘  | 0.8491|Â±  |0.0099|
|                                                          |       |strict-match     |     5|exact_match|â†‘  | 0.8476|Â±  |0.0099|
|hellaswag                                                 |      1|none             |     0|acc        |â†‘  | 0.4907|Â±  |0.0050|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.6529|Â±  |0.0048|
|mmlu                                                      |      2|none             |      |acc        |â†‘  | 0.5799|Â±  |0.0040|
| - humanities                                             |      2|none             |      |acc        |â†‘  | 0.4742|Â±  |0.0069|
|  - formal_logic                                          |      1|none             |     0|acc        |â†‘  | 0.5159|Â±  |0.0447|
|  - high_school_european_history                          |      1|none             |     0|acc        |â†‘  | 0.6606|Â±  |0.0370|
|  - high_school_us_history                                |      1|none             |     0|acc        |â†‘  | 0.6225|Â±  |0.0340|
|  - high_school_world_history                             |      1|none             |     0|acc        |â†‘  | 0.6751|Â±  |0.0305|
|  - international_law                                     |      1|none             |     0|acc        |â†‘  | 0.6942|Â±  |0.0421|
|  - jurisprudence                                         |      1|none             |     0|acc        |â†‘  | 0.6296|Â±  |0.0467|
|  - logical_fallacies                                     |      1|none             |     0|acc        |â†‘  | 0.7055|Â±  |0.0358|
|  - moral_disputes                                        |      1|none             |     0|acc        |â†‘  | 0.6185|Â±  |0.0262|
|  - moral_scenarios                                       |      1|none             |     0|acc        |â†‘  | 0.2425|Â±  |0.0143|
|  - philosophy                                            |      1|none             |     0|acc        |â†‘  | 0.6206|Â±  |0.0276|
|  - prehistory                                            |      1|none             |     0|acc        |â†‘  | 0.5216|Â±  |0.0278|
|  - professional_law                                      |      1|none             |     0|acc        |â†‘  | 0.3963|Â±  |0.0125|
|  - world_religions                                       |      1|none             |     0|acc        |â†‘  | 0.5965|Â±  |0.0376|
| - other                                                  |      2|none             |      |acc        |â†‘  | 0.5845|Â±  |0.0086|
|  - business_ethics                                       |      1|none             |     0|acc        |â†‘  | 0.6400|Â±  |0.0482|
|  - clinical_knowledge                                    |      1|none             |     0|acc        |â†‘  | 0.5811|Â±  |0.0304|
|  - college_medicine                                      |      1|none             |     0|acc        |â†‘  | 0.5029|Â±  |0.0381|
|  - global_facts                                          |      1|none             |     0|acc        |â†‘  | 0.3400|Â±  |0.0476|
|  - human_aging                                           |      1|none             |     0|acc        |â†‘  | 0.6278|Â±  |0.0324|
|  - management                                            |      1|none             |     0|acc        |â†‘  | 0.6990|Â±  |0.0454|
|  - marketing                                             |      1|none             |     0|acc        |â†‘  | 0.8248|Â±  |0.0249|
|  - medical_genetics                                      |      1|none             |     0|acc        |â†‘  | 0.5700|Â±  |0.0498|
|  - miscellaneous                                         |      1|none             |     0|acc        |â†‘  | 0.6782|Â±  |0.0167|
|  - nutrition                                             |      1|none             |     0|acc        |â†‘  | 0.5654|Â±  |0.0284|
|  - professional_accounting                               |      1|none             |     0|acc        |â†‘  | 0.4681|Â±  |0.0298|
|  - professional_medicine                                 |      1|none             |     0|acc        |â†‘  | 0.3713|Â±  |0.0293|
|  - virology                                              |      1|none             |     0|acc        |â†‘  | 0.4699|Â±  |0.0389|
| - social sciences                                        |      2|none             |      |acc        |â†‘  | 0.6724|Â±  |0.0084|
|  - econometrics                                          |      1|none             |     0|acc        |â†‘  | 0.5877|Â±  |0.0463|
|  - high_school_geography                                 |      1|none             |     0|acc        |â†‘  | 0.7020|Â±  |0.0326|
|  - high_school_government_and_politics                   |      1|none             |     0|acc        |â†‘  | 0.7254|Â±  |0.0322|
|  - high_school_macroeconomics                            |      1|none             |     0|acc        |â†‘  | 0.6744|Â±  |0.0238|
|  - high_school_microeconomics                            |      1|none             |     0|acc        |â†‘  | 0.8193|Â±  |0.0250|
|  - high_school_psychology                                |      1|none             |     0|acc        |â†‘  | 0.7266|Â±  |0.0191|
|  - human_sexuality                                       |      1|none             |     0|acc        |â†‘  | 0.5878|Â±  |0.0432|
|  - professional_psychology                               |      1|none             |     0|acc        |â†‘  | 0.5621|Â±  |0.0201|
|  - public_relations                                      |      1|none             |     0|acc        |â†‘  | 0.6182|Â±  |0.0465|
|  - security_studies                                      |      1|none             |     0|acc        |â†‘  | 0.6367|Â±  |0.0308|
|  - sociology                                             |      1|none             |     0|acc        |â†‘  | 0.7413|Â±  |0.0310|
|  - us_foreign_policy                                     |      1|none             |     0|acc        |â†‘  | 0.7500|Â±  |0.0435|
| - stem                                                   |      2|none             |      |acc        |â†‘  | 0.6429|Â±  |0.0084|
|  - abstract_algebra                                      |      1|none             |     0|acc        |â†‘  | 0.5300|Â±  |0.0502|
|  - anatomy                                               |      1|none             |     0|acc        |â†‘  | 0.4370|Â±  |0.0428|
|  - astronomy                                             |      1|none             |     0|acc        |â†‘  | 0.7105|Â±  |0.0369|
|  - college_biology                                       |      1|none             |     0|acc        |â†‘  | 0.6389|Â±  |0.0402|
|  - college_chemistry                                     |      1|none             |     0|acc        |â†‘  | 0.5200|Â±  |0.0502|
|  - college_computer_science                              |      1|none             |     0|acc        |â†‘  | 0.6900|Â±  |0.0465|
|  - college_mathematics                                   |      1|none             |     0|acc        |â†‘  | 0.4800|Â±  |0.0502|
|  - college_physics                                       |      1|none             |     0|acc        |â†‘  | 0.4608|Â±  |0.0496|
|  - computer_security                                     |      1|none             |     0|acc        |â†‘  | 0.6700|Â±  |0.0473|
|  - conceptual_physics                                    |      1|none             |     0|acc        |â†‘  | 0.7447|Â±  |0.0285|
|  - electrical_engineering                                |      1|none             |     0|acc        |â†‘  | 0.6690|Â±  |0.0392|
|  - elementary_mathematics                                |      1|none             |     0|acc        |â†‘  | 0.7169|Â±  |0.0232|
|  - high_school_biology                                   |      1|none             |     0|acc        |â†‘  | 0.7581|Â±  |0.0244|
|  - high_school_chemistry                                 |      1|none             |     0|acc        |â†‘  | 0.6207|Â±  |0.0341|
|  - high_school_computer_science                          |      1|none             |     0|acc        |â†‘  | 0.7500|Â±  |0.0435|
|  - high_school_mathematics                               |      1|none             |     0|acc        |â†‘  | 0.5630|Â±  |0.0302|
|  - high_school_physics                                   |      1|none             |     0|acc        |â†‘  | 0.6026|Â±  |0.0400|
|  - high_school_statistics                                |      1|none             |     0|acc        |â†‘  | 0.7222|Â±  |0.0305|
|  - machine_learning                                      |      1|none             |     0|acc        |â†‘  | 0.4821|Â±  |0.0474|
|nq_open                                                   |      4|remove_whitespace|     0|exact_match|â†‘  | 0.0510|Â±  |0.0037|
|openbookqa                                                |      1|none             |     0|acc        |â†‘  | 0.2720|Â±  |0.0199|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3920|Â±  |0.0219|
|piqa                                                      |      1|none             |     0|acc        |â†‘  | 0.7285|Â±  |0.0104|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.7454|Â±  |0.0102|
|qnli                                                      |      1|none             |     0|acc        |â†‘  | 0.4981|Â±  |0.0068|
|sciq                                                      |      1|none             |     0|acc        |â†‘  | 0.9410|Â±  |0.0075|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.9290|Â±  |0.0081|
|triviaqa                                                  |      3|remove_whitespace|     0|exact_match|â†‘  | 0.2183|Â±  |0.0031|
|truthfulqa_gen                                            |      3|none             |     0|bleu_acc   |â†‘  | 0.3733|Â±  |0.0169|
|                                                          |       |none             |     0|bleu_diff  |â†‘  |-1.3812|Â±  |0.3689|
|                                                          |       |none             |     0|bleu_max   |â†‘  |13.2847|Â±  |0.4547|
|                                                          |       |none             |     0|rouge1_acc |â†‘  | 0.3733|Â±  |0.0169|
|                                                          |       |none             |     0|rouge1_diff|â†‘  |-2.1164|Â±  |0.5749|
|                                                          |       |none             |     0|rouge1_max |â†‘  |36.6193|Â±  |0.7424|
|                                                          |       |none             |     0|rouge2_acc |â†‘  | 0.3158|Â±  |0.0163|
|                                                          |       |none             |     0|rouge2_diff|â†‘  |-2.6990|Â±  |0.6541|
|                                                          |       |none             |     0|rouge2_max |â†‘  |24.5611|Â±  |0.7453|
|                                                          |       |none             |     0|rougeL_acc |â†‘  | 0.3733|Â±  |0.0169|
|                                                          |       |none             |     0|rougeL_diff|â†‘  |-2.1518|Â±  |0.5742|
|                                                          |       |none             |     0|rougeL_max |â†‘  |34.7410|Â±  |0.7352|
|truthfulqa_mc1                                            |      2|none             |     0|acc        |â†‘  | 0.3207|Â±  |0.0163|
|truthfulqa_mc2                                            |      3|none             |     0|acc        |â†‘  | 0.4832|Â±  |0.0150|
|winogrande                                                |      1|none             |     0|acc        |â†‘  | 0.6480|Â±  |0.0134|

|      Groups      |Version|  Filter  |n-shot|  Metric   |   |Value |   |Stderr|
|------------------|------:|----------|------|-----------|---|-----:|---|-----:|
|bbh               |      3|get-answer|      |exact_match|â†‘  |0.6724|Â±  |0.0050|
|mmlu              |      2|none      |      |acc        |â†‘  |0.5799|Â±  |0.0040|
| - humanities     |      2|none      |      |acc        |â†‘  |0.4742|Â±  |0.0069|
| - other          |      2|none      |      |acc        |â†‘  |0.5845|Â±  |0.0086|
| - social sciences|      2|none      |      |acc        |â†‘  |0.6724|Â±  |0.0084|
| - stem           |      2|none      |      |acc        |â†‘  |0.6429|Â±  |0.0084|

Qwen_Qwen2.5-Math-7B,: 27h 22m 7s
âœ… Benchmark completed for Qwen_Qwen2.5-Math-7B,


______________________
ðŸ”¥ Starting benchmark for Qwen_Qwen2.5-7B-Instruct
ðŸ”¥ Starting benchmark for Qwen_Qwen2.5-7B-Instruct
ðŸ”¥ Starting benchmark for Qwen_Qwen2.5-7B-Instruct
ðŸ”¥ Starting benchmark for Qwen_Qwen2.5-7B-Instruct
ðŸ”¥ Starting benchmark for Qwen_Qwen2.5-7B-Instruct
ðŸ”¥ Starting benchmark for Qwen_Qwen2.5-7B-Instruct
ðŸ”¥ Starting benchmark for Qwen_Qwen2.5-7B-Instruct
ðŸ”¥ Starting benchmark for Qwen_Qwen2.5-7B-Instruct
Passed argument batch_size = auto:5.0. Detecting largest batch size
Determined largest batch size: 1
Passed argument batch_size = auto:5.0. Detecting largest batch size
Determined largest batch size: 64
Passed argument batch_size = auto:5.0. Detecting largest batch size
Determined largest batch size: 1
Passed argument batch_size = auto:5.0. Detecting largest batch size
Determined largest batch size: 64
Passed argument batch_size = auto. Detecting largest batch size
Determined Largest batch size: 1
hf (pretrained=/mnt/data8tb/Documents/llm/llm_models/Qwen_Qwen2.5-7B-Instruct), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto:5 (1,64,64,64,64,64)
|                          Tasks                           |Version|     Filter      |n-shot|  Metric   |   | Value |   |Stderr|
|----------------------------------------------------------|------:|-----------------|-----:|-----------|---|------:|---|-----:|
|anli_r1                                                   |      1|none             |     0|acc        |â†‘  | 0.6840|Â±  |0.0147|
|anli_r2                                                   |      1|none             |     0|acc        |â†‘  | 0.5440|Â±  |0.0158|
|anli_r3                                                   |      1|none             |     0|acc        |â†‘  | 0.5492|Â±  |0.0144|
|arc_challenge                                             |      1|none             |     0|acc        |â†‘  | 0.5265|Â±  |0.0146|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.5529|Â±  |0.0145|
|bbh                                                       |      3|get-answer       |      |exact_match|â†‘  | 0.4534|Â±  |0.0051|
| - bbh_cot_fewshot_boolean_expressions                    |      4|get-answer       |     3|exact_match|â†‘  | 0.8120|Â±  |0.0248|
| - bbh_cot_fewshot_causal_judgement                       |      4|get-answer       |     3|exact_match|â†‘  | 0.3529|Â±  |0.0350|
| - bbh_cot_fewshot_date_understanding                     |      4|get-answer       |     3|exact_match|â†‘  | 0.4560|Â±  |0.0316|
| - bbh_cot_fewshot_disambiguation_qa                      |      4|get-answer       |     3|exact_match|â†‘  | 0.6160|Â±  |0.0308|
| - bbh_cot_fewshot_dyck_languages                         |      4|get-answer       |     3|exact_match|â†‘  | 0.0080|Â±  |0.0056|
| - bbh_cot_fewshot_formal_fallacies                       |      4|get-answer       |     3|exact_match|â†‘  | 0.2280|Â±  |0.0266|
| - bbh_cot_fewshot_geometric_shapes                       |      4|get-answer       |     3|exact_match|â†‘  | 0.1040|Â±  |0.0193|
| - bbh_cot_fewshot_hyperbaton                             |      4|get-answer       |     3|exact_match|â†‘  | 0.3920|Â±  |0.0309|
| - bbh_cot_fewshot_logical_deduction_five_objects         |      4|get-answer       |     3|exact_match|â†‘  | 0.3880|Â±  |0.0309|
| - bbh_cot_fewshot_logical_deduction_seven_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.1800|Â±  |0.0243|
| - bbh_cot_fewshot_logical_deduction_three_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.8480|Â±  |0.0228|
| - bbh_cot_fewshot_movie_recommendation                   |      4|get-answer       |     3|exact_match|â†‘  | 0.0360|Â±  |0.0118|
| - bbh_cot_fewshot_multistep_arithmetic_two               |      4|get-answer       |     3|exact_match|â†‘  | 0.3040|Â±  |0.0292|
| - bbh_cot_fewshot_navigate                               |      4|get-answer       |     3|exact_match|â†‘  | 0.8800|Â±  |0.0206|
| - bbh_cot_fewshot_object_counting                        |      4|get-answer       |     3|exact_match|â†‘  | 0.5760|Â±  |0.0313|
| - bbh_cot_fewshot_penguins_in_a_table                    |      4|get-answer       |     3|exact_match|â†‘  | 0.2055|Â±  |0.0336|
| - bbh_cot_fewshot_reasoning_about_colored_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.5560|Â±  |0.0315|
| - bbh_cot_fewshot_ruin_names                             |      4|get-answer       |     3|exact_match|â†‘  | 0.0360|Â±  |0.0118|
| - bbh_cot_fewshot_salient_translation_error_detection    |      4|get-answer       |     3|exact_match|â†‘  | 0.2680|Â±  |0.0281|
| - bbh_cot_fewshot_snarks                                 |      4|get-answer       |     3|exact_match|â†‘  | 0.5281|Â±  |0.0375|
| - bbh_cot_fewshot_sports_understanding                   |      4|get-answer       |     3|exact_match|â†‘  | 0.7760|Â±  |0.0264|
| - bbh_cot_fewshot_temporal_sequences                     |      4|get-answer       |     3|exact_match|â†‘  | 0.7640|Â±  |0.0269|
| - bbh_cot_fewshot_tracking_shuffled_objects_five_objects |      4|get-answer       |     3|exact_match|â†‘  | 0.7600|Â±  |0.0271|
| - bbh_cot_fewshot_tracking_shuffled_objects_seven_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.5840|Â±  |0.0312|
| - bbh_cot_fewshot_tracking_shuffled_objects_three_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.8920|Â±  |0.0197|
| - bbh_cot_fewshot_web_of_lies                            |      4|get-answer       |     3|exact_match|â†‘  | 0.3200|Â±  |0.0296|
| - bbh_cot_fewshot_word_sorting                           |      4|get-answer       |     3|exact_match|â†‘  | 0.2640|Â±  |0.0279|
|boolq                                                     |      2|none             |     0|acc        |â†‘  | 0.8633|Â±  |0.0060|
|drop                                                      |      3|none             |     0|em         |â†‘  | 0.0028|Â±  |0.0005|
|                                                          |       |none             |     0|f1         |â†‘  | 0.0713|Â±  |0.0014|
|gpqa_diamond_cot_n_shot                                   |      2|flexible-extract |     0|exact_match|â†‘  | 0.1414|Â±  |0.0248|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_cot_zeroshot                                 |      1|flexible-extract |     0|exact_match|â†‘  | 0.0859|Â±  |0.0200|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_generative_n_shot                            |      2|flexible-extract |     0|exact_match|â†‘  | 0.2879|Â±  |0.0323|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_n_shot                                       |      2|none             |     0|acc        |â†‘  | 0.3283|Â±  |0.0335|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3283|Â±  |0.0335|
|gpqa_diamond_zeroshot                                     |      1|none             |     0|acc        |â†‘  | 0.3131|Â±  |0.0330|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3131|Â±  |0.0330|
|gpqa_extended_cot_n_shot                                  |      2|flexible-extract |     0|exact_match|â†‘  | 0.1612|Â±  |0.0158|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_cot_zeroshot                                |      1|flexible-extract |     0|exact_match|â†‘  | 0.0989|Â±  |0.0128|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_generative_n_shot                           |      2|flexible-extract |     0|exact_match|â†‘  | 0.2546|Â±  |0.0187|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_n_shot                                      |      2|none             |     0|acc        |â†‘  | 0.3242|Â±  |0.0200|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3242|Â±  |0.0200|
|gpqa_extended_zeroshot                                    |      1|none             |     0|acc        |â†‘  | 0.3388|Â±  |0.0203|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3388|Â±  |0.0203|
|gpqa_main_cot_n_shot                                      |      2|flexible-extract |     0|exact_match|â†‘  | 0.1540|Â±  |0.0171|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_cot_zeroshot                                    |      1|flexible-extract |     0|exact_match|â†‘  | 0.1094|Â±  |0.0148|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_generative_n_shot                               |      2|flexible-extract |     0|exact_match|â†‘  | 0.2746|Â±  |0.0211|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_n_shot                                          |      2|none             |     0|acc        |â†‘  | 0.3237|Â±  |0.0221|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3237|Â±  |0.0221|
|gpqa_main_zeroshot                                        |      1|none             |     0|acc        |â†‘  | 0.3326|Â±  |0.0223|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3326|Â±  |0.0223|
|gsm8k                                                     |      3|flexible-extract |     5|exact_match|â†‘  | 0.8256|Â±  |0.0105|
|                                                          |       |strict-match     |     5|exact_match|â†‘  | 0.7582|Â±  |0.0118|
|hellaswag                                                 |      1|none             |     0|acc        |â†‘  | 0.6198|Â±  |0.0048|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.8039|Â±  |0.0040|
|mmlu                                                      |      2|none             |      |acc        |â†‘  | 0.7175|Â±  |0.0036|
| - humanities                                             |      2|none             |      |acc        |â†‘  | 0.6351|Â±  |0.0066|
|  - formal_logic                                          |      1|none             |     0|acc        |â†‘  | 0.5635|Â±  |0.0444|
|  - high_school_european_history                          |      1|none             |     0|acc        |â†‘  | 0.8485|Â±  |0.0280|
|  - high_school_us_history                                |      1|none             |     0|acc        |â†‘  | 0.9020|Â±  |0.0209|
|  - high_school_world_history                             |      1|none             |     0|acc        |â†‘  | 0.8734|Â±  |0.0216|
|  - international_law                                     |      1|none             |     0|acc        |â†‘  | 0.8099|Â±  |0.0358|
|  - jurisprudence                                         |      1|none             |     0|acc        |â†‘  | 0.7963|Â±  |0.0389|
|  - logical_fallacies                                     |      1|none             |     0|acc        |â†‘  | 0.8221|Â±  |0.0300|
|  - moral_disputes                                        |      1|none             |     0|acc        |â†‘  | 0.7659|Â±  |0.0228|
|  - moral_scenarios                                       |      1|none             |     0|acc        |â†‘  | 0.4190|Â±  |0.0165|
|  - philosophy                                            |      1|none             |     0|acc        |â†‘  | 0.7363|Â±  |0.0250|
|  - prehistory                                            |      1|none             |     0|acc        |â†‘  | 0.8241|Â±  |0.0212|
|  - professional_law                                      |      1|none             |     0|acc        |â†‘  | 0.5156|Â±  |0.0128|
|  - world_religions                                       |      1|none             |     0|acc        |â†‘  | 0.8246|Â±  |0.0292|
| - other                                                  |      2|none             |      |acc        |â†‘  | 0.7650|Â±  |0.0073|
|  - business_ethics                                       |      1|none             |     0|acc        |â†‘  | 0.7800|Â±  |0.0416|
|  - clinical_knowledge                                    |      1|none             |     0|acc        |â†‘  | 0.7774|Â±  |0.0256|
|  - college_medicine                                      |      1|none             |     0|acc        |â†‘  | 0.6705|Â±  |0.0358|
|  - global_facts                                          |      1|none             |     0|acc        |â†‘  | 0.4600|Â±  |0.0501|
|  - human_aging                                           |      1|none             |     0|acc        |â†‘  | 0.7803|Â±  |0.0278|
|  - management                                            |      1|none             |     0|acc        |â†‘  | 0.8835|Â±  |0.0318|
|  - marketing                                             |      1|none             |     0|acc        |â†‘  | 0.9231|Â±  |0.0175|
|  - medical_genetics                                      |      1|none             |     0|acc        |â†‘  | 0.8400|Â±  |0.0368|
|  - miscellaneous                                         |      1|none             |     0|acc        |â†‘  | 0.8531|Â±  |0.0127|
|  - nutrition                                             |      1|none             |     0|acc        |â†‘  | 0.7876|Â±  |0.0234|
|  - professional_accounting                               |      1|none             |     0|acc        |â†‘  | 0.5567|Â±  |0.0296|
|  - professional_medicine                                 |      1|none             |     0|acc        |â†‘  | 0.7831|Â±  |0.0250|
|  - virology                                              |      1|none             |     0|acc        |â†‘  | 0.5241|Â±  |0.0389|
| - social sciences                                        |      2|none             |      |acc        |â†‘  | 0.8274|Â±  |0.0067|
|  - econometrics                                          |      1|none             |     0|acc        |â†‘  | 0.6754|Â±  |0.0440|
|  - high_school_geography                                 |      1|none             |     0|acc        |â†‘  | 0.8788|Â±  |0.0233|
|  - high_school_government_and_politics                   |      1|none             |     0|acc        |â†‘  | 0.9430|Â±  |0.0167|
|  - high_school_macroeconomics                            |      1|none             |     0|acc        |â†‘  | 0.7897|Â±  |0.0207|
|  - high_school_microeconomics                            |      1|none             |     0|acc        |â†‘  | 0.8655|Â±  |0.0222|
|  - high_school_psychology                                |      1|none             |     0|acc        |â†‘  | 0.9064|Â±  |0.0125|
|  - human_sexuality                                       |      1|none             |     0|acc        |â†‘  | 0.8092|Â±  |0.0345|
|  - professional_psychology                               |      1|none             |     0|acc        |â†‘  | 0.7598|Â±  |0.0173|
|  - public_relations                                      |      1|none             |     0|acc        |â†‘  | 0.7182|Â±  |0.0431|
|  - security_studies                                      |      1|none             |     0|acc        |â†‘  | 0.7714|Â±  |0.0269|
|  - sociology                                             |      1|none             |     0|acc        |â†‘  | 0.8955|Â±  |0.0216|
|  - us_foreign_policy                                     |      1|none             |     0|acc        |â†‘  | 0.8600|Â±  |0.0349|
| - stem                                                   |      2|none             |      |acc        |â†‘  | 0.6863|Â±  |0.0080|
|  - abstract_algebra                                      |      1|none             |     0|acc        |â†‘  | 0.5800|Â±  |0.0496|
|  - anatomy                                               |      1|none             |     0|acc        |â†‘  | 0.7333|Â±  |0.0382|
|  - astronomy                                             |      1|none             |     0|acc        |â†‘  | 0.8553|Â±  |0.0286|
|  - college_biology                                       |      1|none             |     0|acc        |â†‘  | 0.8681|Â±  |0.0283|
|  - college_chemistry                                     |      1|none             |     0|acc        |â†‘  | 0.5300|Â±  |0.0502|
|  - college_computer_science                              |      1|none             |     0|acc        |â†‘  | 0.6700|Â±  |0.0473|
|  - college_mathematics                                   |      1|none             |     0|acc        |â†‘  | 0.4300|Â±  |0.0498|
|  - college_physics                                       |      1|none             |     0|acc        |â†‘  | 0.5098|Â±  |0.0497|
|  - computer_security                                     |      1|none             |     0|acc        |â†‘  | 0.7800|Â±  |0.0416|
|  - conceptual_physics                                    |      1|none             |     0|acc        |â†‘  | 0.7702|Â±  |0.0275|
|  - electrical_engineering                                |      1|none             |     0|acc        |â†‘  | 0.7103|Â±  |0.0378|
|  - elementary_mathematics                                |      1|none             |     0|acc        |â†‘  | 0.6720|Â±  |0.0242|
|  - high_school_biology                                   |      1|none             |     0|acc        |â†‘  | 0.8677|Â±  |0.0193|
|  - high_school_chemistry                                 |      1|none             |     0|acc        |â†‘  | 0.6404|Â±  |0.0338|
|  - high_school_computer_science                          |      1|none             |     0|acc        |â†‘  | 0.8100|Â±  |0.0394|
|  - high_school_mathematics                               |      1|none             |     0|acc        |â†‘  | 0.5481|Â±  |0.0303|
|  - high_school_physics                                   |      1|none             |     0|acc        |â†‘  | 0.5762|Â±  |0.0403|
|  - high_school_statistics                                |      1|none             |     0|acc        |â†‘  | 0.6806|Â±  |0.0318|
|  - machine_learning                                      |      1|none             |     0|acc        |â†‘  | 0.5268|Â±  |0.0474|
|nq_open                                                   |      4|remove_whitespace|     0|exact_match|â†‘  | 0.0457|Â±  |0.0035|
|openbookqa                                                |      1|none             |     0|acc        |â†‘  | 0.3440|Â±  |0.0213|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.4860|Â±  |0.0224|
|piqa                                                      |      1|none             |     0|acc        |â†‘  | 0.7943|Â±  |0.0094|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.8009|Â±  |0.0093|
|qnli                                                      |      1|none             |     0|acc        |â†‘  | 0.8047|Â±  |0.0054|
|sciq                                                      |      1|none             |     0|acc        |â†‘  | 0.9560|Â±  |0.0065|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.9360|Â±  |0.0077|
|triviaqa                                                  |      3|remove_whitespace|     0|exact_match|â†‘  | 0.3251|Â±  |0.0035|
|truthfulqa_gen                                            |      3|none             |     0|bleu_acc   |â†‘  | 0.5129|Â±  |0.0175|
|                                                          |       |none             |     0|bleu_diff  |â†‘  | 0.3578|Â±  |0.2345|
|                                                          |       |none             |     0|bleu_max   |â†‘  | 8.0558|Â±  |0.4296|
|                                                          |       |none             |     0|rouge1_acc |â†‘  | 0.5373|Â±  |0.0175|
|                                                          |       |none             |     0|rouge1_diff|â†‘  | 1.0510|Â±  |0.3423|
|                                                          |       |none             |     0|rouge1_max |â†‘  |25.7407|Â±  |0.6485|
|                                                          |       |none             |     0|rouge2_acc |â†‘  | 0.4455|Â±  |0.0174|
|                                                          |       |none             |     0|rouge2_diff|â†‘  | 0.4063|Â±  |0.3684|
|                                                          |       |none             |     0|rouge2_max |â†‘  |15.2839|Â±  |0.6032|
|                                                          |       |none             |     0|rougeL_acc |â†‘  | 0.4884|Â±  |0.0175|
|                                                          |       |none             |     0|rougeL_diff|â†‘  | 0.4719|Â±  |0.3267|
|                                                          |       |none             |     0|rougeL_max |â†‘  |22.5918|Â±  |0.6236|
|truthfulqa_mc1                                            |      2|none             |     0|acc        |â†‘  | 0.4774|Â±  |0.0175|
|truthfulqa_mc2                                            |      3|none             |     0|acc        |â†‘  | 0.6485|Â±  |0.0155|
|winogrande                                                |      1|none             |     0|acc        |â†‘  | 0.7080|Â±  |0.0128|
Passed argument batch_size = auto. Detecting largest batch size
Determined Largest batch size: 1
hf (pretrained=/mnt/data8tb/Documents/llm/llm_models/Qwen_Qwen2.5-7B-Instruct), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto:5 (1,64,64,64,64,64)
|                          Tasks                           |Version|     Filter      |n-shot|  Metric   |   | Value |   |Stderr|
|----------------------------------------------------------|------:|-----------------|-----:|-----------|---|------:|---|-----:|
|anli_r1                                                   |      1|none             |     0|acc        |â†‘  | 0.6840|Â±  |0.0147|
|anli_r2                                                   |      1|none             |     0|acc        |â†‘  | 0.5440|Â±  |0.0158|
|anli_r3                                                   |      1|none             |     0|acc        |â†‘  | 0.5492|Â±  |0.0144|
|arc_challenge                                             |      1|none             |     0|acc        |â†‘  | 0.5265|Â±  |0.0146|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.5529|Â±  |0.0145|
|bbh                                                       |      3|get-answer       |      |exact_match|â†‘  | 0.4534|Â±  |0.0051|
| - bbh_cot_fewshot_boolean_expressions                    |      4|get-answer       |     3|exact_match|â†‘  | 0.8120|Â±  |0.0248|
| - bbh_cot_fewshot_causal_judgement                       |      4|get-answer       |     3|exact_match|â†‘  | 0.3529|Â±  |0.0350|
| - bbh_cot_fewshot_date_understanding                     |      4|get-answer       |     3|exact_match|â†‘  | 0.4560|Â±  |0.0316|
| - bbh_cot_fewshot_disambiguation_qa                      |      4|get-answer       |     3|exact_match|â†‘  | 0.6160|Â±  |0.0308|
| - bbh_cot_fewshot_dyck_languages                         |      4|get-answer       |     3|exact_match|â†‘  | 0.0080|Â±  |0.0056|
| - bbh_cot_fewshot_formal_fallacies                       |      4|get-answer       |     3|exact_match|â†‘  | 0.2280|Â±  |0.0266|
| - bbh_cot_fewshot_geometric_shapes                       |      4|get-answer       |     3|exact_match|â†‘  | 0.1040|Â±  |0.0193|
| - bbh_cot_fewshot_hyperbaton                             |      4|get-answer       |     3|exact_match|â†‘  | 0.3920|Â±  |0.0309|
| - bbh_cot_fewshot_logical_deduction_five_objects         |      4|get-answer       |     3|exact_match|â†‘  | 0.3880|Â±  |0.0309|
| - bbh_cot_fewshot_logical_deduction_seven_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.1800|Â±  |0.0243|
| - bbh_cot_fewshot_logical_deduction_three_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.8480|Â±  |0.0228|
| - bbh_cot_fewshot_movie_recommendation                   |      4|get-answer       |     3|exact_match|â†‘  | 0.0360|Â±  |0.0118|
| - bbh_cot_fewshot_multistep_arithmetic_two               |      4|get-answer       |     3|exact_match|â†‘  | 0.3040|Â±  |0.0292|
| - bbh_cot_fewshot_navigate                               |      4|get-answer       |     3|exact_match|â†‘  | 0.8800|Â±  |0.0206|
| - bbh_cot_fewshot_object_counting                        |      4|get-answer       |     3|exact_match|â†‘  | 0.5760|Â±  |0.0313|
| - bbh_cot_fewshot_penguins_in_a_table                    |      4|get-answer       |     3|exact_match|â†‘  | 0.2055|Â±  |0.0336|
| - bbh_cot_fewshot_reasoning_about_colored_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.5560|Â±  |0.0315|
| - bbh_cot_fewshot_ruin_names                             |      4|get-answer       |     3|exact_match|â†‘  | 0.0360|Â±  |0.0118|
| - bbh_cot_fewshot_salient_translation_error_detection    |      4|get-answer       |     3|exact_match|â†‘  | 0.2680|Â±  |0.0281|
| - bbh_cot_fewshot_snarks                                 |      4|get-answer       |     3|exact_match|â†‘  | 0.5281|Â±  |0.0375|
| - bbh_cot_fewshot_sports_understanding                   |      4|get-answer       |     3|exact_match|â†‘  | 0.7760|Â±  |0.0264|
| - bbh_cot_fewshot_temporal_sequences                     |      4|get-answer       |     3|exact_match|â†‘  | 0.7640|Â±  |0.0269|
| - bbh_cot_fewshot_tracking_shuffled_objects_five_objects |      4|get-answer       |     3|exact_match|â†‘  | 0.7600|Â±  |0.0271|
| - bbh_cot_fewshot_tracking_shuffled_objects_seven_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.5840|Â±  |0.0312|
| - bbh_cot_fewshot_tracking_shuffled_objects_three_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.8920|Â±  |0.0197|
| - bbh_cot_fewshot_web_of_lies                            |      4|get-answer       |     3|exact_match|â†‘  | 0.3200|Â±  |0.0296|
| - bbh_cot_fewshot_word_sorting                           |      4|get-answer       |     3|exact_match|â†‘  | 0.2640|Â±  |0.0279|
|boolq                                                     |      2|none             |     0|acc        |â†‘  | 0.8633|Â±  |0.0060|
|drop                                                      |      3|none             |     0|em         |â†‘  | 0.0028|Â±  |0.0005|
|                                                          |       |none             |     0|f1         |â†‘  | 0.0713|Â±  |0.0014|
|gpqa_diamond_cot_n_shot                                   |      2|flexible-extract |     0|exact_match|â†‘  | 0.1414|Â±  |0.0248|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_cot_zeroshot                                 |      1|flexible-extract |     0|exact_match|â†‘  | 0.0859|Â±  |0.0200|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_generative_n_shot                            |      2|flexible-extract |     0|exact_match|â†‘  | 0.2879|Â±  |0.0323|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_n_shot                                       |      2|none             |     0|acc        |â†‘  | 0.3283|Â±  |0.0335|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3283|Â±  |0.0335|
|gpqa_diamond_zeroshot                                     |      1|none             |     0|acc        |â†‘  | 0.3131|Â±  |0.0330|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3131|Â±  |0.0330|
|gpqa_extended_cot_n_shot                                  |      2|flexible-extract |     0|exact_match|â†‘  | 0.1612|Â±  |0.0158|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_cot_zeroshot                                |      1|flexible-extract |     0|exact_match|â†‘  | 0.0989|Â±  |0.0128|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_generative_n_shot                           |      2|flexible-extract |     0|exact_match|â†‘  | 0.2546|Â±  |0.0187|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_n_shot                                      |      2|none             |     0|acc        |â†‘  | 0.3242|Â±  |0.0200|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3242|Â±  |0.0200|
|gpqa_extended_zeroshot                                    |      1|none             |     0|acc        |â†‘  | 0.3388|Â±  |0.0203|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3388|Â±  |0.0203|
|gpqa_main_cot_n_shot                                      |      2|flexible-extract |     0|exact_match|â†‘  | 0.1540|Â±  |0.0171|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_cot_zeroshot                                    |      1|flexible-extract |     0|exact_match|â†‘  | 0.1094|Â±  |0.0148|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_generative_n_shot                               |      2|flexible-extract |     0|exact_match|â†‘  | 0.2746|Â±  |0.0211|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_n_shot                                          |      2|none             |     0|acc        |â†‘  | 0.3237|Â±  |0.0221|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3237|Â±  |0.0221|
|gpqa_main_zeroshot                                        |      1|none             |     0|acc        |â†‘  | 0.3326|Â±  |0.0223|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3326|Â±  |0.0223|
|gsm8k                                                     |      3|flexible-extract |     5|exact_match|â†‘  | 0.8256|Â±  |0.0105|
|                                                          |       |strict-match     |     5|exact_match|â†‘  | 0.7582|Â±  |0.0118|
|hellaswag                                                 |      1|none             |     0|acc        |â†‘  | 0.6198|Â±  |0.0048|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.8039|Â±  |0.0040|
|mmlu                                                      |      2|none             |      |acc        |â†‘  | 0.7175|Â±  |0.0036|
| - humanities                                             |      2|none             |      |acc        |â†‘  | 0.6351|Â±  |0.0066|
|  - formal_logic                                          |      1|none             |     0|acc        |â†‘  | 0.5635|Â±  |0.0444|
|  - high_school_european_history                          |      1|none             |     0|acc        |â†‘  | 0.8485|Â±  |0.0280|
|  - high_school_us_history                                |      1|none             |     0|acc        |â†‘  | 0.9020|Â±  |0.0209|
|  - high_school_world_history                             |      1|none             |     0|acc        |â†‘  | 0.8734|Â±  |0.0216|
|  - international_law                                     |      1|none             |     0|acc        |â†‘  | 0.8099|Â±  |0.0358|
|  - jurisprudence                                         |      1|none             |     0|acc        |â†‘  | 0.7963|Â±  |0.0389|
|  - logical_fallacies                                     |      1|none             |     0|acc        |â†‘  | 0.8221|Â±  |0.0300|
|  - moral_disputes                                        |      1|none             |     0|acc        |â†‘  | 0.7659|Â±  |0.0228|
|  - moral_scenarios                                       |      1|none             |     0|acc        |â†‘  | 0.4190|Â±  |0.0165|
|  - philosophy                                            |      1|none             |     0|acc        |â†‘  | 0.7363|Â±  |0.0250|
|  - prehistory                                            |      1|none             |     0|acc        |â†‘  | 0.8241|Â±  |0.0212|
|  - professional_law                                      |      1|none             |     0|acc        |â†‘  | 0.5156|Â±  |0.0128|
|  - world_religions                                       |      1|none             |     0|acc        |â†‘  | 0.8246|Â±  |0.0292|
| - other                                                  |      2|none             |      |acc        |â†‘  | 0.7650|Â±  |0.0073|
|  - business_ethics                                       |      1|none             |     0|acc        |â†‘  | 0.7800|Â±  |0.0416|
|  - clinical_knowledge                                    |      1|none             |     0|acc        |â†‘  | 0.7774|Â±  |0.0256|
|  - college_medicine                                      |      1|none             |     0|acc        |â†‘  | 0.6705|Â±  |0.0358|
|  - global_facts                                          |      1|none             |     0|acc        |â†‘  | 0.4600|Â±  |0.0501|
|  - human_aging                                           |      1|none             |     0|acc        |â†‘  | 0.7803|Â±  |0.0278|
|  - management                                            |      1|none             |     0|acc        |â†‘  | 0.8835|Â±  |0.0318|
|  - marketing                                             |      1|none             |     0|acc        |â†‘  | 0.9231|Â±  |0.0175|
|  - medical_genetics                                      |      1|none             |     0|acc        |â†‘  | 0.8400|Â±  |0.0368|
|  - miscellaneous                                         |      1|none             |     0|acc        |â†‘  | 0.8531|Â±  |0.0127|
|  - nutrition                                             |      1|none             |     0|acc        |â†‘  | 0.7876|Â±  |0.0234|
|  - professional_accounting                               |      1|none             |     0|acc        |â†‘  | 0.5567|Â±  |0.0296|
|  - professional_medicine                                 |      1|none             |     0|acc        |â†‘  | 0.7831|Â±  |0.0250|
|  - virology                                              |      1|none             |     0|acc        |â†‘  | 0.5241|Â±  |0.0389|
| - social sciences                                        |      2|none             |      |acc        |â†‘  | 0.8274|Â±  |0.0067|
|  - econometrics                                          |      1|none             |     0|acc        |â†‘  | 0.6754|Â±  |0.0440|
|  - high_school_geography                                 |      1|none             |     0|acc        |â†‘  | 0.8788|Â±  |0.0233|
|  - high_school_government_and_politics                   |      1|none             |     0|acc        |â†‘  | 0.9430|Â±  |0.0167|
|  - high_school_macroeconomics                            |      1|none             |     0|acc        |â†‘  | 0.7897|Â±  |0.0207|
|  - high_school_microeconomics                            |      1|none             |     0|acc        |â†‘  | 0.8655|Â±  |0.0222|
|  - high_school_psychology                                |      1|none             |     0|acc        |â†‘  | 0.9064|Â±  |0.0125|
|  - human_sexuality                                       |      1|none             |     0|acc        |â†‘  | 0.8092|Â±  |0.0345|
|  - professional_psychology                               |      1|none             |     0|acc        |â†‘  | 0.7598|Â±  |0.0173|
|  - public_relations                                      |      1|none             |     0|acc        |â†‘  | 0.7182|Â±  |0.0431|
|  - security_studies                                      |      1|none             |     0|acc        |â†‘  | 0.7714|Â±  |0.0269|
|  - sociology                                             |      1|none             |     0|acc        |â†‘  | 0.8955|Â±  |0.0216|
|  - us_foreign_policy                                     |      1|none             |     0|acc        |â†‘  | 0.8600|Â±  |0.0349|
| - stem                                                   |      2|none             |      |acc        |â†‘  | 0.6863|Â±  |0.0080|
|  - abstract_algebra                                      |      1|none             |     0|acc        |â†‘  | 0.5800|Â±  |0.0496|
|  - anatomy                                               |      1|none             |     0|acc        |â†‘  | 0.7333|Â±  |0.0382|
|  - astronomy                                             |      1|none             |     0|acc        |â†‘  | 0.8553|Â±  |0.0286|
|  - college_biology                                       |      1|none             |     0|acc        |â†‘  | 0.8681|Â±  |0.0283|
|  - college_chemistry                                     |      1|none             |     0|acc        |â†‘  | 0.5300|Â±  |0.0502|
|  - college_computer_science                              |      1|none             |     0|acc        |â†‘  | 0.6700|Â±  |0.0473|
|  - college_mathematics                                   |      1|none             |     0|acc        |â†‘  | 0.4300|Â±  |0.0498|
|  - college_physics                                       |      1|none             |     0|acc        |â†‘  | 0.5098|Â±  |0.0497|
|  - computer_security                                     |      1|none             |     0|acc        |â†‘  | 0.7800|Â±  |0.0416|
|  - conceptual_physics                                    |      1|none             |     0|acc        |â†‘  | 0.7702|Â±  |0.0275|
|  - electrical_engineering                                |      1|none             |     0|acc        |â†‘  | 0.7103|Â±  |0.0378|
|  - elementary_mathematics                                |      1|none             |     0|acc        |â†‘  | 0.6720|Â±  |0.0242|
|  - high_school_biology                                   |      1|none             |     0|acc        |â†‘  | 0.8677|Â±  |0.0193|
|  - high_school_chemistry                                 |      1|none             |     0|acc        |â†‘  | 0.6404|Â±  |0.0338|
|  - high_school_computer_science                          |      1|none             |     0|acc        |â†‘  | 0.8100|Â±  |0.0394|
|  - high_school_mathematics                               |      1|none             |     0|acc        |â†‘  | 0.5481|Â±  |0.0303|
|  - high_school_physics                                   |      1|none             |     0|acc        |â†‘  | 0.5762|Â±  |0.0403|
|  - high_school_statistics                                |      1|none             |     0|acc        |â†‘  | 0.6806|Â±  |0.0318|
|  - machine_learning                                      |      1|none             |     0|acc        |â†‘  | 0.5268|Â±  |0.0474|
|nq_open                                                   |      4|remove_whitespace|     0|exact_match|â†‘  | 0.0457|Â±  |0.0035|
|openbookqa                                                |      1|none             |     0|acc        |â†‘  | 0.3440|Â±  |0.0213|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.4860|Â±  |0.0224|
|piqa                                                      |      1|none             |     0|acc        |â†‘  | 0.7943|Â±  |0.0094|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.8009|Â±  |0.0093|
|qnli                                                      |      1|none             |     0|acc        |â†‘  | 0.8047|Â±  |0.0054|
|sciq                                                      |      1|none             |     0|acc        |â†‘  | 0.9560|Â±  |0.0065|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.9360|Â±  |0.0077|
|triviaqa                                                  |      3|remove_whitespace|     0|exact_match|â†‘  | 0.3251|Â±  |0.0035|
|truthfulqa_gen                                            |      3|none             |     0|bleu_acc   |â†‘  | 0.5129|Â±  |0.0175|
|                                                          |       |none             |     0|bleu_diff  |â†‘  | 0.3578|Â±  |0.2345|
|                                                          |       |none             |     0|bleu_max   |â†‘  | 8.0558|Â±  |0.4296|
|                                                          |       |none             |     0|rouge1_acc |â†‘  | 0.5373|Â±  |0.0175|
|                                                          |       |none             |     0|rouge1_diff|â†‘  | 1.0510|Â±  |0.3423|
|                                                          |       |none             |     0|rouge1_max |â†‘  |25.7407|Â±  |0.6485|
|                                                          |       |none             |     0|rouge2_acc |â†‘  | 0.4455|Â±  |0.0174|
|                                                          |       |none             |     0|rouge2_diff|â†‘  | 0.4063|Â±  |0.3684|
|                                                          |       |none             |     0|rouge2_max |â†‘  |15.2839|Â±  |0.6032|
|                                                          |       |none             |     0|rougeL_acc |â†‘  | 0.4884|Â±  |0.0175|
|                                                          |       |none             |     0|rougeL_diff|â†‘  | 0.4719|Â±  |0.3267|
|                                                          |       |none             |     0|rougeL_max |â†‘  |22.5918|Â±  |0.6236|
|truthfulqa_mc1                                            |      2|none             |     0|acc        |â†‘  | 0.4774|Â±  |0.0175|
|truthfulqa_mc2                                            |      3|none             |     0|acc        |â†‘  | 0.6485|Â±  |0.0155|
|winogrande                                                |      1|none             |     0|acc        |â†‘  | 0.7080|Â±  |0.0128|

|      Groups      |Version|  Filter  |n-shot|  Metric   |   |Value |   |Stderr|
|------------------|------:|----------|------|-----------|---|-----:|---|-----:|
|bbh               |      3|get-answer|      |exact_match|â†‘  |0.4534|Â±  |0.0051|
|mmlu              |      2|none      |      |acc        |â†‘  |0.7175|Â±  |0.0036|
| - humanities     |      2|none      |      |acc        |â†‘  |0.6351|Â±  |0.0066|
| - other          |      2|none      |      |acc        |â†‘  |0.7650|Â±  |0.0073|
| - social sciences|      2|none      |      |acc        |â†‘  |0.8274|Â±  |0.0067|
| - stem           |      2|none      |      |acc        |â†‘  |0.6863|Â±  |0.0080|


|      Groups      |Version|  Filter  |n-shot|  Metric   |   |Value |   |Stderr|
|------------------|------:|----------|------|-----------|---|-----:|---|-----:|
|bbh               |      3|get-answer|      |exact_match|â†‘  |0.4534|Â±  |0.0051|
|mmlu              |      2|none      |      |acc        |â†‘  |0.7175|Â±  |0.0036|
| - humanities     |      2|none      |      |acc        |â†‘  |0.6351|Â±  |0.0066|
| - other          |      2|none      |      |acc        |â†‘  |0.7650|Â±  |0.0073|
| - social sciences|      2|none      |      |acc        |â†‘  |0.8274|Â±  |0.0067|
| - stem           |      2|none      |      |acc        |â†‘  |0.6863|Â±  |0.0080|

Qwen_Qwen2.5-7B-Instruct: 11h 6m 29s
âœ… Benchmark completed for Qwen_Qwen2.5-7B-Instruct

ðŸ”¥ Starting benchmark for deepseek-ai_DeepSeek-R1-Distill-Llama-8B

Passed argument batch_size = auto:5.0. Detecting largest batch size
Determined largest batch size: 1
Passed argument batch_size = auto:5.0. Detecting largest batch size
Determined largest batch size: 64

hf (pretrained=/mnt/data8tb/Documents/llm/llm_models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto:5 (1,64,64,64,64,64)
|                          Tasks                           |Version|     Filter      |n-shot|  Metric   |   | Value |   |Stderr|
|----------------------------------------------------------|------:|-----------------|-----:|-----------|---|------:|---|-----:|
|anli_r1                                                   |      1|none             |     0|acc        |â†‘  | 0.4040|Â±  |0.0155|
|anli_r2                                                   |      1|none             |     0|acc        |â†‘  | 0.4100|Â±  |0.0156|
|anli_r3                                                   |      1|none             |     0|acc        |â†‘  | 0.3883|Â±  |0.0141|
|arc_challenge                                             |      1|none             |     0|acc        |â†‘  | 0.4061|Â±  |0.0144|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.4232|Â±  |0.0144|
|bbh                                                       |      3|get-answer       |      |exact_match|â†‘  | 0.6037|Â±  |0.0050|
| - bbh_cot_fewshot_boolean_expressions                    |      4|get-answer       |     3|exact_match|â†‘  | 0.9400|Â±  |0.0151|
| - bbh_cot_fewshot_causal_judgement                       |      4|get-answer       |     3|exact_match|â†‘  | 0.5508|Â±  |0.0365|
| - bbh_cot_fewshot_date_understanding                     |      4|get-answer       |     3|exact_match|â†‘  | 0.7440|Â±  |0.0277|
| - bbh_cot_fewshot_disambiguation_qa                      |      4|get-answer       |     3|exact_match|â†‘  | 0.6120|Â±  |0.0309|
| - bbh_cot_fewshot_dyck_languages                         |      4|get-answer       |     3|exact_match|â†‘  | 0.2360|Â±  |0.0269|
| - bbh_cot_fewshot_formal_fallacies                       |      4|get-answer       |     3|exact_match|â†‘  | 0.2520|Â±  |0.0275|
| - bbh_cot_fewshot_geometric_shapes                       |      4|get-answer       |     3|exact_match|â†‘  | 0.3200|Â±  |0.0296|
| - bbh_cot_fewshot_hyperbaton                             |      4|get-answer       |     3|exact_match|â†‘  | 0.7920|Â±  |0.0257|
| - bbh_cot_fewshot_logical_deduction_five_objects         |      4|get-answer       |     3|exact_match|â†‘  | 0.2760|Â±  |0.0283|
| - bbh_cot_fewshot_logical_deduction_seven_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.0600|Â±  |0.0151|
| - bbh_cot_fewshot_logical_deduction_three_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.8520|Â±  |0.0225|
| - bbh_cot_fewshot_movie_recommendation                   |      4|get-answer       |     3|exact_match|â†‘  | 0.6840|Â±  |0.0295|
| - bbh_cot_fewshot_multistep_arithmetic_two               |      4|get-answer       |     3|exact_match|â†‘  | 0.8840|Â±  |0.0203|
| - bbh_cot_fewshot_navigate                               |      4|get-answer       |     3|exact_match|â†‘  | 0.8560|Â±  |0.0222|
| - bbh_cot_fewshot_object_counting                        |      4|get-answer       |     3|exact_match|â†‘  | 0.8320|Â±  |0.0237|
| - bbh_cot_fewshot_penguins_in_a_table                    |      4|get-answer       |     3|exact_match|â†‘  | 0.3973|Â±  |0.0406|
| - bbh_cot_fewshot_reasoning_about_colored_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.6360|Â±  |0.0305|
| - bbh_cot_fewshot_ruin_names                             |      4|get-answer       |     3|exact_match|â†‘  | 0.6200|Â±  |0.0308|
| - bbh_cot_fewshot_salient_translation_error_detection    |      4|get-answer       |     3|exact_match|â†‘  | 0.5680|Â±  |0.0314|
| - bbh_cot_fewshot_snarks                                 |      4|get-answer       |     3|exact_match|â†‘  | 0.5506|Â±  |0.0374|
| - bbh_cot_fewshot_sports_understanding                   |      4|get-answer       |     3|exact_match|â†‘  | 0.8200|Â±  |0.0243|
| - bbh_cot_fewshot_temporal_sequences                     |      4|get-answer       |     3|exact_match|â†‘  | 0.8640|Â±  |0.0217|
| - bbh_cot_fewshot_tracking_shuffled_objects_five_objects |      4|get-answer       |     3|exact_match|â†‘  | 0.3920|Â±  |0.0309|
| - bbh_cot_fewshot_tracking_shuffled_objects_seven_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.1040|Â±  |0.0193|
| - bbh_cot_fewshot_tracking_shuffled_objects_three_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.9480|Â±  |0.0141|
| - bbh_cot_fewshot_web_of_lies                            |      4|get-answer       |     3|exact_match|â†‘  | 1.0000|Â±  |0.0000|
| - bbh_cot_fewshot_word_sorting                           |      4|get-answer       |     3|exact_match|â†‘  | 0.3960|Â±  |0.0310|
|boolq                                                     |      2|none             |     0|acc        |â†‘  | 0.8287|Â±  |0.0066|
|drop                                                      |      3|none             |     0|em         |â†‘  | 0.0031|Â±  |0.0006|
|                                                          |       |none             |     0|f1         |â†‘  | 0.0712|Â±  |0.0014|
|gpqa_diamond_cot_n_shot                                   |      2|flexible-extract |     0|exact_match|â†‘  | 0.0909|Â±  |0.0205|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_cot_zeroshot                                 |      1|flexible-extract |     0|exact_match|â†‘  | 0.0707|Â±  |0.0183|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_generative_n_shot                            |      2|flexible-extract |     0|exact_match|â†‘  | 0.2626|Â±  |0.0314|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_n_shot                                       |      2|none             |     0|acc        |â†‘  | 0.3283|Â±  |0.0335|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3283|Â±  |0.0335|
|gpqa_diamond_zeroshot                                     |      1|none             |     0|acc        |â†‘  | 0.2727|Â±  |0.0317|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2727|Â±  |0.0317|
|gpqa_extended_cot_n_shot                                  |      2|flexible-extract |     0|exact_match|â†‘  | 0.1172|Â±  |0.0138|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_cot_zeroshot                                |      1|flexible-extract |     0|exact_match|â†‘  | 0.0952|Â±  |0.0126|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_generative_n_shot                           |      2|flexible-extract |     0|exact_match|â†‘  | 0.3059|Â±  |0.0197|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_n_shot                                      |      2|none             |     0|acc        |â†‘  | 0.2912|Â±  |0.0195|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2912|Â±  |0.0195|
|gpqa_extended_zeroshot                                    |      1|none             |     0|acc        |â†‘  | 0.2967|Â±  |0.0196|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2967|Â±  |0.0196|
|gpqa_main_cot_n_shot                                      |      2|flexible-extract |     0|exact_match|â†‘  | 0.0893|Â±  |0.0135|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_cot_zeroshot                                    |      1|flexible-extract |     0|exact_match|â†‘  | 0.1138|Â±  |0.0150|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_generative_n_shot                               |      2|flexible-extract |     0|exact_match|â†‘  | 0.2746|Â±  |0.0211|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_n_shot                                          |      2|none             |     0|acc        |â†‘  | 0.3125|Â±  |0.0219|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3125|Â±  |0.0219|
|gpqa_main_zeroshot                                        |      1|none             |     0|acc        |â†‘  | 0.2746|Â±  |0.0211|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2746|Â±  |0.0211|
|gsm8k                                                     |      3|flexible-extract |     5|exact_match|â†‘  | 0.6452|Â±  |0.0132|
|                                                          |       |strict-match     |     5|exact_match|â†‘  | 0.6247|Â±  |0.0133|
|hellaswag                                                 |      1|none             |     0|acc        |â†‘  | 0.5562|Â±  |0.0050|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.7430|Â±  |0.0044|
|mmlu                                                      |      2|none             |      |acc        |â†‘  | 0.5327|Â±  |0.0040|
| - humanities                                             |      2|none             |      |acc        |â†‘  | 0.4767|Â±  |0.0069|
|  - formal_logic                                          |      1|none             |     0|acc        |â†‘  | 0.4048|Â±  |0.0439|
|  - high_school_european_history                          |      1|none             |     0|acc        |â†‘  | 0.7030|Â±  |0.0357|
|  - high_school_us_history                                |      1|none             |     0|acc        |â†‘  | 0.6765|Â±  |0.0328|
|  - high_school_world_history                             |      1|none             |     0|acc        |â†‘  | 0.7384|Â±  |0.0286|
|  - international_law                                     |      1|none             |     0|acc        |â†‘  | 0.7025|Â±  |0.0417|
|  - jurisprudence                                         |      1|none             |     0|acc        |â†‘  | 0.6019|Â±  |0.0473|
|  - logical_fallacies                                     |      1|none             |     0|acc        |â†‘  | 0.5706|Â±  |0.0389|
|  - moral_disputes                                        |      1|none             |     0|acc        |â†‘  | 0.5289|Â±  |0.0269|
|  - moral_scenarios                                       |      1|none             |     0|acc        |â†‘  | 0.2425|Â±  |0.0143|
|  - philosophy                                            |      1|none             |     0|acc        |â†‘  | 0.5466|Â±  |0.0283|
|  - prehistory                                            |      1|none             |     0|acc        |â†‘  | 0.6327|Â±  |0.0268|
|  - professional_law                                      |      1|none             |     0|acc        |â†‘  | 0.4068|Â±  |0.0125|
|  - world_religions                                       |      1|none             |     0|acc        |â†‘  | 0.7076|Â±  |0.0349|
| - other                                                  |      2|none             |      |acc        |â†‘  | 0.6041|Â±  |0.0085|
|  - business_ethics                                       |      1|none             |     0|acc        |â†‘  | 0.6000|Â±  |0.0492|
|  - clinical_knowledge                                    |      1|none             |     0|acc        |â†‘  | 0.5811|Â±  |0.0304|
|  - college_medicine                                      |      1|none             |     0|acc        |â†‘  | 0.5318|Â±  |0.0380|
|  - global_facts                                          |      1|none             |     0|acc        |â†‘  | 0.3800|Â±  |0.0488|
|  - human_aging                                           |      1|none             |     0|acc        |â†‘  | 0.5291|Â±  |0.0335|
|  - management                                            |      1|none             |     0|acc        |â†‘  | 0.7282|Â±  |0.0441|
|  - marketing                                             |      1|none             |     0|acc        |â†‘  | 0.7949|Â±  |0.0265|
|  - medical_genetics                                      |      1|none             |     0|acc        |â†‘  | 0.6300|Â±  |0.0485|
|  - miscellaneous                                         |      1|none             |     0|acc        |â†‘  | 0.7152|Â±  |0.0161|
|  - nutrition                                             |      1|none             |     0|acc        |â†‘  | 0.6144|Â±  |0.0279|
|  - professional_accounting                               |      1|none             |     0|acc        |â†‘  | 0.4007|Â±  |0.0292|
|  - professional_medicine                                 |      1|none             |     0|acc        |â†‘  | 0.5625|Â±  |0.0301|
|  - virology                                              |      1|none             |     0|acc        |â†‘  | 0.4639|Â±  |0.0388|
| - social sciences                                        |      2|none             |      |acc        |â†‘  | 0.6074|Â±  |0.0086|
|  - econometrics                                          |      1|none             |     0|acc        |â†‘  | 0.3158|Â±  |0.0437|
|  - high_school_geography                                 |      1|none             |     0|acc        |â†‘  | 0.6465|Â±  |0.0341|
|  - high_school_government_and_politics                   |      1|none             |     0|acc        |â†‘  | 0.6425|Â±  |0.0346|
|  - high_school_macroeconomics                            |      1|none             |     0|acc        |â†‘  | 0.5154|Â±  |0.0253|
|  - high_school_microeconomics                            |      1|none             |     0|acc        |â†‘  | 0.4874|Â±  |0.0325|
|  - high_school_psychology                                |      1|none             |     0|acc        |â†‘  | 0.7009|Â±  |0.0196|
|  - human_sexuality                                       |      1|none             |     0|acc        |â†‘  | 0.6336|Â±  |0.0423|
|  - professional_psychology                               |      1|none             |     0|acc        |â†‘  | 0.5539|Â±  |0.0201|
|  - public_relations                                      |      1|none             |     0|acc        |â†‘  | 0.6273|Â±  |0.0463|
|  - security_studies                                      |      1|none             |     0|acc        |â†‘  | 0.6367|Â±  |0.0308|
|  - sociology                                             |      1|none             |     0|acc        |â†‘  | 0.7363|Â±  |0.0312|
|  - us_foreign_policy                                     |      1|none             |     0|acc        |â†‘  | 0.8700|Â±  |0.0338|
| - stem                                                   |      2|none             |      |acc        |â†‘  | 0.4729|Â±  |0.0088|
|  - abstract_algebra                                      |      1|none             |     0|acc        |â†‘  | 0.3900|Â±  |0.0490|
|  - anatomy                                               |      1|none             |     0|acc        |â†‘  | 0.5111|Â±  |0.0432|
|  - astronomy                                             |      1|none             |     0|acc        |â†‘  | 0.5461|Â±  |0.0405|
|  - college_biology                                       |      1|none             |     0|acc        |â†‘  | 0.5556|Â±  |0.0416|
|  - college_chemistry                                     |      1|none             |     0|acc        |â†‘  | 0.3800|Â±  |0.0488|
|  - college_computer_science                              |      1|none             |     0|acc        |â†‘  | 0.4800|Â±  |0.0502|
|  - college_mathematics                                   |      1|none             |     0|acc        |â†‘  | 0.4100|Â±  |0.0494|
|  - college_physics                                       |      1|none             |     0|acc        |â†‘  | 0.3725|Â±  |0.0481|
|  - computer_security                                     |      1|none             |     0|acc        |â†‘  | 0.6000|Â±  |0.0492|
|  - conceptual_physics                                    |      1|none             |     0|acc        |â†‘  | 0.4681|Â±  |0.0326|
|  - electrical_engineering                                |      1|none             |     0|acc        |â†‘  | 0.5034|Â±  |0.0417|
|  - elementary_mathematics                                |      1|none             |     0|acc        |â†‘  | 0.4286|Â±  |0.0255|
|  - high_school_biology                                   |      1|none             |     0|acc        |â†‘  | 0.6452|Â±  |0.0272|
|  - high_school_chemistry                                 |      1|none             |     0|acc        |â†‘  | 0.4483|Â±  |0.0350|
|  - high_school_computer_science                          |      1|none             |     0|acc        |â†‘  | 0.5600|Â±  |0.0499|
|  - high_school_mathematics                               |      1|none             |     0|acc        |â†‘  | 0.3815|Â±  |0.0296|
|  - high_school_physics                                   |      1|none             |     0|acc        |â†‘  | 0.3907|Â±  |0.0398|
|  - high_school_statistics                                |      1|none             |     0|acc        |â†‘  | 0.4444|Â±  |0.0339|
|  - machine_learning                                      |      1|none             |     0|acc        |â†‘  | 0.4018|Â±  |0.0465|
|nq_open                                                   |      4|remove_whitespace|     0|exact_match|â†‘  | 0.0584|Â±  |0.0039|
|openbookqa                                                |      1|none             |     0|acc        |â†‘  | 0.3160|Â±  |0.0208|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.4100|Â±  |0.0220|
|piqa                                                      |      1|none             |     0|acc        |â†‘  | 0.7595|Â±  |0.0100|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.7758|Â±  |0.0097|
|qnli                                                      |      1|none             |     0|acc        |â†‘  | 0.5147|Â±  |0.0068|
|sciq                                                      |      1|none             |     0|acc        |â†‘  | 0.9290|Â±  |0.0081|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.8990|Â±  |0.0095|
|triviaqa                                                  |      3|remove_whitespace|     0|exact_match|â†‘  | 0.1940|Â±  |0.0030|
|truthfulqa_gen                                            |      3|none             |     0|bleu_acc   |â†‘  | 0.4455|Â±  |0.0174|
|                                                          |       |none             |     0|bleu_diff  |â†‘  |-0.9197|Â±  |0.5624|
|                                                          |       |none             |     0|bleu_max   |â†‘  |15.5776|Â±  |0.6556|
|                                                          |       |none             |     0|rouge1_acc |â†‘  | 0.4517|Â±  |0.0174|
|                                                          |       |none             |     0|rouge1_diff|â†‘  |-1.1087|Â±  |0.7899|
|                                                          |       |none             |     0|rouge1_max |â†‘  |37.4816|Â±  |0.7944|
|                                                          |       |none             |     0|rouge2_acc |â†‘  | 0.2521|Â±  |0.0152|
|                                                          |       |none             |     0|rouge2_diff|â†‘  |-4.0247|Â±  |0.7868|
|                                                          |       |none             |     0|rouge2_max |â†‘  |19.9717|Â±  |0.8482|
|                                                          |       |none             |     0|rougeL_acc |â†‘  | 0.4565|Â±  |0.0174|
|                                                          |       |none             |     0|rougeL_diff|â†‘  |-1.0779|Â±  |0.7931|
|                                                          |       |none             |     0|rougeL_max |â†‘  |34.9414|Â±  |0.7851|
|truthfulqa_mc1                                            |      2|none             |     0|acc        |â†‘  | 0.3219|Â±  |0.0164|
|truthfulqa_mc2                                            |      3|none             |     0|acc        |â†‘  | 0.5045|Â±  |0.0154|
|winogrande                                                |      1|none             |     0|acc        |â†‘  | 0.6780|Â±  |0.0131|
Passed argument batch_size = auto. Detecting largest batch size
Determined Largest batch size: 1
hf (pretrained=/mnt/data8tb/Documents/llm/llm_models/deepseek-ai_DeepSeek-R1-Distill-Llama-8B), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto:5 (1,64,64,64,64,64)

|      Groups      |Version|  Filter  |n-shot|  Metric   |   |Value |   |Stderr|
|------------------|------:|----------|------|-----------|---|-----:|---|-----:|
|bbh               |      3|get-answer|      |exact_match|â†‘  |0.6037|Â±  |0.0050|
|mmlu              |      2|none      |      |acc        |â†‘  |0.5327|Â±  |0.0040|
| - humanities     |      2|none      |      |acc        |â†‘  |0.4767|Â±  |0.0069|
| - other          |      2|none      |      |acc        |â†‘  |0.6041|Â±  |0.0085|
| - social sciences|      2|none      |      |acc        |â†‘  |0.6074|Â±  |0.0086|
| - stem           |      2|none      |      |acc        |â†‘  |0.4729|Â±  |0.0088|


deepseek-ai_DeepSeek-R1-Distill-Llama-8B: 11h 46m 55s
âœ… Benchmark completed for deepseek-ai_DeepSeek-R1-Distill-Llama-8B
ðŸ”¥ Starting benchmark for Qwen_Qwen2.5-7B-Instruct
ðŸ”¥ Starting benchmark for Qwen_Qwen2.5-7B-Instruct
ðŸ”¥ Starting benchmark for Qwen_Qwen-7B-Chat
Qwen_Qwen-7B-Chat: 0h 0m 3s
âœ… Benchmark completed for Qwen_Qwen-7B-Chat

ðŸ”¥ Starting benchmark for Qwen_Qwen-7B
ðŸ”¥ Starting benchmark for Qwen_Qwen-7B-Chat
Qwen_Qwen-7B-Chat: 0h 0m 4s
âœ… Benchmark completed for Qwen_Qwen-7B-Chat

ðŸ”¥ Starting benchmark for Qwen_Qwen-7B
ðŸ”¥ Starting benchmark for Qwen_Qwen-7B-Chat
ðŸ”¥ Starting benchmark for Qwen_Qwen-7B
ðŸ”¥ Starting benchmark for meta-llama_Meta-Llama-3-8B-Instruct
ðŸ”¥ Starting benchmark for mistralai_Mistral-7B-Instruct-v0.3
ðŸ”¥ Starting benchmark for Qwen_Qwen2.5-7B-Instruct
ðŸ”¥ Starting benchmark for Qwen_Qwen2.5-7B-Instruct
ðŸ”¥ Starting benchmark for Qwen_Qwen2.5-7B-Instruct
ðŸ”¥ Starting benchmark for Qwen_Qwen2.5-7B-Instruct
hf (pretrained=/mnt/data8tb/Documents/llm/llm_models/Qwen_Qwen2.5-7B-Instruct,trust_remote_code=True), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 3
|                          Tasks                           |Version|     Filter      |n-shot|  Metric   |   | Value |   |Stderr|
|----------------------------------------------------------|------:|-----------------|-----:|-----------|---|------:|---|-----:|
|anli_r1                                                   |      1|none             |     0|acc        |â†‘  | 0.6850|Â±  |0.0147|
|anli_r2                                                   |      1|none             |     0|acc        |â†‘  | 0.5490|Â±  |0.0157|
|anli_r3                                                   |      1|none             |     0|acc        |â†‘  | 0.5525|Â±  |0.0144|
|arc_challenge                                             |      1|none             |     0|acc        |â†‘  | 0.5239|Â±  |0.0146|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.5529|Â±  |0.0145|
|bbh                                                       |      3|get-answer       |      |exact_match|â†‘  | 0.4488|Â±  |0.0051|
| - bbh_cot_fewshot_boolean_expressions                    |      4|get-answer       |     3|exact_match|â†‘  | 0.8160|Â±  |0.0246|
| - bbh_cot_fewshot_causal_judgement                       |      4|get-answer       |     3|exact_match|â†‘  | 0.3209|Â±  |0.0342|
| - bbh_cot_fewshot_date_understanding                     |      4|get-answer       |     3|exact_match|â†‘  | 0.4520|Â±  |0.0315|
| - bbh_cot_fewshot_disambiguation_qa                      |      4|get-answer       |     3|exact_match|â†‘  | 0.6160|Â±  |0.0308|
| - bbh_cot_fewshot_dyck_languages                         |      4|get-answer       |     3|exact_match|â†‘  | 0.0080|Â±  |0.0056|
| - bbh_cot_fewshot_formal_fallacies                       |      4|get-answer       |     3|exact_match|â†‘  | 0.1960|Â±  |0.0252|
| - bbh_cot_fewshot_geometric_shapes                       |      4|get-answer       |     3|exact_match|â†‘  | 0.0960|Â±  |0.0187|
| - bbh_cot_fewshot_hyperbaton                             |      4|get-answer       |     3|exact_match|â†‘  | 0.4120|Â±  |0.0312|
| - bbh_cot_fewshot_logical_deduction_five_objects         |      4|get-answer       |     3|exact_match|â†‘  | 0.4000|Â±  |0.0310|
| - bbh_cot_fewshot_logical_deduction_seven_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.1600|Â±  |0.0232|
| - bbh_cot_fewshot_logical_deduction_three_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.8400|Â±  |0.0232|
| - bbh_cot_fewshot_movie_recommendation                   |      4|get-answer       |     3|exact_match|â†‘  | 0.0440|Â±  |0.0130|
| - bbh_cot_fewshot_multistep_arithmetic_two               |      4|get-answer       |     3|exact_match|â†‘  | 0.2600|Â±  |0.0278|
| - bbh_cot_fewshot_navigate                               |      4|get-answer       |     3|exact_match|â†‘  | 0.8880|Â±  |0.0200|
| - bbh_cot_fewshot_object_counting                        |      4|get-answer       |     3|exact_match|â†‘  | 0.5840|Â±  |0.0312|
| - bbh_cot_fewshot_penguins_in_a_table                    |      4|get-answer       |     3|exact_match|â†‘  | 0.1918|Â±  |0.0327|
| - bbh_cot_fewshot_reasoning_about_colored_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.5520|Â±  |0.0315|
| - bbh_cot_fewshot_ruin_names                             |      4|get-answer       |     3|exact_match|â†‘  | 0.0480|Â±  |0.0135|
| - bbh_cot_fewshot_salient_translation_error_detection    |      4|get-answer       |     3|exact_match|â†‘  | 0.2080|Â±  |0.0257|
| - bbh_cot_fewshot_snarks                                 |      4|get-answer       |     3|exact_match|â†‘  | 0.5281|Â±  |0.0375|
| - bbh_cot_fewshot_sports_understanding                   |      4|get-answer       |     3|exact_match|â†‘  | 0.7840|Â±  |0.0261|
| - bbh_cot_fewshot_temporal_sequences                     |      4|get-answer       |     3|exact_match|â†‘  | 0.7080|Â±  |0.0288|
| - bbh_cot_fewshot_tracking_shuffled_objects_five_objects |      4|get-answer       |     3|exact_match|â†‘  | 0.7480|Â±  |0.0275|
| - bbh_cot_fewshot_tracking_shuffled_objects_seven_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.5480|Â±  |0.0315|
| - bbh_cot_fewshot_tracking_shuffled_objects_three_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.8920|Â±  |0.0197|
| - bbh_cot_fewshot_web_of_lies                            |      4|get-answer       |     3|exact_match|â†‘  | 0.4280|Â±  |0.0314|
| - bbh_cot_fewshot_word_sorting                           |      4|get-answer       |     3|exact_match|â†‘  | 0.2720|Â±  |0.0282|
|boolq                                                     |      2|none             |     0|acc        |â†‘  | 0.8633|Â±  |0.0060|
|drop                                                      |      3|none             |     0|em         |â†‘  | 0.0025|Â±  |0.0005|
|                                                          |       |none             |     0|f1         |â†‘  | 0.0711|Â±  |0.0014|
|gpqa_diamond_cot_n_shot                                   |      2|flexible-extract |     0|exact_match|â†‘  | 0.1465|Â±  |0.0252|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_cot_zeroshot                                 |      1|flexible-extract |     0|exact_match|â†‘  | 0.0859|Â±  |0.0200|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_generative_n_shot                            |      2|flexible-extract |     0|exact_match|â†‘  | 0.2879|Â±  |0.0323|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_n_shot                                       |      2|none             |     0|acc        |â†‘  | 0.3283|Â±  |0.0335|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3283|Â±  |0.0335|
|gpqa_diamond_zeroshot                                     |      1|none             |     0|acc        |â†‘  | 0.3081|Â±  |0.0329|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3081|Â±  |0.0329|
|gpqa_extended_cot_n_shot                                  |      2|flexible-extract |     0|exact_match|â†‘  | 0.1410|Â±  |0.0149|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_cot_zeroshot                                |      1|flexible-extract |     0|exact_match|â†‘  | 0.1007|Â±  |0.0129|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_generative_n_shot                           |      2|flexible-extract |     0|exact_match|â†‘  | 0.2564|Â±  |0.0187|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_n_shot                                      |      2|none             |     0|acc        |â†‘  | 0.3242|Â±  |0.0200|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3242|Â±  |0.0200|
|gpqa_extended_zeroshot                                    |      1|none             |     0|acc        |â†‘  | 0.3388|Â±  |0.0203|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3388|Â±  |0.0203|
|gpqa_main_cot_n_shot                                      |      2|flexible-extract |     0|exact_match|â†‘  | 0.1429|Â±  |0.0166|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_cot_zeroshot                                    |      1|flexible-extract |     0|exact_match|â†‘  | 0.1027|Â±  |0.0144|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_generative_n_shot                               |      2|flexible-extract |     0|exact_match|â†‘  | 0.2946|Â±  |0.0216|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_n_shot                                          |      2|none             |     0|acc        |â†‘  | 0.3214|Â±  |0.0221|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3214|Â±  |0.0221|
|gpqa_main_zeroshot                                        |      1|none             |     0|acc        |â†‘  | 0.3281|Â±  |0.0222|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3281|Â±  |0.0222|
|gsm8k                                                     |      3|flexible-extract |     5|exact_match|â†‘  | 0.8256|Â±  |0.0105|
|                                                          |       |strict-match     |     5|exact_match|â†‘  | 0.7627|Â±  |0.0117|
|hellaswag                                                 |      1|none             |     0|acc        |â†‘  | 0.6194|Â±  |0.0048|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.8049|Â±  |0.0040|
|mmlu                                                      |      2|none             |      |acc        |â†‘  | 0.7181|Â±  |0.0036|
| - humanities                                             |      2|none             |      |acc        |â†‘  | 0.6372|Â±  |0.0066|
|  - formal_logic                                          |      1|none             |     0|acc        |â†‘  | 0.5714|Â±  |0.0443|
|  - high_school_european_history                          |      1|none             |     0|acc        |â†‘  | 0.8485|Â±  |0.0280|
|  - high_school_us_history                                |      1|none             |     0|acc        |â†‘  | 0.9020|Â±  |0.0209|
|  - high_school_world_history                             |      1|none             |     0|acc        |â†‘  | 0.8734|Â±  |0.0216|
|  - international_law                                     |      1|none             |     0|acc        |â†‘  | 0.8182|Â±  |0.0352|
|  - jurisprudence                                         |      1|none             |     0|acc        |â†‘  | 0.8056|Â±  |0.0383|
|  - logical_fallacies                                     |      1|none             |     0|acc        |â†‘  | 0.8221|Â±  |0.0300|
|  - moral_disputes                                        |      1|none             |     0|acc        |â†‘  | 0.7659|Â±  |0.0228|
|  - moral_scenarios                                       |      1|none             |     0|acc        |â†‘  | 0.4235|Â±  |0.0165|
|  - philosophy                                            |      1|none             |     0|acc        |â†‘  | 0.7395|Â±  |0.0249|
|  - prehistory                                            |      1|none             |     0|acc        |â†‘  | 0.8272|Â±  |0.0210|
|  - professional_law                                      |      1|none             |     0|acc        |â†‘  | 0.5156|Â±  |0.0128|
|  - world_religions                                       |      1|none             |     0|acc        |â†‘  | 0.8304|Â±  |0.0288|
| - other                                                  |      2|none             |      |acc        |â†‘  | 0.7647|Â±  |0.0073|
|  - business_ethics                                       |      1|none             |     0|acc        |â†‘  | 0.7900|Â±  |0.0409|
|  - clinical_knowledge                                    |      1|none             |     0|acc        |â†‘  | 0.7774|Â±  |0.0256|
|  - college_medicine                                      |      1|none             |     0|acc        |â†‘  | 0.6705|Â±  |0.0358|
|  - global_facts                                          |      1|none             |     0|acc        |â†‘  | 0.4700|Â±  |0.0502|
|  - human_aging                                           |      1|none             |     0|acc        |â†‘  | 0.7803|Â±  |0.0278|
|  - management                                            |      1|none             |     0|acc        |â†‘  | 0.8932|Â±  |0.0306|
|  - marketing                                             |      1|none             |     0|acc        |â†‘  | 0.9188|Â±  |0.0179|
|  - medical_genetics                                      |      1|none             |     0|acc        |â†‘  | 0.8400|Â±  |0.0368|
|  - miscellaneous                                         |      1|none             |     0|acc        |â†‘  | 0.8519|Â±  |0.0127|
|  - nutrition                                             |      1|none             |     0|acc        |â†‘  | 0.7843|Â±  |0.0236|
|  - professional_accounting                               |      1|none             |     0|acc        |â†‘  | 0.5567|Â±  |0.0296|
|  - professional_medicine                                 |      1|none             |     0|acc        |â†‘  | 0.7831|Â±  |0.0250|
|  - virology                                              |      1|none             |     0|acc        |â†‘  | 0.5181|Â±  |0.0389|
| - social sciences                                        |      2|none             |      |acc        |â†‘  | 0.8284|Â±  |0.0067|
|  - econometrics                                          |      1|none             |     0|acc        |â†‘  | 0.6579|Â±  |0.0446|
|  - high_school_geography                                 |      1|none             |     0|acc        |â†‘  | 0.8788|Â±  |0.0233|
|  - high_school_government_and_politics                   |      1|none             |     0|acc        |â†‘  | 0.9378|Â±  |0.0174|
|  - high_school_macroeconomics                            |      1|none             |     0|acc        |â†‘  | 0.7949|Â±  |0.0205|
|  - high_school_microeconomics                            |      1|none             |     0|acc        |â†‘  | 0.8697|Â±  |0.0219|
|  - high_school_psychology                                |      1|none             |     0|acc        |â†‘  | 0.9064|Â±  |0.0125|
|  - human_sexuality                                       |      1|none             |     0|acc        |â†‘  | 0.8092|Â±  |0.0345|
|  - professional_psychology                               |      1|none             |     0|acc        |â†‘  | 0.7647|Â±  |0.0172|
|  - public_relations                                      |      1|none             |     0|acc        |â†‘  | 0.7182|Â±  |0.0431|
|  - security_studies                                      |      1|none             |     0|acc        |â†‘  | 0.7714|Â±  |0.0269|
|  - sociology                                             |      1|none             |     0|acc        |â†‘  | 0.8955|Â±  |0.0216|
|  - us_foreign_policy                                     |      1|none             |     0|acc        |â†‘  | 0.8600|Â±  |0.0349|
| - stem                                                   |      2|none             |      |acc        |â†‘  | 0.6851|Â±  |0.0080|
|  - abstract_algebra                                      |      1|none             |     0|acc        |â†‘  | 0.5800|Â±  |0.0496|
|  - anatomy                                               |      1|none             |     0|acc        |â†‘  | 0.7407|Â±  |0.0379|
|  - astronomy                                             |      1|none             |     0|acc        |â†‘  | 0.8553|Â±  |0.0286|
|  - college_biology                                       |      1|none             |     0|acc        |â†‘  | 0.8681|Â±  |0.0283|
|  - college_chemistry                                     |      1|none             |     0|acc        |â†‘  | 0.5300|Â±  |0.0502|
|  - college_computer_science                              |      1|none             |     0|acc        |â†‘  | 0.6700|Â±  |0.0473|
|  - college_mathematics                                   |      1|none             |     0|acc        |â†‘  | 0.4300|Â±  |0.0498|
|  - college_physics                                       |      1|none             |     0|acc        |â†‘  | 0.5098|Â±  |0.0497|
|  - computer_security                                     |      1|none             |     0|acc        |â†‘  | 0.7700|Â±  |0.0423|
|  - conceptual_physics                                    |      1|none             |     0|acc        |â†‘  | 0.7660|Â±  |0.0277|
|  - electrical_engineering                                |      1|none             |     0|acc        |â†‘  | 0.7172|Â±  |0.0375|
|  - elementary_mathematics                                |      1|none             |     0|acc        |â†‘  | 0.6746|Â±  |0.0241|
|  - high_school_biology                                   |      1|none             |     0|acc        |â†‘  | 0.8645|Â±  |0.0195|
|  - high_school_chemistry                                 |      1|none             |     0|acc        |â†‘  | 0.6404|Â±  |0.0338|
|  - high_school_computer_science                          |      1|none             |     0|acc        |â†‘  | 0.8100|Â±  |0.0394|
|  - high_school_mathematics                               |      1|none             |     0|acc        |â†‘  | 0.5407|Â±  |0.0304|
|  - high_school_physics                                   |      1|none             |     0|acc        |â†‘  | 0.5695|Â±  |0.0404|
|  - high_school_statistics                                |      1|none             |     0|acc        |â†‘  | 0.6759|Â±  |0.0319|
|  - machine_learning                                      |      1|none             |     0|acc        |â†‘  | 0.5268|Â±  |0.0474|
|nq_open                                                   |      4|remove_whitespace|     0|exact_match|â†‘  | 0.0457|Â±  |0.0035|
|openbookqa                                                |      1|none             |     0|acc        |â†‘  | 0.3420|Â±  |0.0212|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.4860|Â±  |0.0224|
|piqa                                                      |      1|none             |     0|acc        |â†‘  | 0.7960|Â±  |0.0094|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.8030|Â±  |0.0093|
|qnli                                                      |      1|none             |     0|acc        |â†‘  | 0.8045|Â±  |0.0054|
|sciq                                                      |      1|none             |     0|acc        |â†‘  | 0.9560|Â±  |0.0065|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.9370|Â±  |0.0077|
|triviaqa                                                  |      3|remove_whitespace|     0|exact_match|â†‘  | 0.3254|Â±  |0.0035|
|truthfulqa_gen                                            |      3|none             |     0|bleu_acc   |â†‘  | 0.5006|Â±  |0.0175|
|                                                          |       |none             |     0|bleu_diff  |â†‘  | 0.3019|Â±  |0.2337|
|                                                          |       |none             |     0|bleu_max   |â†‘  | 7.9281|Â±  |0.4300|
|                                                          |       |none             |     0|rouge1_acc |â†‘  | 0.5386|Â±  |0.0175|
|                                                          |       |none             |     0|rouge1_diff|â†‘  | 0.9126|Â±  |0.3421|
|                                                          |       |none             |     0|rouge1_max |â†‘  |25.3456|Â±  |0.6494|
|                                                          |       |none             |     0|rouge2_acc |â†‘  | 0.4455|Â±  |0.0174|
|                                                          |       |none             |     0|rouge2_diff|â†‘  | 0.2550|Â±  |0.3623|
|                                                          |       |none             |     0|rouge2_max |â†‘  |15.0210|Â±  |0.6025|
|                                                          |       |none             |     0|rougeL_acc |â†‘  | 0.4847|Â±  |0.0175|
|                                                          |       |none             |     0|rougeL_diff|â†‘  | 0.3002|Â±  |0.3241|
|                                                          |       |none             |     0|rougeL_max |â†‘  |22.2349|Â±  |0.6237|
|truthfulqa_mc1                                            |      2|none             |     0|acc        |â†‘  | 0.4774|Â±  |0.0175|
|truthfulqa_mc2                                            |      3|none             |     0|acc        |â†‘  | 0.6485|Â±  |0.0155|
|winogrande                                                |      1|none             |     0|acc        |â†‘  | 0.7119|Â±  |0.0127|

|      Groups      |Version|  Filter  |n-shot|  Metric   |   |Value |   |Stderr|
|------------------|------:|----------|------|-----------|---|-----:|---|-----:|
|bbh               |      3|get-answer|      |exact_match|â†‘  |0.4488|Â±  |0.0051|
|mmlu              |      2|none      |      |acc        |â†‘  |0.7181|Â±  |0.0036|
| - humanities     |      2|none      |      |acc        |â†‘  |0.6372|Â±  |0.0066|
| - other          |      2|none      |      |acc        |â†‘  |0.7647|Â±  |0.0073|
| - social sciences|      2|none      |      |acc        |â†‘  |0.8284|Â±  |0.0067|
| - stem           |      2|none      |      |acc        |â†‘  |0.6851|Â±  |0.0080|

Qwen_Qwen2.5-7B-Instruct: 9h 37m 6s
âœ… Benchmark completed for Qwen_Qwen2.5-7B-Instruct

ðŸ”¥ Starting benchmark for Qwen_Qwen-7B-Chat
Qwen_Qwen-7B-Chat: 0h 5m 2s
âœ… Benchmark completed for Qwen_Qwen-7B-Chat

ðŸ”¥ Starting benchmark for Qwen_Qwen-7B
Qwen_Qwen-7B: 0h 5m 1s
âœ… Benchmark completed for Qwen_Qwen-7B

ðŸ”¥ Starting benchmark for meta-llama_Meta-Llama-3-8B-Instruct
hf (pretrained=/mnt/data8tb/Documents/llm/llm_models/meta-llama_Meta-Llama-3-8B-Instruct,trust_remote_code=True), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 3
|                          Tasks                           |Version|     Filter      |n-shot|  Metric   |   | Value |   |Stderr|
|----------------------------------------------------------|------:|-----------------|-----:|-----------|---|------:|---|-----:|
|anli_r1                                                   |      1|none             |     0|acc        |â†‘  | 0.4840|Â±  |0.0158|
|anli_r2                                                   |      1|none             |     0|acc        |â†‘  | 0.4580|Â±  |0.0158|
|anli_r3                                                   |      1|none             |     0|acc        |â†‘  | 0.4483|Â±  |0.0144|
|arc_challenge                                             |      1|none             |     0|acc        |â†‘  | 0.5316|Â±  |0.0146|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.5640|Â±  |0.0145|
|bbh                                                       |      3|get-answer       |      |exact_match|â†‘  | 0.6790|Â±  |0.0053|
| - bbh_cot_fewshot_boolean_expressions                    |      4|get-answer       |     3|exact_match|â†‘  | 0.8960|Â±  |0.0193|
| - bbh_cot_fewshot_causal_judgement                       |      4|get-answer       |     3|exact_match|â†‘  | 0.5668|Â±  |0.0363|
| - bbh_cot_fewshot_date_understanding                     |      4|get-answer       |     3|exact_match|â†‘  | 0.8400|Â±  |0.0232|
| - bbh_cot_fewshot_disambiguation_qa                      |      4|get-answer       |     3|exact_match|â†‘  | 0.6800|Â±  |0.0296|
| - bbh_cot_fewshot_dyck_languages                         |      4|get-answer       |     3|exact_match|â†‘  | 0.0880|Â±  |0.0180|
| - bbh_cot_fewshot_formal_fallacies                       |      4|get-answer       |     3|exact_match|â†‘  | 0.5400|Â±  |0.0316|
| - bbh_cot_fewshot_geometric_shapes                       |      4|get-answer       |     3|exact_match|â†‘  | 0.5160|Â±  |0.0317|
| - bbh_cot_fewshot_hyperbaton                             |      4|get-answer       |     3|exact_match|â†‘  | 0.7880|Â±  |0.0259|
| - bbh_cot_fewshot_logical_deduction_five_objects         |      4|get-answer       |     3|exact_match|â†‘  | 0.5720|Â±  |0.0314|
| - bbh_cot_fewshot_logical_deduction_seven_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.4920|Â±  |0.0317|
| - bbh_cot_fewshot_logical_deduction_three_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.9080|Â±  |0.0183|
| - bbh_cot_fewshot_movie_recommendation                   |      4|get-answer       |     3|exact_match|â†‘  | 0.6640|Â±  |0.0299|
| - bbh_cot_fewshot_multistep_arithmetic_two               |      4|get-answer       |     3|exact_match|â†‘  | 0.4040|Â±  |0.0311|
| - bbh_cot_fewshot_navigate                               |      4|get-answer       |     3|exact_match|â†‘  | 0.8480|Â±  |0.0228|
| - bbh_cot_fewshot_object_counting                        |      4|get-answer       |     3|exact_match|â†‘  | 0.8880|Â±  |0.0200|
| - bbh_cot_fewshot_penguins_in_a_table                    |      4|get-answer       |     3|exact_match|â†‘  | 0.7877|Â±  |0.0340|
| - bbh_cot_fewshot_reasoning_about_colored_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.7320|Â±  |0.0281|
| - bbh_cot_fewshot_ruin_names                             |      4|get-answer       |     3|exact_match|â†‘  | 0.7400|Â±  |0.0278|
| - bbh_cot_fewshot_salient_translation_error_detection    |      4|get-answer       |     3|exact_match|â†‘  | 0.5680|Â±  |0.0314|
| - bbh_cot_fewshot_snarks                                 |      4|get-answer       |     3|exact_match|â†‘  | 0.7135|Â±  |0.0340|
| - bbh_cot_fewshot_sports_understanding                   |      4|get-answer       |     3|exact_match|â†‘  | 0.9160|Â±  |0.0176|
| - bbh_cot_fewshot_temporal_sequences                     |      4|get-answer       |     3|exact_match|â†‘  | 0.6720|Â±  |0.0298|
| - bbh_cot_fewshot_tracking_shuffled_objects_five_objects |      4|get-answer       |     3|exact_match|â†‘  | 0.7080|Â±  |0.0288|
| - bbh_cot_fewshot_tracking_shuffled_objects_seven_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.5280|Â±  |0.0316|
| - bbh_cot_fewshot_tracking_shuffled_objects_three_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.7680|Â±  |0.0268|
| - bbh_cot_fewshot_web_of_lies                            |      4|get-answer       |     3|exact_match|â†‘  | 0.9960|Â±  |0.0040|
| - bbh_cot_fewshot_word_sorting                           |      4|get-answer       |     3|exact_match|â†‘  | 0.5400|Â±  |0.0316|
|boolq                                                     |      2|none             |     0|acc        |â†‘  | 0.8312|Â±  |0.0066|
|drop                                                      |      3|none             |     0|em         |â†‘  | 0.0290|Â±  |0.0017|
|                                                          |       |none             |     0|f1         |â†‘  | 0.1640|Â±  |0.0024|
|gpqa_diamond_cot_n_shot                                   |      2|flexible-extract |     0|exact_match|â†‘  | 0.1515|Â±  |0.0255|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_cot_zeroshot                                 |      1|flexible-extract |     0|exact_match|â†‘  | 0.1414|Â±  |0.0248|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_generative_n_shot                            |      2|flexible-extract |     0|exact_match|â†‘  | 0.2222|Â±  |0.0296|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_n_shot                                       |      2|none             |     0|acc        |â†‘  | 0.3283|Â±  |0.0335|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3283|Â±  |0.0335|
|gpqa_diamond_zeroshot                                     |      1|none             |     0|acc        |â†‘  | 0.2778|Â±  |0.0319|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2778|Â±  |0.0319|
|gpqa_extended_cot_n_shot                                  |      2|flexible-extract |     0|exact_match|â†‘  | 0.1923|Â±  |0.0169|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_cot_zeroshot                                |      1|flexible-extract |     0|exact_match|â†‘  | 0.1575|Â±  |0.0156|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_generative_n_shot                           |      2|flexible-extract |     0|exact_match|â†‘  | 0.2619|Â±  |0.0188|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_n_shot                                      |      2|none             |     0|acc        |â†‘  | 0.2967|Â±  |0.0196|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2967|Â±  |0.0196|
|gpqa_extended_zeroshot                                    |      1|none             |     0|acc        |â†‘  | 0.3260|Â±  |0.0201|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3260|Â±  |0.0201|
|gpqa_main_cot_n_shot                                      |      2|flexible-extract |     0|exact_match|â†‘  | 0.1830|Â±  |0.0183|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0022|Â±  |0.0022|
|gpqa_main_cot_zeroshot                                    |      1|flexible-extract |     0|exact_match|â†‘  | 0.1429|Â±  |0.0166|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_generative_n_shot                               |      2|flexible-extract |     0|exact_match|â†‘  | 0.2790|Â±  |0.0212|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_n_shot                                          |      2|none             |     0|acc        |â†‘  | 0.3058|Â±  |0.0218|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3058|Â±  |0.0218|
|gpqa_main_zeroshot                                        |      1|none             |     0|acc        |â†‘  | 0.3103|Â±  |0.0219|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3103|Â±  |0.0219|
|gsm8k                                                     |      3|flexible-extract |     5|exact_match|â†‘  | 0.7544|Â±  |0.0119|
|                                                          |       |strict-match     |     5|exact_match|â†‘  | 0.7566|Â±  |0.0118|
|hellaswag                                                 |      1|none             |     0|acc        |â†‘  | 0.5764|Â±  |0.0049|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.7592|Â±  |0.0043|
|mmlu                                                      |      2|none             |      |acc        |â†‘  | 0.6387|Â±  |0.0038|
| - humanities                                             |      2|none             |      |acc        |â†‘  | 0.5824|Â±  |0.0068|
|  - formal_logic                                          |      1|none             |     0|acc        |â†‘  | 0.5000|Â±  |0.0447|
|  - high_school_european_history                          |      1|none             |     0|acc        |â†‘  | 0.7455|Â±  |0.0340|
|  - high_school_us_history                                |      1|none             |     0|acc        |â†‘  | 0.8186|Â±  |0.0270|
|  - high_school_world_history                             |      1|none             |     0|acc        |â†‘  | 0.8354|Â±  |0.0241|
|  - international_law                                     |      1|none             |     0|acc        |â†‘  | 0.7603|Â±  |0.0390|
|  - jurisprudence                                         |      1|none             |     0|acc        |â†‘  | 0.7593|Â±  |0.0413|
|  - logical_fallacies                                     |      1|none             |     0|acc        |â†‘  | 0.7607|Â±  |0.0335|
|  - moral_disputes                                        |      1|none             |     0|acc        |â†‘  | 0.6994|Â±  |0.0247|
|  - moral_scenarios                                       |      1|none             |     0|acc        |â†‘  | 0.3441|Â±  |0.0159|
|  - philosophy                                            |      1|none             |     0|acc        |â†‘  | 0.7170|Â±  |0.0256|
|  - prehistory                                            |      1|none             |     0|acc        |â†‘  | 0.7253|Â±  |0.0248|
|  - professional_law                                      |      1|none             |     0|acc        |â†‘  | 0.4896|Â±  |0.0128|
|  - world_religions                                       |      1|none             |     0|acc        |â†‘  | 0.7719|Â±  |0.0322|
| - other                                                  |      2|none             |      |acc        |â†‘  | 0.7187|Â±  |0.0078|
|  - business_ethics                                       |      1|none             |     0|acc        |â†‘  | 0.6600|Â±  |0.0476|
|  - clinical_knowledge                                    |      1|none             |     0|acc        |â†‘  | 0.7208|Â±  |0.0276|
|  - college_medicine                                      |      1|none             |     0|acc        |â†‘  | 0.6243|Â±  |0.0369|
|  - global_facts                                          |      1|none             |     0|acc        |â†‘  | 0.4200|Â±  |0.0496|
|  - human_aging                                           |      1|none             |     0|acc        |â†‘  | 0.6726|Â±  |0.0315|
|  - management                                            |      1|none             |     0|acc        |â†‘  | 0.8350|Â±  |0.0368|
|  - marketing                                             |      1|none             |     0|acc        |â†‘  | 0.8932|Â±  |0.0202|
|  - medical_genetics                                      |      1|none             |     0|acc        |â†‘  | 0.8100|Â±  |0.0394|
|  - miscellaneous                                         |      1|none             |     0|acc        |â†‘  | 0.8059|Â±  |0.0141|
|  - nutrition                                             |      1|none             |     0|acc        |â†‘  | 0.7320|Â±  |0.0254|
|  - professional_accounting                               |      1|none             |     0|acc        |â†‘  | 0.5532|Â±  |0.0297|
|  - professional_medicine                                 |      1|none             |     0|acc        |â†‘  | 0.7500|Â±  |0.0263|
|  - virology                                              |      1|none             |     0|acc        |â†‘  | 0.5120|Â±  |0.0389|
| - social sciences                                        |      2|none             |      |acc        |â†‘  | 0.7413|Â±  |0.0078|
|  - econometrics                                          |      1|none             |     0|acc        |â†‘  | 0.5526|Â±  |0.0468|
|  - high_school_geography                                 |      1|none             |     0|acc        |â†‘  | 0.7778|Â±  |0.0296|
|  - high_school_government_and_politics                   |      1|none             |     0|acc        |â†‘  | 0.8756|Â±  |0.0238|
|  - high_school_macroeconomics                            |      1|none             |     0|acc        |â†‘  | 0.6513|Â±  |0.0242|
|  - high_school_microeconomics                            |      1|none             |     0|acc        |â†‘  | 0.7143|Â±  |0.0293|
|  - high_school_psychology                                |      1|none             |     0|acc        |â†‘  | 0.8239|Â±  |0.0163|
|  - human_sexuality                                       |      1|none             |     0|acc        |â†‘  | 0.7557|Â±  |0.0377|
|  - professional_psychology                               |      1|none             |     0|acc        |â†‘  | 0.6699|Â±  |0.0190|
|  - public_relations                                      |      1|none             |     0|acc        |â†‘  | 0.6909|Â±  |0.0443|
|  - security_studies                                      |      1|none             |     0|acc        |â†‘  | 0.7388|Â±  |0.0281|
|  - sociology                                             |      1|none             |     0|acc        |â†‘  | 0.8557|Â±  |0.0248|
|  - us_foreign_policy                                     |      1|none             |     0|acc        |â†‘  | 0.8400|Â±  |0.0368|
| - stem                                                   |      2|none             |      |acc        |â†‘  | 0.5439|Â±  |0.0086|
|  - abstract_algebra                                      |      1|none             |     0|acc        |â†‘  | 0.3100|Â±  |0.0465|
|  - anatomy                                               |      1|none             |     0|acc        |â†‘  | 0.6370|Â±  |0.0415|
|  - astronomy                                             |      1|none             |     0|acc        |â†‘  | 0.6908|Â±  |0.0376|
|  - college_biology                                       |      1|none             |     0|acc        |â†‘  | 0.7431|Â±  |0.0365|
|  - college_chemistry                                     |      1|none             |     0|acc        |â†‘  | 0.4100|Â±  |0.0494|
|  - college_computer_science                              |      1|none             |     0|acc        |â†‘  | 0.5100|Â±  |0.0502|
|  - college_mathematics                                   |      1|none             |     0|acc        |â†‘  | 0.3300|Â±  |0.0473|
|  - college_physics                                       |      1|none             |     0|acc        |â†‘  | 0.4902|Â±  |0.0497|
|  - computer_security                                     |      1|none             |     0|acc        |â†‘  | 0.7700|Â±  |0.0423|
|  - conceptual_physics                                    |      1|none             |     0|acc        |â†‘  | 0.5489|Â±  |0.0325|
|  - electrical_engineering                                |      1|none             |     0|acc        |â†‘  | 0.6345|Â±  |0.0401|
|  - elementary_mathematics                                |      1|none             |     0|acc        |â†‘  | 0.4577|Â±  |0.0257|
|  - high_school_biology                                   |      1|none             |     0|acc        |â†‘  | 0.7677|Â±  |0.0240|
|  - high_school_chemistry                                 |      1|none             |     0|acc        |â†‘  | 0.4828|Â±  |0.0352|
|  - high_school_computer_science                          |      1|none             |     0|acc        |â†‘  | 0.6800|Â±  |0.0469|
|  - high_school_mathematics                               |      1|none             |     0|acc        |â†‘  | 0.3778|Â±  |0.0296|
|  - high_school_physics                                   |      1|none             |     0|acc        |â†‘  | 0.4437|Â±  |0.0406|
|  - high_school_statistics                                |      1|none             |     0|acc        |â†‘  | 0.5185|Â±  |0.0341|
|  - machine_learning                                      |      1|none             |     0|acc        |â†‘  | 0.4911|Â±  |0.0475|
|nq_open                                                   |      4|remove_whitespace|     0|exact_match|â†‘  | 0.1590|Â±  |0.0061|
|openbookqa                                                |      1|none             |     0|acc        |â†‘  | 0.3400|Â±  |0.0212|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.4300|Â±  |0.0222|
|piqa                                                      |      1|none             |     0|acc        |â†‘  | 0.7824|Â±  |0.0096|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.7873|Â±  |0.0095|
|qnli                                                      |      1|none             |     0|acc        |â†‘  | 0.5464|Â±  |0.0067|
|sciq                                                      |      1|none             |     0|acc        |â†‘  | 0.9630|Â±  |0.0060|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.9320|Â±  |0.0080|
|triviaqa                                                  |      3|remove_whitespace|     0|exact_match|â†‘  | 0.5112|Â±  |0.0037|
|truthfulqa_gen                                            |      3|none             |     0|bleu_acc   |â†‘  | 0.4761|Â±  |0.0175|
|                                                          |       |none             |     0|bleu_diff  |â†‘  |-0.1939|Â±  |0.6341|
|                                                          |       |none             |     0|bleu_max   |â†‘  |20.2147|Â±  |0.7257|
|                                                          |       |none             |     0|rouge1_acc |â†‘  | 0.4957|Â±  |0.0175|
|                                                          |       |none             |     0|rouge1_diff|â†‘  |-0.2355|Â±  |0.8648|
|                                                          |       |none             |     0|rouge1_max |â†‘  |43.2820|Â±  |0.8713|
|                                                          |       |none             |     0|rouge2_acc |â†‘  | 0.3684|Â±  |0.0169|
|                                                          |       |none             |     0|rouge2_diff|â†‘  |-1.5024|Â±  |0.9176|
|                                                          |       |none             |     0|rouge2_max |â†‘  |27.2640|Â±  |0.9552|
|                                                          |       |none             |     0|rougeL_acc |â†‘  | 0.4798|Â±  |0.0175|
|                                                          |       |none             |     0|rougeL_diff|â†‘  |-0.6690|Â±  |0.8701|
|                                                          |       |none             |     0|rougeL_max |â†‘  |40.4168|Â±  |0.8713|
|truthfulqa_mc1                                            |      2|none             |     0|acc        |â†‘  | 0.3635|Â±  |0.0168|
|truthfulqa_mc2                                            |      3|none             |     0|acc        |â†‘  | 0.5171|Â±  |0.0152|
|winogrande                                                |      1|none             |     0|acc        |â†‘  | 0.7167|Â±  |0.0127|

|      Groups      |Version|  Filter  |n-shot|  Metric   |   |Value |   |Stderr|
|------------------|------:|----------|------|-----------|---|-----:|---|-----:|
|bbh               |      3|get-answer|      |exact_match|â†‘  |0.6790|Â±  |0.0053|
|mmlu              |      2|none      |      |acc        |â†‘  |0.6387|Â±  |0.0038|
| - humanities     |      2|none      |      |acc        |â†‘  |0.5824|Â±  |0.0068|
| - other          |      2|none      |      |acc        |â†‘  |0.7187|Â±  |0.0078|
| - social sciences|      2|none      |      |acc        |â†‘  |0.7413|Â±  |0.0078|
| - stem           |      2|none      |      |acc        |â†‘  |0.5439|Â±  |0.0086|

meta-llama_Meta-Llama-3-8B-Instruct: 6h 30m 49s
âœ… Benchmark completed for meta-llama_Meta-Llama-3-8B-Instruct

ðŸ”¥ Starting benchmark for mistralai_Mistral-7B-Instruct-v0.3
hf (pretrained=/mnt/data8tb/Documents/llm/llm_models/mistralai_Mistral-7B-Instruct-v0.3,trust_remote_code=True), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 3
|                          Tasks                           |Version|     Filter      |n-shot|  Metric   |   | Value |   |Stderr|
|----------------------------------------------------------|------:|-----------------|-----:|-----------|---|------:|---|-----:|
|anli_r1                                                   |      1|none             |     0|acc        |â†‘  | 0.4760|Â±  |0.0158|
|anli_r2                                                   |      1|none             |     0|acc        |â†‘  | 0.4430|Â±  |0.0157|
|anli_r3                                                   |      1|none             |     0|acc        |â†‘  | 0.4483|Â±  |0.0144|
|arc_challenge                                             |      1|none             |     0|acc        |â†‘  | 0.5742|Â±  |0.0144|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.5896|Â±  |0.0144|
|bbh                                                       |      3|get-answer       |      |exact_match|â†‘  | 0.5626|Â±  |0.0056|
| - bbh_cot_fewshot_boolean_expressions                    |      4|get-answer       |     3|exact_match|â†‘  | 0.8520|Â±  |0.0225|
| - bbh_cot_fewshot_causal_judgement                       |      4|get-answer       |     3|exact_match|â†‘  | 0.5508|Â±  |0.0365|
| - bbh_cot_fewshot_date_understanding                     |      4|get-answer       |     3|exact_match|â†‘  | 0.6400|Â±  |0.0304|
| - bbh_cot_fewshot_disambiguation_qa                      |      4|get-answer       |     3|exact_match|â†‘  | 0.5960|Â±  |0.0311|
| - bbh_cot_fewshot_dyck_languages                         |      4|get-answer       |     3|exact_match|â†‘  | 0.0720|Â±  |0.0164|
| - bbh_cot_fewshot_formal_fallacies                       |      4|get-answer       |     3|exact_match|â†‘  | 0.5400|Â±  |0.0316|
| - bbh_cot_fewshot_geometric_shapes                       |      4|get-answer       |     3|exact_match|â†‘  | 0.4760|Â±  |0.0316|
| - bbh_cot_fewshot_hyperbaton                             |      4|get-answer       |     3|exact_match|â†‘  | 0.6160|Â±  |0.0308|
| - bbh_cot_fewshot_logical_deduction_five_objects         |      4|get-answer       |     3|exact_match|â†‘  | 0.4800|Â±  |0.0317|
| - bbh_cot_fewshot_logical_deduction_seven_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.4680|Â±  |0.0316|
| - bbh_cot_fewshot_logical_deduction_three_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.8080|Â±  |0.0250|
| - bbh_cot_fewshot_movie_recommendation                   |      4|get-answer       |     3|exact_match|â†‘  | 0.7240|Â±  |0.0283|
| - bbh_cot_fewshot_multistep_arithmetic_two               |      4|get-answer       |     3|exact_match|â†‘  | 0.2960|Â±  |0.0289|
| - bbh_cot_fewshot_navigate                               |      4|get-answer       |     3|exact_match|â†‘  | 0.6680|Â±  |0.0298|
| - bbh_cot_fewshot_object_counting                        |      4|get-answer       |     3|exact_match|â†‘  | 0.6160|Â±  |0.0308|
| - bbh_cot_fewshot_penguins_in_a_table                    |      4|get-answer       |     3|exact_match|â†‘  | 0.5959|Â±  |0.0408|
| - bbh_cot_fewshot_reasoning_about_colored_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.6880|Â±  |0.0294|
| - bbh_cot_fewshot_ruin_names                             |      4|get-answer       |     3|exact_match|â†‘  | 0.4400|Â±  |0.0315|
| - bbh_cot_fewshot_salient_translation_error_detection    |      4|get-answer       |     3|exact_match|â†‘  | 0.4280|Â±  |0.0314|
| - bbh_cot_fewshot_snarks                                 |      4|get-answer       |     3|exact_match|â†‘  | 0.7191|Â±  |0.0338|
| - bbh_cot_fewshot_sports_understanding                   |      4|get-answer       |     3|exact_match|â†‘  | 0.9200|Â±  |0.0172|
| - bbh_cot_fewshot_temporal_sequences                     |      4|get-answer       |     3|exact_match|â†‘  | 0.5240|Â±  |0.0316|
| - bbh_cot_fewshot_tracking_shuffled_objects_five_objects |      4|get-answer       |     3|exact_match|â†‘  | 0.4240|Â±  |0.0313|
| - bbh_cot_fewshot_tracking_shuffled_objects_seven_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.3000|Â±  |0.0290|
| - bbh_cot_fewshot_tracking_shuffled_objects_three_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.5840|Â±  |0.0312|
| - bbh_cot_fewshot_web_of_lies                            |      4|get-answer       |     3|exact_match|â†‘  | 1.0000|Â±  |0.0000|
| - bbh_cot_fewshot_word_sorting                           |      4|get-answer       |     3|exact_match|â†‘  | 0.2200|Â±  |0.0263|
|boolq                                                     |      2|none             |     0|acc        |â†‘  | 0.8584|Â±  |0.0061|
|drop                                                      |      3|none             |     0|em         |â†‘  | 0.0094|Â±  |0.0010|
|                                                          |       |none             |     0|f1         |â†‘  | 0.0900|Â±  |0.0018|
|gpqa_diamond_cot_n_shot                                   |      2|flexible-extract |     0|exact_match|â†‘  | 0.1162|Â±  |0.0228|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_cot_zeroshot                                 |      1|flexible-extract |     0|exact_match|â†‘  | 0.1616|Â±  |0.0262|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_generative_n_shot                            |      2|flexible-extract |     0|exact_match|â†‘  | 0.2172|Â±  |0.0294|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_n_shot                                       |      2|none             |     0|acc        |â†‘  | 0.3283|Â±  |0.0335|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3283|Â±  |0.0335|
|gpqa_diamond_zeroshot                                     |      1|none             |     0|acc        |â†‘  | 0.2677|Â±  |0.0315|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2677|Â±  |0.0315|
|gpqa_extended_cot_n_shot                                  |      2|flexible-extract |     0|exact_match|â†‘  | 0.1777|Â±  |0.0164|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_cot_zeroshot                                |      1|flexible-extract |     0|exact_match|â†‘  | 0.1447|Â±  |0.0151|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_generative_n_shot                           |      2|flexible-extract |     0|exact_match|â†‘  | 0.2582|Â±  |0.0187|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_n_shot                                      |      2|none             |     0|acc        |â†‘  | 0.2894|Â±  |0.0194|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2894|Â±  |0.0194|
|gpqa_extended_zeroshot                                    |      1|none             |     0|acc        |â†‘  | 0.3480|Â±  |0.0204|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3480|Â±  |0.0204|
|gpqa_main_cot_n_shot                                      |      2|flexible-extract |     0|exact_match|â†‘  | 0.1585|Â±  |0.0173|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_cot_zeroshot                                    |      1|flexible-extract |     0|exact_match|â†‘  | 0.1518|Â±  |0.0170|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_generative_n_shot                               |      2|flexible-extract |     0|exact_match|â†‘  | 0.2790|Â±  |0.0212|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_n_shot                                          |      2|none             |     0|acc        |â†‘  | 0.3371|Â±  |0.0224|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3371|Â±  |0.0224|
|gpqa_main_zeroshot                                        |      1|none             |     0|acc        |â†‘  | 0.2835|Â±  |0.0213|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2835|Â±  |0.0213|
|gsm8k                                                     |      3|flexible-extract |     5|exact_match|â†‘  | 0.4913|Â±  |0.0138|
|                                                          |       |strict-match     |     5|exact_match|â†‘  | 0.4898|Â±  |0.0138|
|hellaswag                                                 |      1|none             |     0|acc        |â†‘  | 0.6484|Â±  |0.0048|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.8289|Â±  |0.0038|
|mmlu                                                      |      2|none             |      |acc        |â†‘  | 0.5971|Â±  |0.0039|
| - humanities                                             |      2|none             |      |acc        |â†‘  | 0.5420|Â±  |0.0067|
|  - formal_logic                                          |      1|none             |     0|acc        |â†‘  | 0.4365|Â±  |0.0444|
|  - high_school_european_history                          |      1|none             |     0|acc        |â†‘  | 0.7394|Â±  |0.0343|
|  - high_school_us_history                                |      1|none             |     0|acc        |â†‘  | 0.8039|Â±  |0.0279|
|  - high_school_world_history                             |      1|none             |     0|acc        |â†‘  | 0.7722|Â±  |0.0273|
|  - international_law                                     |      1|none             |     0|acc        |â†‘  | 0.7686|Â±  |0.0385|
|  - jurisprudence                                         |      1|none             |     0|acc        |â†‘  | 0.7593|Â±  |0.0413|
|  - logical_fallacies                                     |      1|none             |     0|acc        |â†‘  | 0.7607|Â±  |0.0335|
|  - moral_disputes                                        |      1|none             |     0|acc        |â†‘  | 0.6763|Â±  |0.0252|
|  - moral_scenarios                                       |      1|none             |     0|acc        |â†‘  | 0.2670|Â±  |0.0148|
|  - philosophy                                            |      1|none             |     0|acc        |â†‘  | 0.6624|Â±  |0.0269|
|  - prehistory                                            |      1|none             |     0|acc        |â†‘  | 0.6883|Â±  |0.0258|
|  - professional_law                                      |      1|none             |     0|acc        |â†‘  | 0.4492|Â±  |0.0127|
|  - world_religions                                       |      1|none             |     0|acc        |â†‘  | 0.7953|Â±  |0.0309|
| - other                                                  |      2|none             |      |acc        |â†‘  | 0.6720|Â±  |0.0081|
|  - business_ethics                                       |      1|none             |     0|acc        |â†‘  | 0.6000|Â±  |0.0492|
|  - clinical_knowledge                                    |      1|none             |     0|acc        |â†‘  | 0.6906|Â±  |0.0285|
|  - college_medicine                                      |      1|none             |     0|acc        |â†‘  | 0.5607|Â±  |0.0378|
|  - global_facts                                          |      1|none             |     0|acc        |â†‘  | 0.3900|Â±  |0.0490|
|  - human_aging                                           |      1|none             |     0|acc        |â†‘  | 0.6368|Â±  |0.0323|
|  - management                                            |      1|none             |     0|acc        |â†‘  | 0.7961|Â±  |0.0399|
|  - marketing                                             |      1|none             |     0|acc        |â†‘  | 0.8718|Â±  |0.0219|
|  - medical_genetics                                      |      1|none             |     0|acc        |â†‘  | 0.7000|Â±  |0.0461|
|  - miscellaneous                                         |      1|none             |     0|acc        |â†‘  | 0.7816|Â±  |0.0148|
|  - nutrition                                             |      1|none             |     0|acc        |â†‘  | 0.6634|Â±  |0.0271|
|  - professional_accounting                               |      1|none             |     0|acc        |â†‘  | 0.4645|Â±  |0.0298|
|  - professional_medicine                                 |      1|none             |     0|acc        |â†‘  | 0.6654|Â±  |0.0287|
|  - virology                                              |      1|none             |     0|acc        |â†‘  | 0.5060|Â±  |0.0389|
| - social sciences                                        |      2|none             |      |acc        |â†‘  | 0.7000|Â±  |0.0080|
|  - econometrics                                          |      1|none             |     0|acc        |â†‘  | 0.4737|Â±  |0.0470|
|  - high_school_geography                                 |      1|none             |     0|acc        |â†‘  | 0.7525|Â±  |0.0307|
|  - high_school_government_and_politics                   |      1|none             |     0|acc        |â†‘  | 0.8705|Â±  |0.0242|
|  - high_school_macroeconomics                            |      1|none             |     0|acc        |â†‘  | 0.5795|Â±  |0.0250|
|  - high_school_microeconomics                            |      1|none             |     0|acc        |â†‘  | 0.6050|Â±  |0.0318|
|  - high_school_psychology                                |      1|none             |     0|acc        |â†‘  | 0.8092|Â±  |0.0168|
|  - human_sexuality                                       |      1|none             |     0|acc        |â†‘  | 0.6947|Â±  |0.0404|
|  - professional_psychology                               |      1|none             |     0|acc        |â†‘  | 0.6242|Â±  |0.0196|
|  - public_relations                                      |      1|none             |     0|acc        |â†‘  | 0.6455|Â±  |0.0458|
|  - security_studies                                      |      1|none             |     0|acc        |â†‘  | 0.7061|Â±  |0.0292|
|  - sociology                                             |      1|none             |     0|acc        |â†‘  | 0.8458|Â±  |0.0255|
|  - us_foreign_policy                                     |      1|none             |     0|acc        |â†‘  | 0.8500|Â±  |0.0359|
| - stem                                                   |      2|none             |      |acc        |â†‘  | 0.5052|Â±  |0.0086|
|  - abstract_algebra                                      |      1|none             |     0|acc        |â†‘  | 0.2700|Â±  |0.0446|
|  - anatomy                                               |      1|none             |     0|acc        |â†‘  | 0.5926|Â±  |0.0424|
|  - astronomy                                             |      1|none             |     0|acc        |â†‘  | 0.6382|Â±  |0.0391|
|  - college_biology                                       |      1|none             |     0|acc        |â†‘  | 0.7292|Â±  |0.0372|
|  - college_chemistry                                     |      1|none             |     0|acc        |â†‘  | 0.4800|Â±  |0.0502|
|  - college_computer_science                              |      1|none             |     0|acc        |â†‘  | 0.5000|Â±  |0.0503|
|  - college_mathematics                                   |      1|none             |     0|acc        |â†‘  | 0.3500|Â±  |0.0479|
|  - college_physics                                       |      1|none             |     0|acc        |â†‘  | 0.4608|Â±  |0.0496|
|  - computer_security                                     |      1|none             |     0|acc        |â†‘  | 0.6600|Â±  |0.0476|
|  - conceptual_physics                                    |      1|none             |     0|acc        |â†‘  | 0.5234|Â±  |0.0327|
|  - electrical_engineering                                |      1|none             |     0|acc        |â†‘  | 0.5655|Â±  |0.0413|
|  - elementary_mathematics                                |      1|none             |     0|acc        |â†‘  | 0.3704|Â±  |0.0249|
|  - high_school_biology                                   |      1|none             |     0|acc        |â†‘  | 0.7323|Â±  |0.0252|
|  - high_school_chemistry                                 |      1|none             |     0|acc        |â†‘  | 0.5123|Â±  |0.0352|
|  - high_school_computer_science                          |      1|none             |     0|acc        |â†‘  | 0.6400|Â±  |0.0482|
|  - high_school_mathematics                               |      1|none             |     0|acc        |â†‘  | 0.3370|Â±  |0.0288|
|  - high_school_physics                                   |      1|none             |     0|acc        |â†‘  | 0.2980|Â±  |0.0373|
|  - high_school_statistics                                |      1|none             |     0|acc        |â†‘  | 0.4676|Â±  |0.0340|
|  - machine_learning                                      |      1|none             |     0|acc        |â†‘  | 0.5446|Â±  |0.0473|
|nq_open                                                   |      4|remove_whitespace|     0|exact_match|â†‘  | 0.1537|Â±  |0.0060|
|openbookqa                                                |      1|none             |     0|acc        |â†‘  | 0.3540|Â±  |0.0214|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.4700|Â±  |0.0223|
|piqa                                                      |      1|none             |     0|acc        |â†‘  | 0.8156|Â±  |0.0090|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.8270|Â±  |0.0088|
|qnli                                                      |      1|none             |     0|acc        |â†‘  | 0.5146|Â±  |0.0068|
|sciq                                                      |      1|none             |     0|acc        |â†‘  | 0.9600|Â±  |0.0062|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.9430|Â±  |0.0073|
|triviaqa                                                  |      3|remove_whitespace|     0|exact_match|â†‘  | 0.5683|Â±  |0.0037|
|truthfulqa_gen                                            |      3|none             |     0|bleu_acc   |â†‘  | 0.5643|Â±  |0.0174|
|                                                          |       |none             |     0|bleu_diff  |â†‘  | 8.1688|Â±  |0.8460|
|                                                          |       |none             |     0|bleu_max   |â†‘  |27.6629|Â±  |0.8109|
|                                                          |       |none             |     0|rouge1_acc |â†‘  | 0.5716|Â±  |0.0173|
|                                                          |       |none             |     0|rouge1_diff|â†‘  |12.0899|Â±  |1.1860|
|                                                          |       |none             |     0|rouge1_max |â†‘  |54.8010|Â±  |0.8641|
|                                                          |       |none             |     0|rouge2_acc |â†‘  | 0.5202|Â±  |0.0175|
|                                                          |       |none             |     0|rouge2_diff|â†‘  |12.2282|Â±  |1.2604|
|                                                          |       |none             |     0|rouge2_max |â†‘  |41.2658|Â±  |1.0220|
|                                                          |       |none             |     0|rougeL_acc |â†‘  | 0.5692|Â±  |0.0173|
|                                                          |       |none             |     0|rougeL_diff|â†‘  |11.8949|Â±  |1.1929|
|                                                          |       |none             |     0|rougeL_max |â†‘  |51.6072|Â±  |0.8958|
|truthfulqa_mc1                                            |      2|none             |     0|acc        |â†‘  | 0.4211|Â±  |0.0173|
|truthfulqa_mc2                                            |      3|none             |     0|acc        |â†‘  | 0.5968|Â±  |0.0155|
|winogrande                                                |      1|none             |     0|acc        |â†‘  | 0.7403|Â±  |0.0123|

|      Groups      |Version|  Filter  |n-shot|  Metric   |   |Value |   |Stderr|
|------------------|------:|----------|------|-----------|---|-----:|---|-----:|
|bbh               |      3|get-answer|      |exact_match|â†‘  |0.5626|Â±  |0.0056|
|mmlu              |      2|none      |      |acc        |â†‘  |0.5971|Â±  |0.0039|
| - humanities     |      2|none      |      |acc        |â†‘  |0.5420|Â±  |0.0067|
| - other          |      2|none      |      |acc        |â†‘  |0.6720|Â±  |0.0081|
| - social sciences|      2|none      |      |acc        |â†‘  |0.7000|Â±  |0.0080|
| - stem           |      2|none      |      |acc        |â†‘  |0.5052|Â±  |0.0086|

mistralai_Mistral-7B-Instruct-v0.3: 8h 38m 15s
âœ… Benchmark completed for mistralai_Mistral-7B-Instruct-v0.3

ðŸ”¥ Starting benchmark for openchat_openchat-3.6-8b-20240522
ðŸ”¥ Starting benchmark for internlm_internlm2_5-7b-chat
ðŸ”¥ Starting benchmark for THUDM_chatglm3-6b
ðŸ”¥ Starting benchmark for NousResearch_Hermes-2-Pro-Mistral-7B
ðŸ”¥ Starting benchmark for Qwen_Qwen-1_8B-Chat
Qwen_Qwen-1_8B-Chat: 0h 18m 23s
âœ… Benchmark completed for Qwen_Qwen-1_8B-Chat

ðŸ”¥ Starting benchmark for Qwen_Qwen-1_8B
Qwen_Qwen-1_8B: 0h 18m 19s
âœ… Benchmark completed for Qwen_Qwen-1_8B

ðŸ”¥ Starting benchmark for deepseek-ai_DeepSeek-R1-Distill-Qwen-7B
hf (pretrained=/mnt/data8tb/Documents/llm/llm_models/deepseek-ai_DeepSeek-R1-Distill-Qwen-7B,trust_remote_code=True), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 3
|                          Tasks                           |Version|     Filter      |n-shot|  Metric   |   | Value |   |Stderr|
|----------------------------------------------------------|------:|-----------------|-----:|-----------|---|------:|---|-----:|
|anli_r1                                                   |      1|none             |     0|acc        |â†‘  | 0.4450|Â±  |0.0157|
|anli_r2                                                   |      1|none             |     0|acc        |â†‘  | 0.4180|Â±  |0.0156|
|anli_r3                                                   |      1|none             |     0|acc        |â†‘  | 0.4100|Â±  |0.0142|
|arc_challenge                                             |      1|none             |     0|acc        |â†‘  | 0.4215|Â±  |0.0144|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.4377|Â±  |0.0145|
|bbh                                                       |      3|get-answer       |      |exact_match|â†‘  | 0.5569|Â±  |0.0050|
| - bbh_cot_fewshot_boolean_expressions                    |      4|get-answer       |     3|exact_match|â†‘  | 0.9600|Â±  |0.0124|
| - bbh_cot_fewshot_causal_judgement                       |      4|get-answer       |     3|exact_match|â†‘  | 0.5187|Â±  |0.0366|
| - bbh_cot_fewshot_date_understanding                     |      4|get-answer       |     3|exact_match|â†‘  | 0.6760|Â±  |0.0297|
| - bbh_cot_fewshot_disambiguation_qa                      |      4|get-answer       |     3|exact_match|â†‘  | 0.5320|Â±  |0.0316|
| - bbh_cot_fewshot_dyck_languages                         |      4|get-answer       |     3|exact_match|â†‘  | 0.2640|Â±  |0.0279|
| - bbh_cot_fewshot_formal_fallacies                       |      4|get-answer       |     3|exact_match|â†‘  | 0.2720|Â±  |0.0282|
| - bbh_cot_fewshot_geometric_shapes                       |      4|get-answer       |     3|exact_match|â†‘  | 0.3480|Â±  |0.0302|
| - bbh_cot_fewshot_hyperbaton                             |      4|get-answer       |     3|exact_match|â†‘  | 0.8400|Â±  |0.0232|
| - bbh_cot_fewshot_logical_deduction_five_objects         |      4|get-answer       |     3|exact_match|â†‘  | 0.1800|Â±  |0.0243|
| - bbh_cot_fewshot_logical_deduction_seven_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.0200|Â±  |0.0089|
| - bbh_cot_fewshot_logical_deduction_three_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.8040|Â±  |0.0252|
| - bbh_cot_fewshot_movie_recommendation                   |      4|get-answer       |     3|exact_match|â†‘  | 0.5000|Â±  |0.0317|
| - bbh_cot_fewshot_multistep_arithmetic_two               |      4|get-answer       |     3|exact_match|â†‘  | 0.9520|Â±  |0.0135|
| - bbh_cot_fewshot_navigate                               |      4|get-answer       |     3|exact_match|â†‘  | 0.9040|Â±  |0.0187|
| - bbh_cot_fewshot_object_counting                        |      4|get-answer       |     3|exact_match|â†‘  | 0.8320|Â±  |0.0237|
| - bbh_cot_fewshot_penguins_in_a_table                    |      4|get-answer       |     3|exact_match|â†‘  | 0.7603|Â±  |0.0355|
| - bbh_cot_fewshot_reasoning_about_colored_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.6960|Â±  |0.0292|
| - bbh_cot_fewshot_ruin_names                             |      4|get-answer       |     3|exact_match|â†‘  | 0.5120|Â±  |0.0317|
| - bbh_cot_fewshot_salient_translation_error_detection    |      4|get-answer       |     3|exact_match|â†‘  | 0.3760|Â±  |0.0307|
| - bbh_cot_fewshot_snarks                                 |      4|get-answer       |     3|exact_match|â†‘  | 0.5506|Â±  |0.0374|
| - bbh_cot_fewshot_sports_understanding                   |      4|get-answer       |     3|exact_match|â†‘  | 0.7480|Â±  |0.0275|
| - bbh_cot_fewshot_temporal_sequences                     |      4|get-answer       |     3|exact_match|â†‘  | 0.3760|Â±  |0.0307|
| - bbh_cot_fewshot_tracking_shuffled_objects_five_objects |      4|get-answer       |     3|exact_match|â†‘  | 0.4360|Â±  |0.0314|
| - bbh_cot_fewshot_tracking_shuffled_objects_seven_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.0040|Â±  |0.0040|
| - bbh_cot_fewshot_tracking_shuffled_objects_three_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.8080|Â±  |0.0250|
| - bbh_cot_fewshot_web_of_lies                            |      4|get-answer       |     3|exact_match|â†‘  | 1.0000|Â±  |0.0000|
| - bbh_cot_fewshot_word_sorting                           |      4|get-answer       |     3|exact_match|â†‘  | 0.2400|Â±  |0.0271|
|boolq                                                     |      2|none             |     0|acc        |â†‘  | 0.7783|Â±  |0.0073|
|drop                                                      |      3|none             |     0|em         |â†‘  | 0.0023|Â±  |0.0005|
|                                                          |       |none             |     0|f1         |â†‘  | 0.0412|Â±  |0.0011|
|gpqa_diamond_cot_n_shot                                   |      2|flexible-extract |     0|exact_match|â†‘  | 0.0758|Â±  |0.0189|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_cot_zeroshot                                 |      1|flexible-extract |     0|exact_match|â†‘  | 0.0404|Â±  |0.0140|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_generative_n_shot                            |      2|flexible-extract |     0|exact_match|â†‘  | 0.1667|Â±  |0.0266|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_n_shot                                       |      2|none             |     0|acc        |â†‘  | 0.2677|Â±  |0.0315|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2677|Â±  |0.0315|
|gpqa_diamond_zeroshot                                     |      1|none             |     0|acc        |â†‘  | 0.2778|Â±  |0.0319|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2778|Â±  |0.0319|
|gpqa_extended_cot_n_shot                                  |      2|flexible-extract |     0|exact_match|â†‘  | 0.1007|Â±  |0.0129|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_cot_zeroshot                                |      1|flexible-extract |     0|exact_match|â†‘  | 0.0934|Â±  |0.0125|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_generative_n_shot                           |      2|flexible-extract |     0|exact_match|â†‘  | 0.1758|Â±  |0.0163|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_n_shot                                      |      2|none             |     0|acc        |â†‘  | 0.2839|Â±  |0.0193|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2839|Â±  |0.0193|
|gpqa_extended_zeroshot                                    |      1|none             |     0|acc        |â†‘  | 0.3480|Â±  |0.0204|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3480|Â±  |0.0204|
|gpqa_main_cot_n_shot                                      |      2|flexible-extract |     0|exact_match|â†‘  | 0.0848|Â±  |0.0132|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_cot_zeroshot                                    |      1|flexible-extract |     0|exact_match|â†‘  | 0.0759|Â±  |0.0125|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_generative_n_shot                               |      2|flexible-extract |     0|exact_match|â†‘  | 0.1473|Â±  |0.0168|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_n_shot                                          |      2|none             |     0|acc        |â†‘  | 0.2634|Â±  |0.0208|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2634|Â±  |0.0208|
|gpqa_main_zeroshot                                        |      1|none             |     0|acc        |â†‘  | 0.3348|Â±  |0.0223|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3348|Â±  |0.0223|
|gsm8k                                                     |      3|flexible-extract |     5|exact_match|â†‘  | 0.7998|Â±  |0.0110|
|                                                          |       |strict-match     |     5|exact_match|â†‘  | 0.7862|Â±  |0.0113|
|hellaswag                                                 |      1|none             |     0|acc        |â†‘  | 0.4627|Â±  |0.0050|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.6026|Â±  |0.0049|
|mmlu                                                      |      2|none             |      |acc        |â†‘  | 0.5263|Â±  |0.0041|
| - humanities                                             |      2|none             |      |acc        |â†‘  | 0.4406|Â±  |0.0070|
|  - formal_logic                                          |      1|none             |     0|acc        |â†‘  | 0.4921|Â±  |0.0447|
|  - high_school_european_history                          |      1|none             |     0|acc        |â†‘  | 0.6121|Â±  |0.0380|
|  - high_school_us_history                                |      1|none             |     0|acc        |â†‘  | 0.6176|Â±  |0.0341|
|  - high_school_world_history                             |      1|none             |     0|acc        |â†‘  | 0.6203|Â±  |0.0316|
|  - international_law                                     |      1|none             |     0|acc        |â†‘  | 0.6116|Â±  |0.0445|
|  - jurisprudence                                         |      1|none             |     0|acc        |â†‘  | 0.6481|Â±  |0.0462|
|  - logical_fallacies                                     |      1|none             |     0|acc        |â†‘  | 0.6319|Â±  |0.0379|
|  - moral_disputes                                        |      1|none             |     0|acc        |â†‘  | 0.5578|Â±  |0.0267|
|  - moral_scenarios                                       |      1|none             |     0|acc        |â†‘  | 0.2581|Â±  |0.0146|
|  - philosophy                                            |      1|none             |     0|acc        |â†‘  | 0.5498|Â±  |0.0283|
|  - prehistory                                            |      1|none             |     0|acc        |â†‘  | 0.4599|Â±  |0.0277|
|  - professional_law                                      |      1|none             |     0|acc        |â†‘  | 0.3585|Â±  |0.0122|
|  - world_religions                                       |      1|none             |     0|acc        |â†‘  | 0.5614|Â±  |0.0381|
| - other                                                  |      2|none             |      |acc        |â†‘  | 0.5391|Â±  |0.0087|
|  - business_ethics                                       |      1|none             |     0|acc        |â†‘  | 0.5700|Â±  |0.0498|
|  - clinical_knowledge                                    |      1|none             |     0|acc        |â†‘  | 0.5358|Â±  |0.0307|
|  - college_medicine                                      |      1|none             |     0|acc        |â†‘  | 0.4971|Â±  |0.0381|
|  - global_facts                                          |      1|none             |     0|acc        |â†‘  | 0.3400|Â±  |0.0476|
|  - human_aging                                           |      1|none             |     0|acc        |â†‘  | 0.5426|Â±  |0.0334|
|  - management                                            |      1|none             |     0|acc        |â†‘  | 0.6796|Â±  |0.0462|
|  - marketing                                             |      1|none             |     0|acc        |â†‘  | 0.7479|Â±  |0.0284|
|  - medical_genetics                                      |      1|none             |     0|acc        |â†‘  | 0.5300|Â±  |0.0502|
|  - miscellaneous                                         |      1|none             |     0|acc        |â†‘  | 0.6309|Â±  |0.0173|
|  - nutrition                                             |      1|none             |     0|acc        |â†‘  | 0.5359|Â±  |0.0286|
|  - professional_accounting                               |      1|none             |     0|acc        |â†‘  | 0.3936|Â±  |0.0291|
|  - professional_medicine                                 |      1|none             |     0|acc        |â†‘  | 0.3713|Â±  |0.0293|
|  - virology                                              |      1|none             |     0|acc        |â†‘  | 0.4036|Â±  |0.0382|
| - social sciences                                        |      2|none             |      |acc        |â†‘  | 0.6123|Â±  |0.0087|
|  - econometrics                                          |      1|none             |     0|acc        |â†‘  | 0.5702|Â±  |0.0466|
|  - high_school_geography                                 |      1|none             |     0|acc        |â†‘  | 0.6212|Â±  |0.0346|
|  - high_school_government_and_politics                   |      1|none             |     0|acc        |â†‘  | 0.6839|Â±  |0.0336|
|  - high_school_macroeconomics                            |      1|none             |     0|acc        |â†‘  | 0.5974|Â±  |0.0249|
|  - high_school_microeconomics                            |      1|none             |     0|acc        |â†‘  | 0.7185|Â±  |0.0292|
|  - high_school_psychology                                |      1|none             |     0|acc        |â†‘  | 0.6550|Â±  |0.0204|
|  - human_sexuality                                       |      1|none             |     0|acc        |â†‘  | 0.5420|Â±  |0.0437|
|  - professional_psychology                               |      1|none             |     0|acc        |â†‘  | 0.5033|Â±  |0.0202|
|  - public_relations                                      |      1|none             |     0|acc        |â†‘  | 0.5818|Â±  |0.0472|
|  - security_studies                                      |      1|none             |     0|acc        |â†‘  | 0.5918|Â±  |0.0315|
|  - sociology                                             |      1|none             |     0|acc        |â†‘  | 0.7214|Â±  |0.0317|
|  - us_foreign_policy                                     |      1|none             |     0|acc        |â†‘  | 0.7000|Â±  |0.0461|
| - stem                                                   |      2|none             |      |acc        |â†‘  | 0.5579|Â±  |0.0087|
|  - abstract_algebra                                      |      1|none             |     0|acc        |â†‘  | 0.4600|Â±  |0.0501|
|  - anatomy                                               |      1|none             |     0|acc        |â†‘  | 0.4444|Â±  |0.0429|
|  - astronomy                                             |      1|none             |     0|acc        |â†‘  | 0.5855|Â±  |0.0401|
|  - college_biology                                       |      1|none             |     0|acc        |â†‘  | 0.5208|Â±  |0.0418|
|  - college_chemistry                                     |      1|none             |     0|acc        |â†‘  | 0.4700|Â±  |0.0502|
|  - college_computer_science                              |      1|none             |     0|acc        |â†‘  | 0.5800|Â±  |0.0496|
|  - college_mathematics                                   |      1|none             |     0|acc        |â†‘  | 0.4300|Â±  |0.0498|
|  - college_physics                                       |      1|none             |     0|acc        |â†‘  | 0.4020|Â±  |0.0488|
|  - computer_security                                     |      1|none             |     0|acc        |â†‘  | 0.6900|Â±  |0.0465|
|  - conceptual_physics                                    |      1|none             |     0|acc        |â†‘  | 0.6936|Â±  |0.0301|
|  - electrical_engineering                                |      1|none             |     0|acc        |â†‘  | 0.5931|Â±  |0.0409|
|  - elementary_mathematics                                |      1|none             |     0|acc        |â†‘  | 0.6058|Â±  |0.0252|
|  - high_school_biology                                   |      1|none             |     0|acc        |â†‘  | 0.6323|Â±  |0.0274|
|  - high_school_chemistry                                 |      1|none             |     0|acc        |â†‘  | 0.4975|Â±  |0.0352|
|  - high_school_computer_science                          |      1|none             |     0|acc        |â†‘  | 0.7300|Â±  |0.0446|
|  - high_school_mathematics                               |      1|none             |     0|acc        |â†‘  | 0.4556|Â±  |0.0304|
|  - high_school_physics                                   |      1|none             |     0|acc        |â†‘  | 0.4901|Â±  |0.0408|
|  - high_school_statistics                                |      1|none             |     0|acc        |â†‘  | 0.6389|Â±  |0.0328|
|  - machine_learning                                      |      1|none             |     0|acc        |â†‘  | 0.4286|Â±  |0.0470|
|nq_open                                                   |      4|remove_whitespace|     0|exact_match|â†‘  | 0.0321|Â±  |0.0029|
|openbookqa                                                |      1|none             |     0|acc        |â†‘  | 0.2620|Â±  |0.0197|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3600|Â±  |0.0215|
|piqa                                                      |      1|none             |     0|acc        |â†‘  | 0.7067|Â±  |0.0106|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.7165|Â±  |0.0105|
|qnli                                                      |      1|none             |     0|acc        |â†‘  | 0.5210|Â±  |0.0068|
|sciq                                                      |      1|none             |     0|acc        |â†‘  | 0.9360|Â±  |0.0077|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.9180|Â±  |0.0087|
|triviaqa                                                  |      3|remove_whitespace|     0|exact_match|â†‘  | 0.0592|Â±  |0.0018|
|truthfulqa_gen                                            |      3|none             |     0|bleu_acc   |â†‘  | 0.3843|Â±  |0.0170|
|                                                          |       |none             |     0|bleu_diff  |â†‘  |-0.3986|Â±  |0.4145|
|                                                          |       |none             |     0|bleu_max   |â†‘  |12.1556|Â±  |0.4381|
|                                                          |       |none             |     0|rouge1_acc |â†‘  | 0.4076|Â±  |0.0172|
|                                                          |       |none             |     0|rouge1_diff|â†‘  |-0.5348|Â±  |0.7118|
|                                                          |       |none             |     0|rouge1_max |â†‘  |34.2455|Â±  |0.7048|
|                                                          |       |none             |     0|rouge2_acc |â†‘  | 0.2925|Â±  |0.0159|
|                                                          |       |none             |     0|rouge2_diff|â†‘  |-1.4234|Â±  |0.7703|
|                                                          |       |none             |     0|rouge2_max |â†‘  |20.4502|Â±  |0.7267|
|                                                          |       |none             |     0|rougeL_acc |â†‘  | 0.3978|Â±  |0.0171|
|                                                          |       |none             |     0|rougeL_diff|â†‘  |-0.5833|Â±  |0.7113|
|                                                          |       |none             |     0|rougeL_max |â†‘  |32.3124|Â±  |0.7036|
|truthfulqa_mc1                                            |      2|none             |     0|acc        |â†‘  | 0.2889|Â±  |0.0159|
|truthfulqa_mc2                                            |      3|none             |     0|acc        |â†‘  | 0.4563|Â±  |0.0154|
|winogrande                                                |      1|none             |     0|acc        |â†‘  | 0.5991|Â±  |0.0138|

|      Groups      |Version|  Filter  |n-shot|  Metric   |   |Value |   |Stderr|
|------------------|------:|----------|------|-----------|---|-----:|---|-----:|
|bbh               |      3|get-answer|      |exact_match|â†‘  |0.5569|Â±  |0.0050|
|mmlu              |      2|none      |      |acc        |â†‘  |0.5263|Â±  |0.0041|
| - humanities     |      2|none      |      |acc        |â†‘  |0.4406|Â±  |0.0070|
| - other          |      2|none      |      |acc        |â†‘  |0.5391|Â±  |0.0087|
| - social sciences|      2|none      |      |acc        |â†‘  |0.6123|Â±  |0.0087|
| - stem           |      2|none      |      |acc        |â†‘  |0.5579|Â±  |0.0087|

deepseek-ai_DeepSeek-R1-Distill-Qwen-7B: 6h 28m 41s
âœ… Benchmark completed for deepseek-ai_DeepSeek-R1-Distill-Qwen-7B

ðŸ”¥ Starting benchmark for deepseek-ai_deepseek-math-7b-rl
hf (pretrained=/mnt/data8tb/Documents/llm/llm_models/deepseek-ai_deepseek-math-7b-rl,trust_remote_code=True), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 3
|                          Tasks                           |Version|     Filter      |n-shot|  Metric   |   | Value |   |Stderr|
|----------------------------------------------------------|------:|-----------------|-----:|-----------|---|------:|---|-----:|
|anli_r1                                                   |      1|none             |     0|acc        |â†‘  | 0.3680|Â±  |0.0153|
|anli_r2                                                   |      1|none             |     0|acc        |â†‘  | 0.3890|Â±  |0.0154|
|anli_r3                                                   |      1|none             |     0|acc        |â†‘  | 0.4050|Â±  |0.0142|
|arc_challenge                                             |      1|none             |     0|acc        |â†‘  | 0.4795|Â±  |0.0146|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.4898|Â±  |0.0146|
|bbh                                                       |      3|get-answer       |      |exact_match|â†‘  | 0.5247|Â±  |0.0054|
| - bbh_cot_fewshot_boolean_expressions                    |      4|get-answer       |     3|exact_match|â†‘  | 0.9040|Â±  |0.0187|
| - bbh_cot_fewshot_causal_judgement                       |      4|get-answer       |     3|exact_match|â†‘  | 0.4439|Â±  |0.0364|
| - bbh_cot_fewshot_date_understanding                     |      4|get-answer       |     3|exact_match|â†‘  | 0.5240|Â±  |0.0316|
| - bbh_cot_fewshot_disambiguation_qa                      |      4|get-answer       |     3|exact_match|â†‘  | 0.5200|Â±  |0.0317|
| - bbh_cot_fewshot_dyck_languages                         |      4|get-answer       |     3|exact_match|â†‘  | 0.1840|Â±  |0.0246|
| - bbh_cot_fewshot_formal_fallacies                       |      4|get-answer       |     3|exact_match|â†‘  | 0.1680|Â±  |0.0237|
| - bbh_cot_fewshot_geometric_shapes                       |      4|get-answer       |     3|exact_match|â†‘  | 0.3200|Â±  |0.0296|
| - bbh_cot_fewshot_hyperbaton                             |      4|get-answer       |     3|exact_match|â†‘  | 0.7040|Â±  |0.0289|
| - bbh_cot_fewshot_logical_deduction_five_objects         |      4|get-answer       |     3|exact_match|â†‘  | 0.1200|Â±  |0.0206|
| - bbh_cot_fewshot_logical_deduction_seven_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.0600|Â±  |0.0151|
| - bbh_cot_fewshot_logical_deduction_three_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.8080|Â±  |0.0250|
| - bbh_cot_fewshot_movie_recommendation                   |      4|get-answer       |     3|exact_match|â†‘  | 0.8040|Â±  |0.0252|
| - bbh_cot_fewshot_multistep_arithmetic_two               |      4|get-answer       |     3|exact_match|â†‘  | 0.7880|Â±  |0.0259|
| - bbh_cot_fewshot_navigate                               |      4|get-answer       |     3|exact_match|â†‘  | 0.8960|Â±  |0.0193|
| - bbh_cot_fewshot_object_counting                        |      4|get-answer       |     3|exact_match|â†‘  | 0.6720|Â±  |0.0298|
| - bbh_cot_fewshot_penguins_in_a_table                    |      4|get-answer       |     3|exact_match|â†‘  | 0.3219|Â±  |0.0388|
| - bbh_cot_fewshot_reasoning_about_colored_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.5000|Â±  |0.0317|
| - bbh_cot_fewshot_ruin_names                             |      4|get-answer       |     3|exact_match|â†‘  | 0.5360|Â±  |0.0316|
| - bbh_cot_fewshot_salient_translation_error_detection    |      4|get-answer       |     3|exact_match|â†‘  | 0.4760|Â±  |0.0316|
| - bbh_cot_fewshot_snarks                                 |      4|get-answer       |     3|exact_match|â†‘  | 0.4888|Â±  |0.0376|
| - bbh_cot_fewshot_sports_understanding                   |      4|get-answer       |     3|exact_match|â†‘  | 0.7000|Â±  |0.0290|
| - bbh_cot_fewshot_temporal_sequences                     |      4|get-answer       |     3|exact_match|â†‘  | 0.3040|Â±  |0.0292|
| - bbh_cot_fewshot_tracking_shuffled_objects_five_objects |      4|get-answer       |     3|exact_match|â†‘  | 0.5360|Â±  |0.0316|
| - bbh_cot_fewshot_tracking_shuffled_objects_seven_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.4480|Â±  |0.0315|
| - bbh_cot_fewshot_tracking_shuffled_objects_three_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.7640|Â±  |0.0269|
| - bbh_cot_fewshot_web_of_lies                            |      4|get-answer       |     3|exact_match|â†‘  | 0.7440|Â±  |0.0277|
| - bbh_cot_fewshot_word_sorting                           |      4|get-answer       |     3|exact_match|â†‘  | 0.3160|Â±  |0.0295|
|boolq                                                     |      2|none             |     0|acc        |â†‘  | 0.7560|Â±  |0.0075|
|drop                                                      |      3|none             |     0|em         |â†‘  | 0.0166|Â±  |0.0013|
|                                                          |       |none             |     0|f1         |â†‘  | 0.1190|Â±  |0.0021|
|gpqa_diamond_cot_n_shot                                   |      2|flexible-extract |     0|exact_match|â†‘  | 0.2222|Â±  |0.0296|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0051|Â±  |0.0051|
|gpqa_diamond_cot_zeroshot                                 |      1|flexible-extract |     0|exact_match|â†‘  | 0.2222|Â±  |0.0296|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_generative_n_shot                            |      2|flexible-extract |     0|exact_match|â†‘  | 0.1667|Â±  |0.0266|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0101|Â±  |0.0071|
|gpqa_diamond_n_shot                                       |      2|none             |     0|acc        |â†‘  | 0.3131|Â±  |0.0330|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3131|Â±  |0.0330|
|gpqa_diamond_zeroshot                                     |      1|none             |     0|acc        |â†‘  | 0.2879|Â±  |0.0323|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2879|Â±  |0.0323|
|gpqa_extended_cot_n_shot                                  |      2|flexible-extract |     0|exact_match|â†‘  | 0.2234|Â±  |0.0178|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0018|Â±  |0.0018|
|gpqa_extended_cot_zeroshot                                |      1|flexible-extract |     0|exact_match|â†‘  | 0.1960|Â±  |0.0170|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0110|Â±  |0.0045|
|gpqa_extended_generative_n_shot                           |      2|flexible-extract |     0|exact_match|â†‘  | 0.1502|Â±  |0.0153|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0037|Â±  |0.0026|
|gpqa_extended_n_shot                                      |      2|none             |     0|acc        |â†‘  | 0.2857|Â±  |0.0194|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2857|Â±  |0.0194|
|gpqa_extended_zeroshot                                    |      1|none             |     0|acc        |â†‘  | 0.2949|Â±  |0.0195|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2949|Â±  |0.0195|
|gpqa_main_cot_n_shot                                      |      2|flexible-extract |     0|exact_match|â†‘  | 0.1786|Â±  |0.0181|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0022|Â±  |0.0022|
|gpqa_main_cot_zeroshot                                    |      1|flexible-extract |     0|exact_match|â†‘  | 0.1964|Â±  |0.0188|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0045|Â±  |0.0032|
|gpqa_main_generative_n_shot                               |      2|flexible-extract |     0|exact_match|â†‘  | 0.1897|Â±  |0.0185|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0089|Â±  |0.0044|
|gpqa_main_n_shot                                          |      2|none             |     0|acc        |â†‘  | 0.3036|Â±  |0.0217|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3036|Â±  |0.0217|
|gpqa_main_zeroshot                                        |      1|none             |     0|acc        |â†‘  | 0.2723|Â±  |0.0211|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2723|Â±  |0.0211|
|gsm8k                                                     |      3|flexible-extract |     5|exact_match|â†‘  | 0.1865|Â±  |0.0107|
|                                                          |       |strict-match     |     5|exact_match|â†‘  | 0.1425|Â±  |0.0096|
|hellaswag                                                 |      1|none             |     0|acc        |â†‘  | 0.5293|Â±  |0.0050|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.6896|Â±  |0.0046|
|mmlu                                                      |      2|none             |      |acc        |â†‘  | 0.5250|Â±  |0.0041|
| - humanities                                             |      2|none             |      |acc        |â†‘  | 0.4417|Â±  |0.0070|
|  - formal_logic                                          |      1|none             |     0|acc        |â†‘  | 0.5000|Â±  |0.0447|
|  - high_school_european_history                          |      1|none             |     0|acc        |â†‘  | 0.6424|Â±  |0.0374|
|  - high_school_us_history                                |      1|none             |     0|acc        |â†‘  | 0.5294|Â±  |0.0350|
|  - high_school_world_history                             |      1|none             |     0|acc        |â†‘  | 0.6667|Â±  |0.0307|
|  - international_law                                     |      1|none             |     0|acc        |â†‘  | 0.6116|Â±  |0.0445|
|  - jurisprudence                                         |      1|none             |     0|acc        |â†‘  | 0.5463|Â±  |0.0481|
|  - logical_fallacies                                     |      1|none             |     0|acc        |â†‘  | 0.6442|Â±  |0.0376|
|  - moral_disputes                                        |      1|none             |     0|acc        |â†‘  | 0.5376|Â±  |0.0268|
|  - moral_scenarios                                       |      1|none             |     0|acc        |â†‘  | 0.2492|Â±  |0.0145|
|  - philosophy                                            |      1|none             |     0|acc        |â†‘  | 0.5466|Â±  |0.0283|
|  - prehistory                                            |      1|none             |     0|acc        |â†‘  | 0.4877|Â±  |0.0278|
|  - professional_law                                      |      1|none             |     0|acc        |â†‘  | 0.3722|Â±  |0.0123|
|  - world_religions                                       |      1|none             |     0|acc        |â†‘  | 0.5673|Â±  |0.0380|
| - other                                                  |      2|none             |      |acc        |â†‘  | 0.5552|Â±  |0.0087|
|  - business_ethics                                       |      1|none             |     0|acc        |â†‘  | 0.5200|Â±  |0.0502|
|  - clinical_knowledge                                    |      1|none             |     0|acc        |â†‘  | 0.5396|Â±  |0.0307|
|  - college_medicine                                      |      1|none             |     0|acc        |â†‘  | 0.5549|Â±  |0.0379|
|  - global_facts                                          |      1|none             |     0|acc        |â†‘  | 0.2900|Â±  |0.0456|
|  - human_aging                                           |      1|none             |     0|acc        |â†‘  | 0.5471|Â±  |0.0334|
|  - management                                            |      1|none             |     0|acc        |â†‘  | 0.6990|Â±  |0.0454|
|  - marketing                                             |      1|none             |     0|acc        |â†‘  | 0.7735|Â±  |0.0274|
|  - medical_genetics                                      |      1|none             |     0|acc        |â†‘  | 0.5900|Â±  |0.0494|
|  - miscellaneous                                         |      1|none             |     0|acc        |â†‘  | 0.6424|Â±  |0.0171|
|  - nutrition                                             |      1|none             |     0|acc        |â†‘  | 0.5490|Â±  |0.0285|
|  - professional_accounting                               |      1|none             |     0|acc        |â†‘  | 0.3901|Â±  |0.0291|
|  - professional_medicine                                 |      1|none             |     0|acc        |â†‘  | 0.4669|Â±  |0.0303|
|  - virology                                              |      1|none             |     0|acc        |â†‘  | 0.3795|Â±  |0.0378|
| - social sciences                                        |      2|none             |      |acc        |â†‘  | 0.6107|Â±  |0.0087|
|  - econometrics                                          |      1|none             |     0|acc        |â†‘  | 0.4825|Â±  |0.0470|
|  - high_school_geography                                 |      1|none             |     0|acc        |â†‘  | 0.6616|Â±  |0.0337|
|  - high_school_government_and_politics                   |      1|none             |     0|acc        |â†‘  | 0.6632|Â±  |0.0341|
|  - high_school_macroeconomics                            |      1|none             |     0|acc        |â†‘  | 0.5718|Â±  |0.0251|
|  - high_school_microeconomics                            |      1|none             |     0|acc        |â†‘  | 0.6639|Â±  |0.0307|
|  - high_school_psychology                                |      1|none             |     0|acc        |â†‘  | 0.6899|Â±  |0.0198|
|  - human_sexuality                                       |      1|none             |     0|acc        |â†‘  | 0.5878|Â±  |0.0432|
|  - professional_psychology                               |      1|none             |     0|acc        |â†‘  | 0.4902|Â±  |0.0202|
|  - public_relations                                      |      1|none             |     0|acc        |â†‘  | 0.5909|Â±  |0.0471|
|  - security_studies                                      |      1|none             |     0|acc        |â†‘  | 0.5918|Â±  |0.0315|
|  - sociology                                             |      1|none             |     0|acc        |â†‘  | 0.7512|Â±  |0.0306|
|  - us_foreign_policy                                     |      1|none             |     0|acc        |â†‘  | 0.7000|Â±  |0.0461|
| - stem                                                   |      2|none             |      |acc        |â†‘  | 0.5360|Â±  |0.0088|
|  - abstract_algebra                                      |      1|none             |     0|acc        |â†‘  | 0.4000|Â±  |0.0492|
|  - anatomy                                               |      1|none             |     0|acc        |â†‘  | 0.4667|Â±  |0.0431|
|  - astronomy                                             |      1|none             |     0|acc        |â†‘  | 0.6382|Â±  |0.0391|
|  - college_biology                                       |      1|none             |     0|acc        |â†‘  | 0.6042|Â±  |0.0409|
|  - college_chemistry                                     |      1|none             |     0|acc        |â†‘  | 0.4700|Â±  |0.0502|
|  - college_computer_science                              |      1|none             |     0|acc        |â†‘  | 0.5100|Â±  |0.0502|
|  - college_mathematics                                   |      1|none             |     0|acc        |â†‘  | 0.4200|Â±  |0.0496|
|  - college_physics                                       |      1|none             |     0|acc        |â†‘  | 0.4020|Â±  |0.0488|
|  - computer_security                                     |      1|none             |     0|acc        |â†‘  | 0.6500|Â±  |0.0479|
|  - conceptual_physics                                    |      1|none             |     0|acc        |â†‘  | 0.5702|Â±  |0.0324|
|  - electrical_engineering                                |      1|none             |     0|acc        |â†‘  | 0.6069|Â±  |0.0407|
|  - elementary_mathematics                                |      1|none             |     0|acc        |â†‘  | 0.5608|Â±  |0.0256|
|  - high_school_biology                                   |      1|none             |     0|acc        |â†‘  | 0.6387|Â±  |0.0273|
|  - high_school_chemistry                                 |      1|none             |     0|acc        |â†‘  | 0.5172|Â±  |0.0352|
|  - high_school_computer_science                          |      1|none             |     0|acc        |â†‘  | 0.6900|Â±  |0.0465|
|  - high_school_mathematics                               |      1|none             |     0|acc        |â†‘  | 0.4185|Â±  |0.0301|
|  - high_school_physics                                   |      1|none             |     0|acc        |â†‘  | 0.3709|Â±  |0.0394|
|  - high_school_statistics                                |      1|none             |     0|acc        |â†‘  | 0.5741|Â±  |0.0337|
|  - machine_learning                                      |      1|none             |     0|acc        |â†‘  | 0.5179|Â±  |0.0474|
|nq_open                                                   |      4|remove_whitespace|     0|exact_match|â†‘  | 0.0393|Â±  |0.0032|
|openbookqa                                                |      1|none             |     0|acc        |â†‘  | 0.3280|Â±  |0.0210|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.4240|Â±  |0.0221|
|piqa                                                      |      1|none             |     0|acc        |â†‘  | 0.7410|Â±  |0.0102|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.7503|Â±  |0.0101|
|qnli                                                      |      1|none             |     0|acc        |â†‘  | 0.4990|Â±  |0.0068|
|sciq                                                      |      1|none             |     0|acc        |â†‘  | 0.9540|Â±  |0.0066|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.9280|Â±  |0.0082|
|triviaqa                                                  |      3|remove_whitespace|     0|exact_match|â†‘  | 0.1747|Â±  |0.0028|
|truthfulqa_gen                                            |      3|none             |     0|bleu_acc   |â†‘  | 0.3452|Â±  |0.0166|
|                                                          |       |none             |     0|bleu_diff  |â†‘  |-4.1463|Â±  |0.6373|
|                                                          |       |none             |     0|bleu_max   |â†‘  |21.0959|Â±  |0.7171|
|                                                          |       |none             |     0|rouge1_acc |â†‘  | 0.3256|Â±  |0.0164|
|                                                          |       |none             |     0|rouge1_diff|â†‘  |-6.5427|Â±  |0.7175|
|                                                          |       |none             |     0|rouge1_max |â†‘  |44.3011|Â±  |0.8363|
|                                                          |       |none             |     0|rouge2_acc |â†‘  | 0.2815|Â±  |0.0157|
|                                                          |       |none             |     0|rouge2_diff|â†‘  |-6.6740|Â±  |0.8391|
|                                                          |       |none             |     0|rouge2_max |â†‘  |29.9891|Â±  |0.9108|
|                                                          |       |none             |     0|rougeL_acc |â†‘  | 0.3097|Â±  |0.0162|
|                                                          |       |none             |     0|rougeL_diff|â†‘  |-6.8840|Â±  |0.7254|
|                                                          |       |none             |     0|rougeL_max |â†‘  |41.2479|Â±  |0.8408|
|truthfulqa_mc1                                            |      2|none             |     0|acc        |â†‘  | 0.2876|Â±  |0.0158|
|truthfulqa_mc2                                            |      3|none             |     0|acc        |â†‘  | 0.4029|Â±  |0.0153|
|winogrande                                                |      1|none             |     0|acc        |â†‘  | 0.6511|Â±  |0.0134|

|      Groups      |Version|  Filter  |n-shot|  Metric   |   |Value |   |Stderr|
|------------------|------:|----------|------|-----------|---|-----:|---|-----:|
|bbh               |      3|get-answer|      |exact_match|â†‘  |0.5247|Â±  |0.0054|
|mmlu              |      2|none      |      |acc        |â†‘  |0.5250|Â±  |0.0041|
| - humanities     |      2|none      |      |acc        |â†‘  |0.4417|Â±  |0.0070|
| - other          |      2|none      |      |acc        |â†‘  |0.5552|Â±  |0.0087|
| - social sciences|      2|none      |      |acc        |â†‘  |0.6107|Â±  |0.0087|
| - stem           |      2|none      |      |acc        |â†‘  |0.5360|Â±  |0.0088|

deepseek-ai_deepseek-math-7b-rl: 8h 2m 14s
âœ… Benchmark completed for deepseek-ai_deepseek-math-7b-rl

ðŸ”¥ Starting benchmark for deepseek-ai_deepseek-llm-7b-chat
hf (pretrained=/mnt/data8tb/Documents/llm/llm_models/deepseek-ai_deepseek-llm-7b-chat,trust_remote_code=True), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 3
|                          Tasks                           |Version|     Filter      |n-shot|  Metric   |   | Value |   |Stderr|
|----------------------------------------------------------|------:|-----------------|-----:|-----------|---|------:|---|-----:|
|anli_r1                                                   |      1|none             |     0|acc        |â†‘  | 0.4230|Â±  |0.0156|
|anli_r2                                                   |      1|none             |     0|acc        |â†‘  | 0.4190|Â±  |0.0156|
|anli_r3                                                   |      1|none             |     0|acc        |â†‘  | 0.4208|Â±  |0.0143|
|arc_challenge                                             |      1|none             |     0|acc        |â†‘  | 0.4812|Â±  |0.0146|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.4966|Â±  |0.0146|
|bbh                                                       |      3|get-answer       |      |exact_match|â†‘  | 0.4548|Â±  |0.0054|
| - bbh_cot_fewshot_boolean_expressions                    |      4|get-answer       |     3|exact_match|â†‘  | 0.7320|Â±  |0.0281|
| - bbh_cot_fewshot_causal_judgement                       |      4|get-answer       |     3|exact_match|â†‘  | 0.5615|Â±  |0.0364|
| - bbh_cot_fewshot_date_understanding                     |      4|get-answer       |     3|exact_match|â†‘  | 0.5760|Â±  |0.0313|
| - bbh_cot_fewshot_disambiguation_qa                      |      4|get-answer       |     3|exact_match|â†‘  | 0.5800|Â±  |0.0313|
| - bbh_cot_fewshot_dyck_languages                         |      4|get-answer       |     3|exact_match|â†‘  | 0.0160|Â±  |0.0080|
| - bbh_cot_fewshot_formal_fallacies                       |      4|get-answer       |     3|exact_match|â†‘  | 0.5160|Â±  |0.0317|
| - bbh_cot_fewshot_geometric_shapes                       |      4|get-answer       |     3|exact_match|â†‘  | 0.3520|Â±  |0.0303|
| - bbh_cot_fewshot_hyperbaton                             |      4|get-answer       |     3|exact_match|â†‘  | 0.7080|Â±  |0.0288|
| - bbh_cot_fewshot_logical_deduction_five_objects         |      4|get-answer       |     3|exact_match|â†‘  | 0.2840|Â±  |0.0286|
| - bbh_cot_fewshot_logical_deduction_seven_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.2160|Â±  |0.0261|
| - bbh_cot_fewshot_logical_deduction_three_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.5920|Â±  |0.0311|
| - bbh_cot_fewshot_movie_recommendation                   |      4|get-answer       |     3|exact_match|â†‘  | 0.7680|Â±  |0.0268|
| - bbh_cot_fewshot_multistep_arithmetic_two               |      4|get-answer       |     3|exact_match|â†‘  | 0.1000|Â±  |0.0190|
| - bbh_cot_fewshot_navigate                               |      4|get-answer       |     3|exact_match|â†‘  | 0.5920|Â±  |0.0311|
| - bbh_cot_fewshot_object_counting                        |      4|get-answer       |     3|exact_match|â†‘  | 0.5960|Â±  |0.0311|
| - bbh_cot_fewshot_penguins_in_a_table                    |      4|get-answer       |     3|exact_match|â†‘  | 0.4795|Â±  |0.0415|
| - bbh_cot_fewshot_reasoning_about_colored_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.4840|Â±  |0.0317|
| - bbh_cot_fewshot_ruin_names                             |      4|get-answer       |     3|exact_match|â†‘  | 0.4560|Â±  |0.0316|
| - bbh_cot_fewshot_salient_translation_error_detection    |      4|get-answer       |     3|exact_match|â†‘  | 0.3880|Â±  |0.0309|
| - bbh_cot_fewshot_snarks                                 |      4|get-answer       |     3|exact_match|â†‘  | 0.5225|Â±  |0.0375|
| - bbh_cot_fewshot_sports_understanding                   |      4|get-answer       |     3|exact_match|â†‘  | 0.9160|Â±  |0.0176|
| - bbh_cot_fewshot_temporal_sequences                     |      4|get-answer       |     3|exact_match|â†‘  | 0.2960|Â±  |0.0289|
| - bbh_cot_fewshot_tracking_shuffled_objects_five_objects |      4|get-answer       |     3|exact_match|â†‘  | 0.1520|Â±  |0.0228|
| - bbh_cot_fewshot_tracking_shuffled_objects_seven_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.1360|Â±  |0.0217|
| - bbh_cot_fewshot_tracking_shuffled_objects_three_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.3200|Â±  |0.0296|
| - bbh_cot_fewshot_web_of_lies                            |      4|get-answer       |     3|exact_match|â†‘  | 0.9320|Â±  |0.0160|
| - bbh_cot_fewshot_word_sorting                           |      4|get-answer       |     3|exact_match|â†‘  | 0.0640|Â±  |0.0155|
|boolq                                                     |      2|none             |     0|acc        |â†‘  | 0.8330|Â±  |0.0065|
|drop                                                      |      3|none             |     0|em         |â†‘  | 0.0113|Â±  |0.0011|
|                                                          |       |none             |     0|f1         |â†‘  | 0.1030|Â±  |0.0019|
|gpqa_diamond_cot_n_shot                                   |      2|flexible-extract |     0|exact_match|â†‘  | 0.0808|Â±  |0.0194|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_cot_zeroshot                                 |      1|flexible-extract |     0|exact_match|â†‘  | 0.1515|Â±  |0.0255|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_generative_n_shot                            |      2|flexible-extract |     0|exact_match|â†‘  | 0.2626|Â±  |0.0314|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_n_shot                                       |      2|none             |     0|acc        |â†‘  | 0.3030|Â±  |0.0327|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3030|Â±  |0.0327|
|gpqa_diamond_zeroshot                                     |      1|none             |     0|acc        |â†‘  | 0.3485|Â±  |0.0339|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3485|Â±  |0.0339|
|gpqa_extended_cot_n_shot                                  |      2|flexible-extract |     0|exact_match|â†‘  | 0.1612|Â±  |0.0158|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_cot_zeroshot                                |      1|flexible-extract |     0|exact_match|â†‘  | 0.1612|Â±  |0.0158|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_generative_n_shot                           |      2|flexible-extract |     0|exact_match|â†‘  | 0.2564|Â±  |0.0187|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0018|Â±  |0.0018|
|gpqa_extended_n_shot                                      |      2|none             |     0|acc        |â†‘  | 0.2711|Â±  |0.0190|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2711|Â±  |0.0190|
|gpqa_extended_zeroshot                                    |      1|none             |     0|acc        |â†‘  | 0.2766|Â±  |0.0192|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2766|Â±  |0.0192|
|gpqa_main_cot_n_shot                                      |      2|flexible-extract |     0|exact_match|â†‘  | 0.1607|Â±  |0.0174|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_cot_zeroshot                                    |      1|flexible-extract |     0|exact_match|â†‘  | 0.1496|Â±  |0.0169|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0022|Â±  |0.0022|
|gpqa_main_generative_n_shot                               |      2|flexible-extract |     0|exact_match|â†‘  | 0.2679|Â±  |0.0209|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_n_shot                                          |      2|none             |     0|acc        |â†‘  | 0.2746|Â±  |0.0211|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2746|Â±  |0.0211|
|gpqa_main_zeroshot                                        |      1|none             |     0|acc        |â†‘  | 0.2924|Â±  |0.0215|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2924|Â±  |0.0215|
|gsm8k                                                     |      3|flexible-extract |     5|exact_match|â†‘  | 0.5087|Â±  |0.0138|
|                                                          |       |strict-match     |     5|exact_match|â†‘  | 0.4640|Â±  |0.0137|
|hellaswag                                                 |      1|none             |     0|acc        |â†‘  | 0.5914|Â±  |0.0049|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.7772|Â±  |0.0042|
|mmlu                                                      |      2|none             |      |acc        |â†‘  | 0.4988|Â±  |0.0040|
| - humanities                                             |      2|none             |      |acc        |â†‘  | 0.4627|Â±  |0.0069|
|  - formal_logic                                          |      1|none             |     0|acc        |â†‘  | 0.3254|Â±  |0.0419|
|  - high_school_european_history                          |      1|none             |     0|acc        |â†‘  | 0.6485|Â±  |0.0373|
|  - high_school_us_history                                |      1|none             |     0|acc        |â†‘  | 0.6912|Â±  |0.0324|
|  - high_school_world_history                             |      1|none             |     0|acc        |â†‘  | 0.7215|Â±  |0.0292|
|  - international_law                                     |      1|none             |     0|acc        |â†‘  | 0.6364|Â±  |0.0439|
|  - jurisprudence                                         |      1|none             |     0|acc        |â†‘  | 0.6019|Â±  |0.0473|
|  - logical_fallacies                                     |      1|none             |     0|acc        |â†‘  | 0.6626|Â±  |0.0371|
|  - moral_disputes                                        |      1|none             |     0|acc        |â†‘  | 0.5607|Â±  |0.0267|
|  - moral_scenarios                                       |      1|none             |     0|acc        |â†‘  | 0.2391|Â±  |0.0143|
|  - philosophy                                            |      1|none             |     0|acc        |â†‘  | 0.5627|Â±  |0.0282|
|  - prehistory                                            |      1|none             |     0|acc        |â†‘  | 0.5586|Â±  |0.0276|
|  - professional_law                                      |      1|none             |     0|acc        |â†‘  | 0.3761|Â±  |0.0124|
|  - world_religions                                       |      1|none             |     0|acc        |â†‘  | 0.7368|Â±  |0.0338|
| - other                                                  |      2|none             |      |acc        |â†‘  | 0.5768|Â±  |0.0086|
|  - business_ethics                                       |      1|none             |     0|acc        |â†‘  | 0.5600|Â±  |0.0499|
|  - clinical_knowledge                                    |      1|none             |     0|acc        |â†‘  | 0.5585|Â±  |0.0306|
|  - college_medicine                                      |      1|none             |     0|acc        |â†‘  | 0.4740|Â±  |0.0381|
|  - global_facts                                          |      1|none             |     0|acc        |â†‘  | 0.4100|Â±  |0.0494|
|  - human_aging                                           |      1|none             |     0|acc        |â†‘  | 0.5291|Â±  |0.0335|
|  - management                                            |      1|none             |     0|acc        |â†‘  | 0.6893|Â±  |0.0458|
|  - marketing                                             |      1|none             |     0|acc        |â†‘  | 0.7821|Â±  |0.0270|
|  - medical_genetics                                      |      1|none             |     0|acc        |â†‘  | 0.5900|Â±  |0.0494|
|  - miscellaneous                                         |      1|none             |     0|acc        |â†‘  | 0.7241|Â±  |0.0160|
|  - nutrition                                             |      1|none             |     0|acc        |â†‘  | 0.5490|Â±  |0.0285|
|  - professional_accounting                               |      1|none             |     0|acc        |â†‘  | 0.3546|Â±  |0.0285|
|  - professional_medicine                                 |      1|none             |     0|acc        |â†‘  | 0.4522|Â±  |0.0302|
|  - virology                                              |      1|none             |     0|acc        |â†‘  | 0.4578|Â±  |0.0388|
| - social sciences                                        |      2|none             |      |acc        |â†‘  | 0.5635|Â±  |0.0087|
|  - econometrics                                          |      1|none             |     0|acc        |â†‘  | 0.3246|Â±  |0.0440|
|  - high_school_geography                                 |      1|none             |     0|acc        |â†‘  | 0.6364|Â±  |0.0343|
|  - high_school_government_and_politics                   |      1|none             |     0|acc        |â†‘  | 0.6839|Â±  |0.0336|
|  - high_school_macroeconomics                            |      1|none             |     0|acc        |â†‘  | 0.4333|Â±  |0.0251|
|  - high_school_microeconomics                            |      1|none             |     0|acc        |â†‘  | 0.4328|Â±  |0.0322|
|  - high_school_psychology                                |      1|none             |     0|acc        |â†‘  | 0.6991|Â±  |0.0197|
|  - human_sexuality                                       |      1|none             |     0|acc        |â†‘  | 0.5420|Â±  |0.0437|
|  - professional_psychology                               |      1|none             |     0|acc        |â†‘  | 0.4886|Â±  |0.0202|
|  - public_relations                                      |      1|none             |     0|acc        |â†‘  | 0.5727|Â±  |0.0474|
|  - security_studies                                      |      1|none             |     0|acc        |â†‘  | 0.5796|Â±  |0.0316|
|  - sociology                                             |      1|none             |     0|acc        |â†‘  | 0.7015|Â±  |0.0324|
|  - us_foreign_policy                                     |      1|none             |     0|acc        |â†‘  | 0.7000|Â±  |0.0461|
| - stem                                                   |      2|none             |      |acc        |â†‘  | 0.4126|Â±  |0.0086|
|  - abstract_algebra                                      |      1|none             |     0|acc        |â†‘  | 0.2900|Â±  |0.0456|
|  - anatomy                                               |      1|none             |     0|acc        |â†‘  | 0.4963|Â±  |0.0432|
|  - astronomy                                             |      1|none             |     0|acc        |â†‘  | 0.5329|Â±  |0.0406|
|  - college_biology                                       |      1|none             |     0|acc        |â†‘  | 0.5208|Â±  |0.0418|
|  - college_chemistry                                     |      1|none             |     0|acc        |â†‘  | 0.3700|Â±  |0.0485|
|  - college_computer_science                              |      1|none             |     0|acc        |â†‘  | 0.4100|Â±  |0.0494|
|  - college_mathematics                                   |      1|none             |     0|acc        |â†‘  | 0.3900|Â±  |0.0490|
|  - college_physics                                       |      1|none             |     0|acc        |â†‘  | 0.3235|Â±  |0.0466|
|  - computer_security                                     |      1|none             |     0|acc        |â†‘  | 0.6200|Â±  |0.0488|
|  - conceptual_physics                                    |      1|none             |     0|acc        |â†‘  | 0.3915|Â±  |0.0319|
|  - electrical_engineering                                |      1|none             |     0|acc        |â†‘  | 0.4069|Â±  |0.0409|
|  - elementary_mathematics                                |      1|none             |     0|acc        |â†‘  | 0.3307|Â±  |0.0242|
|  - high_school_biology                                   |      1|none             |     0|acc        |â†‘  | 0.5903|Â±  |0.0280|
|  - high_school_chemistry                                 |      1|none             |     0|acc        |â†‘  | 0.3793|Â±  |0.0341|
|  - high_school_computer_science                          |      1|none             |     0|acc        |â†‘  | 0.4600|Â±  |0.0501|
|  - high_school_mathematics                               |      1|none             |     0|acc        |â†‘  | 0.2741|Â±  |0.0272|
|  - high_school_physics                                   |      1|none             |     0|acc        |â†‘  | 0.2914|Â±  |0.0371|
|  - high_school_statistics                                |      1|none             |     0|acc        |â†‘  | 0.3750|Â±  |0.0330|
|  - machine_learning                                      |      1|none             |     0|acc        |â†‘  | 0.5000|Â±  |0.0475|
|nq_open                                                   |      4|remove_whitespace|     0|exact_match|â†‘  | 0.0634|Â±  |0.0041|
|openbookqa                                                |      1|none             |     0|acc        |â†‘  | 0.3500|Â±  |0.0214|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.4600|Â±  |0.0223|
|piqa                                                      |      1|none             |     0|acc        |â†‘  | 0.7949|Â±  |0.0094|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.8014|Â±  |0.0093|
|qnli                                                      |      1|none             |     0|acc        |â†‘  | 0.4970|Â±  |0.0068|
|sciq                                                      |      1|none             |     0|acc        |â†‘  | 0.9250|Â±  |0.0083|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.8930|Â±  |0.0098|
|triviaqa                                                  |      3|remove_whitespace|     0|exact_match|â†‘  | 0.3112|Â±  |0.0035|
|truthfulqa_gen                                            |      3|none             |     0|bleu_acc   |â†‘  | 0.4321|Â±  |0.0173|
|                                                          |       |none             |     0|bleu_diff  |â†‘  |-1.9106|Â±  |0.5822|
|                                                          |       |none             |     0|bleu_max   |â†‘  |20.8129|Â±  |0.7241|
|                                                          |       |none             |     0|rouge1_acc |â†‘  | 0.4541|Â±  |0.0174|
|                                                          |       |none             |     0|rouge1_diff|â†‘  |-2.6308|Â±  |0.6640|
|                                                          |       |none             |     0|rouge1_max |â†‘  |45.5368|Â±  |0.8107|
|                                                          |       |none             |     0|rouge2_acc |â†‘  | 0.3415|Â±  |0.0166|
|                                                          |       |none             |     0|rouge2_diff|â†‘  |-3.7356|Â±  |0.7920|
|                                                          |       |none             |     0|rouge2_max |â†‘  |31.2711|Â±  |0.9083|
|                                                          |       |none             |     0|rougeL_acc |â†‘  | 0.4272|Â±  |0.0173|
|                                                          |       |none             |     0|rougeL_diff|â†‘  |-2.8527|Â±  |0.6674|
|                                                          |       |none             |     0|rougeL_max |â†‘  |42.3939|Â±  |0.8172|
|truthfulqa_mc1                                            |      2|none             |     0|acc        |â†‘  | 0.3488|Â±  |0.0167|
|truthfulqa_mc2                                            |      3|none             |     0|acc        |â†‘  | 0.4789|Â±  |0.0154|
|winogrande                                                |      1|none             |     0|acc        |â†‘  | 0.7017|Â±  |0.0129|

|      Groups      |Version|  Filter  |n-shot|  Metric   |   |Value |   |Stderr|
|------------------|------:|----------|------|-----------|---|-----:|---|-----:|
|bbh               |      3|get-answer|      |exact_match|â†‘  |0.4548|Â±  |0.0054|
|mmlu              |      2|none      |      |acc        |â†‘  |0.4988|Â±  |0.0040|
| - humanities     |      2|none      |      |acc        |â†‘  |0.4627|Â±  |0.0069|
| - other          |      2|none      |      |acc        |â†‘  |0.5768|Â±  |0.0086|
| - social sciences|      2|none      |      |acc        |â†‘  |0.5635|Â±  |0.0087|
| - stem           |      2|none      |      |acc        |â†‘  |0.4126|Â±  |0.0086|

deepseek-ai_deepseek-llm-7b-chat: 10h 7m 2s
âœ… Benchmark completed for deepseek-ai_deepseek-llm-7b-chat

ðŸ”¥ Starting benchmark for deepseek-ai_deepseek-llm-7b-base
hf (pretrained=/mnt/data8tb/Documents/llm/llm_models/deepseek-ai_deepseek-llm-7b-base,trust_remote_code=True), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 3
|                          Tasks                           |Version|     Filter      |n-shot|  Metric   |   | Value  |   |Stderr|
|----------------------------------------------------------|------:|-----------------|-----:|-----------|---|-------:|---|-----:|
|anli_r1                                                   |      1|none             |     0|acc        |â†‘  |  0.3400|Â±  |0.0150|
|anli_r2                                                   |      1|none             |     0|acc        |â†‘  |  0.3630|Â±  |0.0152|
|anli_r3                                                   |      1|none             |     0|acc        |â†‘  |  0.3775|Â±  |0.0140|
|arc_challenge                                             |      1|none             |     0|acc        |â†‘  |  0.4352|Â±  |0.0145|
|                                                          |       |none             |     0|acc_norm   |â†‘  |  0.4454|Â±  |0.0145|
|bbh                                                       |      3|get-answer       |      |exact_match|â†‘  |  0.4237|Â±  |0.0054|
| - bbh_cot_fewshot_boolean_expressions                    |      4|get-answer       |     3|exact_match|â†‘  |  0.7320|Â±  |0.0281|
| - bbh_cot_fewshot_causal_judgement                       |      4|get-answer       |     3|exact_match|â†‘  |  0.4759|Â±  |0.0366|
| - bbh_cot_fewshot_date_understanding                     |      4|get-answer       |     3|exact_match|â†‘  |  0.6640|Â±  |0.0299|
| - bbh_cot_fewshot_disambiguation_qa                      |      4|get-answer       |     3|exact_match|â†‘  |  0.4120|Â±  |0.0312|
| - bbh_cot_fewshot_dyck_languages                         |      4|get-answer       |     3|exact_match|â†‘  |  0.0160|Â±  |0.0080|
| - bbh_cot_fewshot_formal_fallacies                       |      4|get-answer       |     3|exact_match|â†‘  |  0.5160|Â±  |0.0317|
| - bbh_cot_fewshot_geometric_shapes                       |      4|get-answer       |     3|exact_match|â†‘  |  0.3720|Â±  |0.0306|
| - bbh_cot_fewshot_hyperbaton                             |      4|get-answer       |     3|exact_match|â†‘  |  0.6720|Â±  |0.0298|
| - bbh_cot_fewshot_logical_deduction_five_objects         |      4|get-answer       |     3|exact_match|â†‘  |  0.2960|Â±  |0.0289|
| - bbh_cot_fewshot_logical_deduction_seven_objects        |      4|get-answer       |     3|exact_match|â†‘  |  0.1920|Â±  |0.0250|
| - bbh_cot_fewshot_logical_deduction_three_objects        |      4|get-answer       |     3|exact_match|â†‘  |  0.4880|Â±  |0.0317|
| - bbh_cot_fewshot_movie_recommendation                   |      4|get-answer       |     3|exact_match|â†‘  |  0.7920|Â±  |0.0257|
| - bbh_cot_fewshot_multistep_arithmetic_two               |      4|get-answer       |     3|exact_match|â†‘  |  0.0160|Â±  |0.0080|
| - bbh_cot_fewshot_navigate                               |      4|get-answer       |     3|exact_match|â†‘  |  0.5600|Â±  |0.0315|
| - bbh_cot_fewshot_object_counting                        |      4|get-answer       |     3|exact_match|â†‘  |  0.4880|Â±  |0.0317|
| - bbh_cot_fewshot_penguins_in_a_table                    |      4|get-answer       |     3|exact_match|â†‘  |  0.3973|Â±  |0.0406|
| - bbh_cot_fewshot_reasoning_about_colored_objects        |      4|get-answer       |     3|exact_match|â†‘  |  0.4680|Â±  |0.0316|
| - bbh_cot_fewshot_ruin_names                             |      4|get-answer       |     3|exact_match|â†‘  |  0.4560|Â±  |0.0316|
| - bbh_cot_fewshot_salient_translation_error_detection    |      4|get-answer       |     3|exact_match|â†‘  |  0.3160|Â±  |0.0295|
| - bbh_cot_fewshot_snarks                                 |      4|get-answer       |     3|exact_match|â†‘  |  0.5056|Â±  |0.0376|
| - bbh_cot_fewshot_sports_understanding                   |      4|get-answer       |     3|exact_match|â†‘  |  0.9040|Â±  |0.0187|
| - bbh_cot_fewshot_temporal_sequences                     |      4|get-answer       |     3|exact_match|â†‘  |  0.2800|Â±  |0.0285|
| - bbh_cot_fewshot_tracking_shuffled_objects_five_objects |      4|get-answer       |     3|exact_match|â†‘  |  0.1920|Â±  |0.0250|
| - bbh_cot_fewshot_tracking_shuffled_objects_seven_objects|      4|get-answer       |     3|exact_match|â†‘  |  0.1040|Â±  |0.0193|
| - bbh_cot_fewshot_tracking_shuffled_objects_three_objects|      4|get-answer       |     3|exact_match|â†‘  |  0.3240|Â±  |0.0297|
| - bbh_cot_fewshot_web_of_lies                            |      4|get-answer       |     3|exact_match|â†‘  |  0.7640|Â±  |0.0269|
| - bbh_cot_fewshot_word_sorting                           |      4|get-answer       |     3|exact_match|â†‘  |  0.0640|Â±  |0.0155|
|boolq                                                     |      2|none             |     0|acc        |â†‘  |  0.7235|Â±  |0.0078|
|drop                                                      |      3|none             |     0|em         |â†‘  |  0.0168|Â±  |0.0013|
|                                                          |       |none             |     0|f1         |â†‘  |  0.0422|Â±  |0.0016|
|gpqa_diamond_cot_n_shot                                   |      2|flexible-extract |     0|exact_match|â†‘  |  0.1061|Â±  |0.0219|
|                                                          |       |strict-match     |     0|exact_match|â†‘  |  0.0303|Â±  |0.0122|
|gpqa_diamond_cot_zeroshot                                 |      1|flexible-extract |     0|exact_match|â†‘  |  0.0960|Â±  |0.0210|
|                                                          |       |strict-match     |     0|exact_match|â†‘  |  0.0152|Â±  |0.0087|
|gpqa_diamond_generative_n_shot                            |      2|flexible-extract |     0|exact_match|â†‘  |  0.2020|Â±  |0.0286|
|                                                          |       |strict-match     |     0|exact_match|â†‘  |  0.0101|Â±  |0.0071|
|gpqa_diamond_n_shot                                       |      2|none             |     0|acc        |â†‘  |  0.2677|Â±  |0.0315|
|                                                          |       |none             |     0|acc_norm   |â†‘  |  0.2677|Â±  |0.0315|
|gpqa_diamond_zeroshot                                     |      1|none             |     0|acc        |â†‘  |  0.2677|Â±  |0.0315|
|                                                          |       |none             |     0|acc_norm   |â†‘  |  0.2677|Â±  |0.0315|
|gpqa_extended_cot_n_shot                                  |      2|flexible-extract |     0|exact_match|â†‘  |  0.1026|Â±  |0.0130|
|                                                          |       |strict-match     |     0|exact_match|â†‘  |  0.0165|Â±  |0.0055|
|gpqa_extended_cot_zeroshot                                |      1|flexible-extract |     0|exact_match|â†‘  |  0.1062|Â±  |0.0132|
|                                                          |       |strict-match     |     0|exact_match|â†‘  |  0.0018|Â±  |0.0018|
|gpqa_extended_generative_n_shot                           |      2|flexible-extract |     0|exact_match|â†‘  |  0.1960|Â±  |0.0170|
|                                                          |       |strict-match     |     0|exact_match|â†‘  |  0.0055|Â±  |0.0032|
|gpqa_extended_n_shot                                      |      2|none             |     0|acc        |â†‘  |  0.2711|Â±  |0.0190|
|                                                          |       |none             |     0|acc_norm   |â†‘  |  0.2711|Â±  |0.0190|
|gpqa_extended_zeroshot                                    |      1|none             |     0|acc        |â†‘  |  0.2747|Â±  |0.0191|
|                                                          |       |none             |     0|acc_norm   |â†‘  |  0.2747|Â±  |0.0191|
|gpqa_main_cot_n_shot                                      |      2|flexible-extract |     0|exact_match|â†‘  |  0.1250|Â±  |0.0156|
|                                                          |       |strict-match     |     0|exact_match|â†‘  |  0.0223|Â±  |0.0070|
|gpqa_main_cot_zeroshot                                    |      1|flexible-extract |     0|exact_match|â†‘  |  0.0871|Â±  |0.0133|
|                                                          |       |strict-match     |     0|exact_match|â†‘  |  0.0045|Â±  |0.0032|
|gpqa_main_generative_n_shot                               |      2|flexible-extract |     0|exact_match|â†‘  |  0.2344|Â±  |0.0200|
|                                                          |       |strict-match     |     0|exact_match|â†‘  |  0.0045|Â±  |0.0032|
|gpqa_main_n_shot                                          |      2|none             |     0|acc        |â†‘  |  0.2567|Â±  |0.0207|
|                                                          |       |none             |     0|acc_norm   |â†‘  |  0.2567|Â±  |0.0207|
|gpqa_main_zeroshot                                        |      1|none             |     0|acc        |â†‘  |  0.2522|Â±  |0.0205|
|                                                          |       |none             |     0|acc_norm   |â†‘  |  0.2522|Â±  |0.0205|
|gsm8k                                                     |      3|flexible-extract |     5|exact_match|â†‘  |  0.1638|Â±  |0.0102|
|                                                          |       |strict-match     |     5|exact_match|â†‘  |  0.1622|Â±  |0.0102|
|hellaswag                                                 |      1|none             |     0|acc        |â†‘  |  0.5706|Â±  |0.0049|
|                                                          |       |none             |     0|acc_norm   |â†‘  |  0.7606|Â±  |0.0043|
|mmlu                                                      |      2|none             |      |acc        |â†‘  |  0.4428|Â±  |0.0041|
| - humanities                                             |      2|none             |      |acc        |â†‘  |  0.4106|Â±  |0.0069|
|  - formal_logic                                          |      1|none             |     0|acc        |â†‘  |  0.2540|Â±  |0.0389|
|  - high_school_european_history                          |      1|none             |     0|acc        |â†‘  |  0.5576|Â±  |0.0388|
|  - high_school_us_history                                |      1|none             |     0|acc        |â†‘  |  0.5490|Â±  |0.0349|
|  - high_school_world_history                             |      1|none             |     0|acc        |â†‘  |  0.5992|Â±  |0.0319|
|  - international_law                                     |      1|none             |     0|acc        |â†‘  |  0.5868|Â±  |0.0450|
|  - jurisprudence                                         |      1|none             |     0|acc        |â†‘  |  0.5926|Â±  |0.0475|
|  - logical_fallacies                                     |      1|none             |     0|acc        |â†‘  |  0.5951|Â±  |0.0386|
|  - moral_disputes                                        |      1|none             |     0|acc        |â†‘  |  0.4827|Â±  |0.0269|
|  - moral_scenarios                                       |      1|none             |     0|acc        |â†‘  |  0.2425|Â±  |0.0143|
|  - philosophy                                            |      1|none             |     0|acc        |â†‘  |  0.5498|Â±  |0.0283|
|  - prehistory                                            |      1|none             |     0|acc        |â†‘  |  0.5062|Â±  |0.0278|
|  - professional_law                                      |      1|none             |     0|acc        |â†‘  |  0.3214|Â±  |0.0119|
|  - world_religions                                       |      1|none             |     0|acc        |â†‘  |  0.6433|Â±  |0.0367|
| - other                                                  |      2|none             |      |acc        |â†‘  |  0.4982|Â±  |0.0088|
|  - business_ethics                                       |      1|none             |     0|acc        |â†‘  |  0.4200|Â±  |0.0496|
|  - clinical_knowledge                                    |      1|none             |     0|acc        |â†‘  |  0.4792|Â±  |0.0307|
|  - college_medicine                                      |      1|none             |     0|acc        |â†‘  |  0.4104|Â±  |0.0375|
|  - global_facts                                          |      1|none             |     0|acc        |â†‘  |  0.3300|Â±  |0.0473|
|  - human_aging                                           |      1|none             |     0|acc        |â†‘  |  0.4798|Â±  |0.0335|
|  - management                                            |      1|none             |     0|acc        |â†‘  |  0.5437|Â±  |0.0493|
|  - marketing                                             |      1|none             |     0|acc        |â†‘  |  0.6453|Â±  |0.0313|
|  - medical_genetics                                      |      1|none             |     0|acc        |â†‘  |  0.4600|Â±  |0.0501|
|  - miscellaneous                                         |      1|none             |     0|acc        |â†‘  |  0.6245|Â±  |0.0173|
|  - nutrition                                             |      1|none             |     0|acc        |â†‘  |  0.4771|Â±  |0.0286|
|  - professional_accounting                               |      1|none             |     0|acc        |â†‘  |  0.3794|Â±  |0.0289|
|  - professional_medicine                                 |      1|none             |     0|acc        |â†‘  |  0.3897|Â±  |0.0296|
|  - virology                                              |      1|none             |     0|acc        |â†‘  |  0.4036|Â±  |0.0382|
| - social sciences                                        |      2|none             |      |acc        |â†‘  |  0.5005|Â±  |0.0089|
|  - econometrics                                          |      1|none             |     0|acc        |â†‘  |  0.2719|Â±  |0.0419|
|  - high_school_geography                                 |      1|none             |     0|acc        |â†‘  |  0.4697|Â±  |0.0356|
|  - high_school_government_and_politics                   |      1|none             |     0|acc        |â†‘  |  0.5492|Â±  |0.0359|
|  - high_school_macroeconomics                            |      1|none             |     0|acc        |â†‘  |  0.4179|Â±  |0.0250|
|  - high_school_microeconomics                            |      1|none             |     0|acc        |â†‘  |  0.3992|Â±  |0.0318|
|  - high_school_psychology                                |      1|none             |     0|acc        |â†‘  |  0.5817|Â±  |0.0211|
|  - human_sexuality                                       |      1|none             |     0|acc        |â†‘  |  0.5725|Â±  |0.0434|
|  - professional_psychology                               |      1|none             |     0|acc        |â†‘  |  0.4641|Â±  |0.0202|
|  - public_relations                                      |      1|none             |     0|acc        |â†‘  |  0.5091|Â±  |0.0479|
|  - security_studies                                      |      1|none             |     0|acc        |â†‘  |  0.5020|Â±  |0.0320|
|  - sociology                                             |      1|none             |     0|acc        |â†‘  |  0.6368|Â±  |0.0340|
|  - us_foreign_policy                                     |      1|none             |     0|acc        |â†‘  |  0.6900|Â±  |0.0465|
| - stem                                                   |      2|none             |      |acc        |â†‘  |  0.3800|Â±  |0.0085|
|  - abstract_algebra                                      |      1|none             |     0|acc        |â†‘  |  0.3000|Â±  |0.0461|
|  - anatomy                                               |      1|none             |     0|acc        |â†‘  |  0.4815|Â±  |0.0432|
|  - astronomy                                             |      1|none             |     0|acc        |â†‘  |  0.4934|Â±  |0.0407|
|  - college_biology                                       |      1|none             |     0|acc        |â†‘  |  0.4653|Â±  |0.0417|
|  - college_chemistry                                     |      1|none             |     0|acc        |â†‘  |  0.3300|Â±  |0.0473|
|  - college_computer_science                              |      1|none             |     0|acc        |â†‘  |  0.3500|Â±  |0.0479|
|  - college_mathematics                                   |      1|none             |     0|acc        |â†‘  |  0.3200|Â±  |0.0469|
|  - college_physics                                       |      1|none             |     0|acc        |â†‘  |  0.2745|Â±  |0.0444|
|  - computer_security                                     |      1|none             |     0|acc        |â†‘  |  0.5200|Â±  |0.0502|
|  - conceptual_physics                                    |      1|none             |     0|acc        |â†‘  |  0.3957|Â±  |0.0320|
|  - electrical_engineering                                |      1|none             |     0|acc        |â†‘  |  0.4621|Â±  |0.0415|
|  - elementary_mathematics                                |      1|none             |     0|acc        |â†‘  |  0.2857|Â±  |0.0233|
|  - high_school_biology                                   |      1|none             |     0|acc        |â†‘  |  0.5032|Â±  |0.0284|
|  - high_school_chemistry                                 |      1|none             |     0|acc        |â†‘  |  0.3744|Â±  |0.0341|
|  - high_school_computer_science                          |      1|none             |     0|acc        |â†‘  |  0.4200|Â±  |0.0496|
|  - high_school_mathematics                               |      1|none             |     0|acc        |â†‘  |  0.2815|Â±  |0.0274|
|  - high_school_physics                                   |      1|none             |     0|acc        |â†‘  |  0.3245|Â±  |0.0382|
|  - high_school_statistics                                |      1|none             |     0|acc        |â†‘  |  0.3796|Â±  |0.0331|
|  - machine_learning                                      |      1|none             |     0|acc        |â†‘  |  0.2857|Â±  |0.0429|
|nq_open                                                   |      4|remove_whitespace|     0|exact_match|â†‘  |  0.1510|Â±  |0.0060|
|openbookqa                                                |      1|none             |     0|acc        |â†‘  |  0.3260|Â±  |0.0210|
|                                                          |       |none             |     0|acc_norm   |â†‘  |  0.4340|Â±  |0.0222|
|piqa                                                      |      1|none             |     0|acc        |â†‘  |  0.7894|Â±  |0.0095|
|                                                          |       |none             |     0|acc_norm   |â†‘  |  0.7976|Â±  |0.0094|
|qnli                                                      |      1|none             |     0|acc        |â†‘  |  0.4959|Â±  |0.0068|
|sciq                                                      |      1|none             |     0|acc        |â†‘  |  0.9400|Â±  |0.0075|
|                                                          |       |none             |     0|acc_norm   |â†‘  |  0.9150|Â±  |0.0088|
|triviaqa                                                  |      3|remove_whitespace|     0|exact_match|â†‘  |  0.5004|Â±  |0.0037|
|truthfulqa_gen                                            |      3|none             |     0|bleu_acc   |â†‘  |  0.3097|Â±  |0.0162|
|                                                          |       |none             |     0|bleu_diff  |â†‘  | -8.7271|Â±  |0.7712|
|                                                          |       |none             |     0|bleu_max   |â†‘  | 24.9259|Â±  |0.7566|
|                                                          |       |none             |     0|rouge1_acc |â†‘  |  0.2974|Â±  |0.0160|
|                                                          |       |none             |     0|rouge1_diff|â†‘  |-11.0783|Â±  |0.8128|
|                                                          |       |none             |     0|rouge1_max |â†‘  | 50.8642|Â±  |0.8265|
|                                                          |       |none             |     0|rouge2_acc |â†‘  |  0.2436|Â±  |0.0150|
|                                                          |       |none             |     0|rouge2_diff|â†‘  |-13.5478|Â±  |0.9872|
|                                                          |       |none             |     0|rouge2_max |â†‘  | 34.4263|Â±  |0.9544|
|                                                          |       |none             |     0|rougeL_acc |â†‘  |  0.2876|Â±  |0.0158|
|                                                          |       |none             |     0|rougeL_diff|â†‘  |-11.6501|Â±  |0.8204|
|                                                          |       |none             |     0|rougeL_max |â†‘  | 47.8267|Â±  |0.8414|
|truthfulqa_mc1                                            |      2|none             |     0|acc        |â†‘  |  0.2326|Â±  |0.0148|
|truthfulqa_mc2                                            |      3|none             |     0|acc        |â†‘  |  0.3492|Â±  |0.0137|
|winogrande                                                |      1|none             |     0|acc        |â†‘  |  0.6938|Â±  |0.0130|

|      Groups      |Version|  Filter  |n-shot|  Metric   |   |Value |   |Stderr|
|------------------|------:|----------|------|-----------|---|-----:|---|-----:|
|bbh               |      3|get-answer|      |exact_match|â†‘  |0.4237|Â±  |0.0054|
|mmlu              |      2|none      |      |acc        |â†‘  |0.4428|Â±  |0.0041|
| - humanities     |      2|none      |      |acc        |â†‘  |0.4106|Â±  |0.0069|
| - other          |      2|none      |      |acc        |â†‘  |0.4982|Â±  |0.0088|
| - social sciences|      2|none      |      |acc        |â†‘  |0.5005|Â±  |0.0089|
| - stem           |      2|none      |      |acc        |â†‘  |0.3800|Â±  |0.0085|

deepseek-ai_deepseek-llm-7b-base: 7h 11m 27s
âœ… Benchmark completed for deepseek-ai_deepseek-llm-7b-base

ðŸ”¥ Starting benchmark for openchat_openchat-3.6-8b-20240522
hf (pretrained=/mnt/data8tb/Documents/llm/llm_models/openchat_openchat-3.6-8b-20240522,trust_remote_code=True), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 3
|                          Tasks                           |Version|     Filter      |n-shot|  Metric   |   | Value |   |Stderr|
|----------------------------------------------------------|------:|-----------------|-----:|-----------|---|------:|---|-----:|
|anli_r1                                                   |      1|none             |     0|acc        |â†‘  | 0.5560|Â±  |0.0157|
|anli_r2                                                   |      1|none             |     0|acc        |â†‘  | 0.5130|Â±  |0.0158|
|anli_r3                                                   |      1|none             |     0|acc        |â†‘  | 0.4800|Â±  |0.0144|
|arc_challenge                                             |      1|none             |     0|acc        |â†‘  | 0.5640|Â±  |0.0145|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.6032|Â±  |0.0143|
|bbh                                                       |      3|get-answer       |      |exact_match|â†‘  | 0.6179|Â±  |0.0054|
| - bbh_cot_fewshot_boolean_expressions                    |      4|get-answer       |     3|exact_match|â†‘  | 0.8480|Â±  |0.0228|
| - bbh_cot_fewshot_causal_judgement                       |      4|get-answer       |     3|exact_match|â†‘  | 0.5294|Â±  |0.0366|
| - bbh_cot_fewshot_date_understanding                     |      4|get-answer       |     3|exact_match|â†‘  | 0.5840|Â±  |0.0312|
| - bbh_cot_fewshot_disambiguation_qa                      |      4|get-answer       |     3|exact_match|â†‘  | 0.7800|Â±  |0.0263|
| - bbh_cot_fewshot_dyck_languages                         |      4|get-answer       |     3|exact_match|â†‘  | 0.0320|Â±  |0.0112|
| - bbh_cot_fewshot_formal_fallacies                       |      4|get-answer       |     3|exact_match|â†‘  | 0.5480|Â±  |0.0315|
| - bbh_cot_fewshot_geometric_shapes                       |      4|get-answer       |     3|exact_match|â†‘  | 0.4600|Â±  |0.0316|
| - bbh_cot_fewshot_hyperbaton                             |      4|get-answer       |     3|exact_match|â†‘  | 0.9440|Â±  |0.0146|
| - bbh_cot_fewshot_logical_deduction_five_objects         |      4|get-answer       |     3|exact_match|â†‘  | 0.4200|Â±  |0.0313|
| - bbh_cot_fewshot_logical_deduction_seven_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.3600|Â±  |0.0304|
| - bbh_cot_fewshot_logical_deduction_three_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.7480|Â±  |0.0275|
| - bbh_cot_fewshot_movie_recommendation                   |      4|get-answer       |     3|exact_match|â†‘  | 0.8720|Â±  |0.0212|
| - bbh_cot_fewshot_multistep_arithmetic_two               |      4|get-answer       |     3|exact_match|â†‘  | 0.5200|Â±  |0.0317|
| - bbh_cot_fewshot_navigate                               |      4|get-answer       |     3|exact_match|â†‘  | 0.7320|Â±  |0.0281|
| - bbh_cot_fewshot_object_counting                        |      4|get-answer       |     3|exact_match|â†‘  | 0.8600|Â±  |0.0220|
| - bbh_cot_fewshot_penguins_in_a_table                    |      4|get-answer       |     3|exact_match|â†‘  | 0.6233|Â±  |0.0402|
| - bbh_cot_fewshot_reasoning_about_colored_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.6200|Â±  |0.0308|
| - bbh_cot_fewshot_ruin_names                             |      4|get-answer       |     3|exact_match|â†‘  | 0.6200|Â±  |0.0308|
| - bbh_cot_fewshot_salient_translation_error_detection    |      4|get-answer       |     3|exact_match|â†‘  | 0.5720|Â±  |0.0314|
| - bbh_cot_fewshot_snarks                                 |      4|get-answer       |     3|exact_match|â†‘  | 0.4551|Â±  |0.0374|
| - bbh_cot_fewshot_sports_understanding                   |      4|get-answer       |     3|exact_match|â†‘  | 0.9080|Â±  |0.0183|
| - bbh_cot_fewshot_temporal_sequences                     |      4|get-answer       |     3|exact_match|â†‘  | 0.8760|Â±  |0.0209|
| - bbh_cot_fewshot_tracking_shuffled_objects_five_objects |      4|get-answer       |     3|exact_match|â†‘  | 0.4720|Â±  |0.0316|
| - bbh_cot_fewshot_tracking_shuffled_objects_seven_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.4360|Â±  |0.0314|
| - bbh_cot_fewshot_tracking_shuffled_objects_three_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.5120|Â±  |0.0317|
| - bbh_cot_fewshot_web_of_lies                            |      4|get-answer       |     3|exact_match|â†‘  | 0.8920|Â±  |0.0197|
| - bbh_cot_fewshot_word_sorting                           |      4|get-answer       |     3|exact_match|â†‘  | 0.3920|Â±  |0.0309|
|boolq                                                     |      2|none             |     0|acc        |â†‘  | 0.8728|Â±  |0.0058|
|drop                                                      |      3|none             |     0|em         |â†‘  | 0.0547|Â±  |0.0023|
|                                                          |       |none             |     0|f1         |â†‘  | 0.2516|Â±  |0.0032|
|gpqa_diamond_cot_n_shot                                   |      2|flexible-extract |     0|exact_match|â†‘  | 0.2172|Â±  |0.0294|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0202|Â±  |0.0100|
|gpqa_diamond_cot_zeroshot                                 |      1|flexible-extract |     0|exact_match|â†‘  | 0.2020|Â±  |0.0286|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0556|Â±  |0.0163|
|gpqa_diamond_generative_n_shot                            |      2|flexible-extract |     0|exact_match|â†‘  | 0.2980|Â±  |0.0326|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_n_shot                                       |      2|none             |     0|acc        |â†‘  | 0.3283|Â±  |0.0335|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3283|Â±  |0.0335|
|gpqa_diamond_zeroshot                                     |      1|none             |     0|acc        |â†‘  | 0.3333|Â±  |0.0336|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3333|Â±  |0.0336|
|gpqa_extended_cot_n_shot                                  |      2|flexible-extract |     0|exact_match|â†‘  | 0.2216|Â±  |0.0178|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0147|Â±  |0.0051|
|gpqa_extended_cot_zeroshot                                |      1|flexible-extract |     0|exact_match|â†‘  | 0.2546|Â±  |0.0187|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0403|Â±  |0.0084|
|gpqa_extended_generative_n_shot                           |      2|flexible-extract |     0|exact_match|â†‘  | 0.3095|Â±  |0.0198|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_n_shot                                      |      2|none             |     0|acc        |â†‘  | 0.3242|Â±  |0.0200|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3242|Â±  |0.0200|
|gpqa_extended_zeroshot                                    |      1|none             |     0|acc        |â†‘  | 0.3425|Â±  |0.0203|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3425|Â±  |0.0203|
|gpqa_main_cot_n_shot                                      |      2|flexible-extract |     0|exact_match|â†‘  | 0.2098|Â±  |0.0193|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0134|Â±  |0.0054|
|gpqa_main_cot_zeroshot                                    |      1|flexible-extract |     0|exact_match|â†‘  | 0.2567|Â±  |0.0207|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0268|Â±  |0.0076|
|gpqa_main_generative_n_shot                               |      2|flexible-extract |     0|exact_match|â†‘  | 0.3237|Â±  |0.0221|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0022|Â±  |0.0022|
|gpqa_main_n_shot                                          |      2|none             |     0|acc        |â†‘  | 0.3438|Â±  |0.0225|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3438|Â±  |0.0225|
|gpqa_main_zeroshot                                        |      1|none             |     0|acc        |â†‘  | 0.3326|Â±  |0.0223|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3326|Â±  |0.0223|
|gsm8k                                                     |      3|flexible-extract |     5|exact_match|â†‘  | 0.7521|Â±  |0.0119|
|                                                          |       |strict-match     |     5|exact_match|â†‘  | 0.7506|Â±  |0.0119|
|hellaswag                                                 |      1|none             |     0|acc        |â†‘  | 0.6116|Â±  |0.0049|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.7978|Â±  |0.0040|
|mmlu                                                      |      2|none             |      |acc        |â†‘  | 0.6431|Â±  |0.0038|
| - humanities                                             |      2|none             |      |acc        |â†‘  | 0.5966|Â±  |0.0068|
|  - formal_logic                                          |      1|none             |     0|acc        |â†‘  | 0.5000|Â±  |0.0447|
|  - high_school_european_history                          |      1|none             |     0|acc        |â†‘  | 0.7515|Â±  |0.0337|
|  - high_school_us_history                                |      1|none             |     0|acc        |â†‘  | 0.8333|Â±  |0.0262|
|  - high_school_world_history                             |      1|none             |     0|acc        |â†‘  | 0.8439|Â±  |0.0236|
|  - international_law                                     |      1|none             |     0|acc        |â†‘  | 0.7438|Â±  |0.0398|
|  - jurisprudence                                         |      1|none             |     0|acc        |â†‘  | 0.7870|Â±  |0.0396|
|  - logical_fallacies                                     |      1|none             |     0|acc        |â†‘  | 0.7423|Â±  |0.0344|
|  - moral_disputes                                        |      1|none             |     0|acc        |â†‘  | 0.7110|Â±  |0.0244|
|  - moral_scenarios                                       |      1|none             |     0|acc        |â†‘  | 0.4313|Â±  |0.0166|
|  - philosophy                                            |      1|none             |     0|acc        |â†‘  | 0.6849|Â±  |0.0264|
|  - prehistory                                            |      1|none             |     0|acc        |â†‘  | 0.7191|Â±  |0.0250|
|  - professional_law                                      |      1|none             |     0|acc        |â†‘  | 0.4831|Â±  |0.0128|
|  - world_religions                                       |      1|none             |     0|acc        |â†‘  | 0.7895|Â±  |0.0313|
| - other                                                  |      2|none             |      |acc        |â†‘  | 0.7071|Â±  |0.0079|
|  - business_ethics                                       |      1|none             |     0|acc        |â†‘  | 0.6500|Â±  |0.0479|
|  - clinical_knowledge                                    |      1|none             |     0|acc        |â†‘  | 0.7245|Â±  |0.0275|
|  - college_medicine                                      |      1|none             |     0|acc        |â†‘  | 0.6358|Â±  |0.0367|
|  - global_facts                                          |      1|none             |     0|acc        |â†‘  | 0.4200|Â±  |0.0496|
|  - human_aging                                           |      1|none             |     0|acc        |â†‘  | 0.7175|Â±  |0.0302|
|  - management                                            |      1|none             |     0|acc        |â†‘  | 0.7961|Â±  |0.0399|
|  - marketing                                             |      1|none             |     0|acc        |â†‘  | 0.8803|Â±  |0.0213|
|  - medical_genetics                                      |      1|none             |     0|acc        |â†‘  | 0.7200|Â±  |0.0451|
|  - miscellaneous                                         |      1|none             |     0|acc        |â†‘  | 0.8238|Â±  |0.0136|
|  - nutrition                                             |      1|none             |     0|acc        |â†‘  | 0.7124|Â±  |0.0259|
|  - professional_accounting                               |      1|none             |     0|acc        |â†‘  | 0.5142|Â±  |0.0298|
|  - professional_medicine                                 |      1|none             |     0|acc        |â†‘  | 0.6434|Â±  |0.0291|
|  - virology                                              |      1|none             |     0|acc        |â†‘  | 0.5120|Â±  |0.0389|
| - social sciences                                        |      2|none             |      |acc        |â†‘  | 0.7452|Â±  |0.0077|
|  - econometrics                                          |      1|none             |     0|acc        |â†‘  | 0.4561|Â±  |0.0469|
|  - high_school_geography                                 |      1|none             |     0|acc        |â†‘  | 0.7828|Â±  |0.0294|
|  - high_school_government_and_politics                   |      1|none             |     0|acc        |â†‘  | 0.9067|Â±  |0.0210|
|  - high_school_macroeconomics                            |      1|none             |     0|acc        |â†‘  | 0.6590|Â±  |0.0240|
|  - high_school_microeconomics                            |      1|none             |     0|acc        |â†‘  | 0.7353|Â±  |0.0287|
|  - high_school_psychology                                |      1|none             |     0|acc        |â†‘  | 0.8349|Â±  |0.0159|
|  - human_sexuality                                       |      1|none             |     0|acc        |â†‘  | 0.7710|Â±  |0.0369|
|  - professional_psychology                               |      1|none             |     0|acc        |â†‘  | 0.6765|Â±  |0.0189|
|  - public_relations                                      |      1|none             |     0|acc        |â†‘  | 0.6364|Â±  |0.0461|
|  - security_studies                                      |      1|none             |     0|acc        |â†‘  | 0.7347|Â±  |0.0283|
|  - sociology                                             |      1|none             |     0|acc        |â†‘  | 0.8458|Â±  |0.0255|
|  - us_foreign_policy                                     |      1|none             |     0|acc        |â†‘  | 0.8900|Â±  |0.0314|
| - stem                                                   |      2|none             |      |acc        |â†‘  | 0.5496|Â±  |0.0086|
|  - abstract_algebra                                      |      1|none             |     0|acc        |â†‘  | 0.3600|Â±  |0.0482|
|  - anatomy                                               |      1|none             |     0|acc        |â†‘  | 0.6815|Â±  |0.0402|
|  - astronomy                                             |      1|none             |     0|acc        |â†‘  | 0.7368|Â±  |0.0358|
|  - college_biology                                       |      1|none             |     0|acc        |â†‘  | 0.7153|Â±  |0.0377|
|  - college_chemistry                                     |      1|none             |     0|acc        |â†‘  | 0.4100|Â±  |0.0494|
|  - college_computer_science                              |      1|none             |     0|acc        |â†‘  | 0.5200|Â±  |0.0502|
|  - college_mathematics                                   |      1|none             |     0|acc        |â†‘  | 0.3300|Â±  |0.0473|
|  - college_physics                                       |      1|none             |     0|acc        |â†‘  | 0.4804|Â±  |0.0497|
|  - computer_security                                     |      1|none             |     0|acc        |â†‘  | 0.7500|Â±  |0.0435|
|  - conceptual_physics                                    |      1|none             |     0|acc        |â†‘  | 0.5787|Â±  |0.0323|
|  - electrical_engineering                                |      1|none             |     0|acc        |â†‘  | 0.5655|Â±  |0.0413|
|  - elementary_mathematics                                |      1|none             |     0|acc        |â†‘  | 0.4656|Â±  |0.0257|
|  - high_school_biology                                   |      1|none             |     0|acc        |â†‘  | 0.7484|Â±  |0.0247|
|  - high_school_chemistry                                 |      1|none             |     0|acc        |â†‘  | 0.5320|Â±  |0.0351|
|  - high_school_computer_science                          |      1|none             |     0|acc        |â†‘  | 0.6600|Â±  |0.0476|
|  - high_school_mathematics                               |      1|none             |     0|acc        |â†‘  | 0.4000|Â±  |0.0299|
|  - high_school_physics                                   |      1|none             |     0|acc        |â†‘  | 0.3907|Â±  |0.0398|
|  - high_school_statistics                                |      1|none             |     0|acc        |â†‘  | 0.5093|Â±  |0.0341|
|  - machine_learning                                      |      1|none             |     0|acc        |â†‘  | 0.5625|Â±  |0.0471|
|nq_open                                                   |      4|remove_whitespace|     0|exact_match|â†‘  | 0.1706|Â±  |0.0063|
|openbookqa                                                |      1|none             |     0|acc        |â†‘  | 0.3700|Â±  |0.0216|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.4620|Â±  |0.0223|
|piqa                                                      |      1|none             |     0|acc        |â†‘  | 0.8041|Â±  |0.0093|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.8183|Â±  |0.0090|
|qnli                                                      |      1|none             |     0|acc        |â†‘  | 0.7300|Â±  |0.0060|
|sciq                                                      |      1|none             |     0|acc        |â†‘  | 0.9730|Â±  |0.0051|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.9640|Â±  |0.0059|
|triviaqa                                                  |      3|remove_whitespace|     0|exact_match|â†‘  | 0.5659|Â±  |0.0037|
|truthfulqa_gen                                            |      3|none             |     0|bleu_acc   |â†‘  | 0.4162|Â±  |0.0173|
|                                                          |       |none             |     0|bleu_diff  |â†‘  |-2.4558|Â±  |0.6495|
|                                                          |       |none             |     0|bleu_max   |â†‘  |22.9231|Â±  |0.7496|
|                                                          |       |none             |     0|rouge1_acc |â†‘  | 0.4088|Â±  |0.0172|
|                                                          |       |none             |     0|rouge1_diff|â†‘  |-3.9076|Â±  |0.7660|
|                                                          |       |none             |     0|rouge1_max |â†‘  |47.4547|Â±  |0.8751|
|                                                          |       |none             |     0|rouge2_acc |â†‘  | 0.3550|Â±  |0.0168|
|                                                          |       |none             |     0|rouge2_diff|â†‘  |-4.4347|Â±  |0.8978|
|                                                          |       |none             |     0|rouge2_max |â†‘  |33.1938|Â±  |0.9499|
|                                                          |       |none             |     0|rougeL_acc |â†‘  | 0.4051|Â±  |0.0172|
|                                                          |       |none             |     0|rougeL_diff|â†‘  |-3.9650|Â±  |0.7656|
|                                                          |       |none             |     0|rougeL_max |â†‘  |44.6201|Â±  |0.8785|
|truthfulqa_mc1                                            |      2|none             |     0|acc        |â†‘  | 0.3525|Â±  |0.0167|
|truthfulqa_mc2                                            |      3|none             |     0|acc        |â†‘  | 0.4976|Â±  |0.0152|
|winogrande                                                |      1|none             |     0|acc        |â†‘  | 0.7632|Â±  |0.0119|

|      Groups      |Version|  Filter  |n-shot|  Metric   |   |Value |   |Stderr|
|------------------|------:|----------|------|-----------|---|-----:|---|-----:|
|bbh               |      3|get-answer|      |exact_match|â†‘  |0.6179|Â±  |0.0054|
|mmlu              |      2|none      |      |acc        |â†‘  |0.6431|Â±  |0.0038|
| - humanities     |      2|none      |      |acc        |â†‘  |0.5966|Â±  |0.0068|
| - other          |      2|none      |      |acc        |â†‘  |0.7071|Â±  |0.0079|
| - social sciences|      2|none      |      |acc        |â†‘  |0.7452|Â±  |0.0077|
| - stem           |      2|none      |      |acc        |â†‘  |0.5496|Â±  |0.0086|

openchat_openchat-3.6-8b-20240522: 7h 51m 28s
âœ… Benchmark completed for openchat_openchat-3.6-8b-20240522

ðŸ”¥ Starting benchmark for internlm_internlm2_5-7b-chat
internlm_internlm2_5-7b-chat: 0h 4m 58s
âœ… Benchmark completed for internlm_internlm2_5-7b-chat

ðŸ”¥ Starting benchmark for THUDM_chatglm3-6b
THUDM_chatglm3-6b: 0h 32m 25s
âœ… Benchmark completed for THUDM_chatglm3-6b

ðŸ”¥ Starting benchmark for NousResearch_Hermes-2-Pro-Mistral-7B
hf (pretrained=/mnt/data8tb/Documents/llm/llm_models/NousResearch_Hermes-2-Pro-Mistral-7B,trust_remote_code=True), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 3
|                          Tasks                           |Version|     Filter      |n-shot|  Metric   |   | Value |   |Stderr|
|----------------------------------------------------------|------:|-----------------|-----:|-----------|---|------:|---|-----:|
|anli_r1                                                   |      1|none             |     0|acc        |â†‘  | 0.5310|Â±  |0.0158|
|anli_r2                                                   |      1|none             |     0|acc        |â†‘  | 0.4960|Â±  |0.0158|
|anli_r3                                                   |      1|none             |     0|acc        |â†‘  | 0.5000|Â±  |0.0144|
|arc_challenge                                             |      1|none             |     0|acc        |â†‘  | 0.5444|Â±  |0.0146|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.5657|Â±  |0.0145|
|bbh                                                       |      3|get-answer       |      |exact_match|â†‘  | 0.5738|Â±  |0.0055|
| - bbh_cot_fewshot_boolean_expressions                    |      4|get-answer       |     3|exact_match|â†‘  | 0.8080|Â±  |0.0250|
| - bbh_cot_fewshot_causal_judgement                       |      4|get-answer       |     3|exact_match|â†‘  | 0.5615|Â±  |0.0364|
| - bbh_cot_fewshot_date_understanding                     |      4|get-answer       |     3|exact_match|â†‘  | 0.6320|Â±  |0.0306|
| - bbh_cot_fewshot_disambiguation_qa                      |      4|get-answer       |     3|exact_match|â†‘  | 0.6200|Â±  |0.0308|
| - bbh_cot_fewshot_dyck_languages                         |      4|get-answer       |     3|exact_match|â†‘  | 0.0720|Â±  |0.0164|
| - bbh_cot_fewshot_formal_fallacies                       |      4|get-answer       |     3|exact_match|â†‘  | 0.5360|Â±  |0.0316|
| - bbh_cot_fewshot_geometric_shapes                       |      4|get-answer       |     3|exact_match|â†‘  | 0.3920|Â±  |0.0309|
| - bbh_cot_fewshot_hyperbaton                             |      4|get-answer       |     3|exact_match|â†‘  | 0.8000|Â±  |0.0253|
| - bbh_cot_fewshot_logical_deduction_five_objects         |      4|get-answer       |     3|exact_match|â†‘  | 0.4640|Â±  |0.0316|
| - bbh_cot_fewshot_logical_deduction_seven_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.2240|Â±  |0.0264|
| - bbh_cot_fewshot_logical_deduction_three_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.8680|Â±  |0.0215|
| - bbh_cot_fewshot_movie_recommendation                   |      4|get-answer       |     3|exact_match|â†‘  | 0.7880|Â±  |0.0259|
| - bbh_cot_fewshot_multistep_arithmetic_two               |      4|get-answer       |     3|exact_match|â†‘  | 0.3800|Â±  |0.0308|
| - bbh_cot_fewshot_navigate                               |      4|get-answer       |     3|exact_match|â†‘  | 0.6440|Â±  |0.0303|
| - bbh_cot_fewshot_object_counting                        |      4|get-answer       |     3|exact_match|â†‘  | 0.6840|Â±  |0.0295|
| - bbh_cot_fewshot_penguins_in_a_table                    |      4|get-answer       |     3|exact_match|â†‘  | 0.5959|Â±  |0.0408|
| - bbh_cot_fewshot_reasoning_about_colored_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.6360|Â±  |0.0305|
| - bbh_cot_fewshot_ruin_names                             |      4|get-answer       |     3|exact_match|â†‘  | 0.6480|Â±  |0.0303|
| - bbh_cot_fewshot_salient_translation_error_detection    |      4|get-answer       |     3|exact_match|â†‘  | 0.5600|Â±  |0.0315|
| - bbh_cot_fewshot_snarks                                 |      4|get-answer       |     3|exact_match|â†‘  | 0.7022|Â±  |0.0344|
| - bbh_cot_fewshot_sports_understanding                   |      4|get-answer       |     3|exact_match|â†‘  | 0.9400|Â±  |0.0151|
| - bbh_cot_fewshot_temporal_sequences                     |      4|get-answer       |     3|exact_match|â†‘  | 0.6480|Â±  |0.0303|
| - bbh_cot_fewshot_tracking_shuffled_objects_five_objects |      4|get-answer       |     3|exact_match|â†‘  | 0.3800|Â±  |0.0308|
| - bbh_cot_fewshot_tracking_shuffled_objects_seven_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.2720|Â±  |0.0282|
| - bbh_cot_fewshot_tracking_shuffled_objects_three_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.4840|Â±  |0.0317|
| - bbh_cot_fewshot_web_of_lies                            |      4|get-answer       |     3|exact_match|â†‘  | 0.9640|Â±  |0.0118|
| - bbh_cot_fewshot_word_sorting                           |      4|get-answer       |     3|exact_match|â†‘  | 0.2320|Â±  |0.0268|
|boolq                                                     |      2|none             |     0|acc        |â†‘  | 0.8682|Â±  |0.0059|
|drop                                                      |      3|none             |     0|em         |â†‘  | 0.0167|Â±  |0.0013|
|                                                          |       |none             |     0|f1         |â†‘  | 0.1098|Â±  |0.0022|
|gpqa_diamond_cot_n_shot                                   |      2|flexible-extract |     0|exact_match|â†‘  | 0.1162|Â±  |0.0228|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_cot_zeroshot                                 |      1|flexible-extract |     0|exact_match|â†‘  | 0.0960|Â±  |0.0210|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_generative_n_shot                            |      2|flexible-extract |     0|exact_match|â†‘  | 0.2525|Â±  |0.0310|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_n_shot                                       |      2|none             |     0|acc        |â†‘  | 0.3232|Â±  |0.0333|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3232|Â±  |0.0333|
|gpqa_diamond_zeroshot                                     |      1|none             |     0|acc        |â†‘  | 0.2677|Â±  |0.0315|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2677|Â±  |0.0315|
|gpqa_extended_cot_n_shot                                  |      2|flexible-extract |     0|exact_match|â†‘  | 0.1685|Â±  |0.0160|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_cot_zeroshot                                |      1|flexible-extract |     0|exact_match|â†‘  | 0.1520|Â±  |0.0154|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_generative_n_shot                           |      2|flexible-extract |     0|exact_match|â†‘  | 0.2637|Â±  |0.0189|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_n_shot                                      |      2|none             |     0|acc        |â†‘  | 0.2875|Â±  |0.0194|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2875|Â±  |0.0194|
|gpqa_extended_zeroshot                                    |      1|none             |     0|acc        |â†‘  | 0.2949|Â±  |0.0195|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2949|Â±  |0.0195|
|gpqa_main_cot_n_shot                                      |      2|flexible-extract |     0|exact_match|â†‘  | 0.1317|Â±  |0.0160|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_cot_zeroshot                                    |      1|flexible-extract |     0|exact_match|â†‘  | 0.1228|Â±  |0.0155|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_generative_n_shot                               |      2|flexible-extract |     0|exact_match|â†‘  | 0.2366|Â±  |0.0201|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_n_shot                                          |      2|none             |     0|acc        |â†‘  | 0.2991|Â±  |0.0217|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2991|Â±  |0.0217|
|gpqa_main_zeroshot                                        |      1|none             |     0|acc        |â†‘  | 0.2768|Â±  |0.0212|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2768|Â±  |0.0212|
|gsm8k                                                     |      3|flexible-extract |     5|exact_match|â†‘  | 0.6861|Â±  |0.0128|
|                                                          |       |strict-match     |     5|exact_match|â†‘  | 0.6854|Â±  |0.0128|
|hellaswag                                                 |      1|none             |     0|acc        |â†‘  | 0.6270|Â±  |0.0048|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.8049|Â±  |0.0040|
|mmlu                                                      |      2|none             |      |acc        |â†‘  | 0.6051|Â±  |0.0039|
| - humanities                                             |      2|none             |      |acc        |â†‘  | 0.5484|Â±  |0.0067|
|  - formal_logic                                          |      1|none             |     0|acc        |â†‘  | 0.4127|Â±  |0.0440|
|  - high_school_european_history                          |      1|none             |     0|acc        |â†‘  | 0.7697|Â±  |0.0329|
|  - high_school_us_history                                |      1|none             |     0|acc        |â†‘  | 0.7990|Â±  |0.0281|
|  - high_school_world_history                             |      1|none             |     0|acc        |â†‘  | 0.8354|Â±  |0.0241|
|  - international_law                                     |      1|none             |     0|acc        |â†‘  | 0.7769|Â±  |0.0380|
|  - jurisprudence                                         |      1|none             |     0|acc        |â†‘  | 0.7130|Â±  |0.0437|
|  - logical_fallacies                                     |      1|none             |     0|acc        |â†‘  | 0.7178|Â±  |0.0354|
|  - moral_disputes                                        |      1|none             |     0|acc        |â†‘  | 0.6821|Â±  |0.0251|
|  - moral_scenarios                                       |      1|none             |     0|acc        |â†‘  | 0.2492|Â±  |0.0145|
|  - philosophy                                            |      1|none             |     0|acc        |â†‘  | 0.6752|Â±  |0.0266|
|  - prehistory                                            |      1|none             |     0|acc        |â†‘  | 0.7253|Â±  |0.0248|
|  - professional_law                                      |      1|none             |     0|acc        |â†‘  | 0.4622|Â±  |0.0127|
|  - world_religions                                       |      1|none             |     0|acc        |â†‘  | 0.8129|Â±  |0.0299|
| - other                                                  |      2|none             |      |acc        |â†‘  | 0.6807|Â±  |0.0081|
|  - business_ethics                                       |      1|none             |     0|acc        |â†‘  | 0.5600|Â±  |0.0499|
|  - clinical_knowledge                                    |      1|none             |     0|acc        |â†‘  | 0.6755|Â±  |0.0288|
|  - college_medicine                                      |      1|none             |     0|acc        |â†‘  | 0.6301|Â±  |0.0368|
|  - global_facts                                          |      1|none             |     0|acc        |â†‘  | 0.4000|Â±  |0.0492|
|  - human_aging                                           |      1|none             |     0|acc        |â†‘  | 0.6771|Â±  |0.0314|
|  - management                                            |      1|none             |     0|acc        |â†‘  | 0.7864|Â±  |0.0406|
|  - marketing                                             |      1|none             |     0|acc        |â†‘  | 0.8632|Â±  |0.0225|
|  - medical_genetics                                      |      1|none             |     0|acc        |â†‘  | 0.6500|Â±  |0.0479|
|  - miscellaneous                                         |      1|none             |     0|acc        |â†‘  | 0.8020|Â±  |0.0142|
|  - nutrition                                             |      1|none             |     0|acc        |â†‘  | 0.6928|Â±  |0.0264|
|  - professional_accounting                               |      1|none             |     0|acc        |â†‘  | 0.4362|Â±  |0.0296|
|  - professional_medicine                                 |      1|none             |     0|acc        |â†‘  | 0.6765|Â±  |0.0284|
|  - virology                                              |      1|none             |     0|acc        |â†‘  | 0.5120|Â±  |0.0389|
| - social sciences                                        |      2|none             |      |acc        |â†‘  | 0.7082|Â±  |0.0080|
|  - econometrics                                          |      1|none             |     0|acc        |â†‘  | 0.3860|Â±  |0.0458|
|  - high_school_geography                                 |      1|none             |     0|acc        |â†‘  | 0.7424|Â±  |0.0312|
|  - high_school_government_and_politics                   |      1|none             |     0|acc        |â†‘  | 0.8446|Â±  |0.0261|
|  - high_school_macroeconomics                            |      1|none             |     0|acc        |â†‘  | 0.6256|Â±  |0.0245|
|  - high_school_microeconomics                            |      1|none             |     0|acc        |â†‘  | 0.6681|Â±  |0.0306|
|  - high_school_psychology                                |      1|none             |     0|acc        |â†‘  | 0.8239|Â±  |0.0163|
|  - human_sexuality                                       |      1|none             |     0|acc        |â†‘  | 0.7328|Â±  |0.0388|
|  - professional_psychology                               |      1|none             |     0|acc        |â†‘  | 0.6373|Â±  |0.0195|
|  - public_relations                                      |      1|none             |     0|acc        |â†‘  | 0.5909|Â±  |0.0471|
|  - security_studies                                      |      1|none             |     0|acc        |â†‘  | 0.7020|Â±  |0.0293|
|  - sociology                                             |      1|none             |     0|acc        |â†‘  | 0.8159|Â±  |0.0274|
|  - us_foreign_policy                                     |      1|none             |     0|acc        |â†‘  | 0.8600|Â±  |0.0349|
| - stem                                                   |      2|none             |      |acc        |â†‘  | 0.5147|Â±  |0.0086|
|  - abstract_algebra                                      |      1|none             |     0|acc        |â†‘  | 0.3200|Â±  |0.0469|
|  - anatomy                                               |      1|none             |     0|acc        |â†‘  | 0.5704|Â±  |0.0428|
|  - astronomy                                             |      1|none             |     0|acc        |â†‘  | 0.6382|Â±  |0.0391|
|  - college_biology                                       |      1|none             |     0|acc        |â†‘  | 0.7083|Â±  |0.0380|
|  - college_chemistry                                     |      1|none             |     0|acc        |â†‘  | 0.4300|Â±  |0.0498|
|  - college_computer_science                              |      1|none             |     0|acc        |â†‘  | 0.5100|Â±  |0.0502|
|  - college_mathematics                                   |      1|none             |     0|acc        |â†‘  | 0.2800|Â±  |0.0451|
|  - college_physics                                       |      1|none             |     0|acc        |â†‘  | 0.4216|Â±  |0.0491|
|  - computer_security                                     |      1|none             |     0|acc        |â†‘  | 0.7500|Â±  |0.0435|
|  - conceptual_physics                                    |      1|none             |     0|acc        |â†‘  | 0.5617|Â±  |0.0324|
|  - electrical_engineering                                |      1|none             |     0|acc        |â†‘  | 0.4828|Â±  |0.0416|
|  - elementary_mathematics                                |      1|none             |     0|acc        |â†‘  | 0.4471|Â±  |0.0256|
|  - high_school_biology                                   |      1|none             |     0|acc        |â†‘  | 0.7387|Â±  |0.0250|
|  - high_school_chemistry                                 |      1|none             |     0|acc        |â†‘  | 0.4828|Â±  |0.0352|
|  - high_school_computer_science                          |      1|none             |     0|acc        |â†‘  | 0.6400|Â±  |0.0482|
|  - high_school_mathematics                               |      1|none             |     0|acc        |â†‘  | 0.3704|Â±  |0.0294|
|  - high_school_physics                                   |      1|none             |     0|acc        |â†‘  | 0.3377|Â±  |0.0386|
|  - high_school_statistics                                |      1|none             |     0|acc        |â†‘  | 0.4954|Â±  |0.0341|
|  - machine_learning                                      |      1|none             |     0|acc        |â†‘  | 0.4911|Â±  |0.0475|
|nq_open                                                   |      4|remove_whitespace|     0|exact_match|â†‘  | 0.0404|Â±  |0.0033|
|openbookqa                                                |      1|none             |     0|acc        |â†‘  | 0.3380|Â±  |0.0212|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.4340|Â±  |0.0222|
|piqa                                                      |      1|none             |     0|acc        |â†‘  | 0.7938|Â±  |0.0094|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.7987|Â±  |0.0094|
|qnli                                                      |      1|none             |     0|acc        |â†‘  | 0.5565|Â±  |0.0067|
|sciq                                                      |      1|none             |     0|acc        |â†‘  | 0.9500|Â±  |0.0069|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.9170|Â±  |0.0087|
|triviaqa                                                  |      3|remove_whitespace|     0|exact_match|â†‘  | 0.4711|Â±  |0.0037|
|truthfulqa_gen                                            |      3|none             |     0|bleu_acc   |â†‘  | 0.5606|Â±  |0.0174|
|                                                          |       |none             |     0|bleu_diff  |â†‘  | 8.7131|Â±  |0.8710|
|                                                          |       |none             |     0|bleu_max   |â†‘  |27.5391|Â±  |0.8368|
|                                                          |       |none             |     0|rouge1_acc |â†‘  | 0.5887|Â±  |0.0172|
|                                                          |       |none             |     0|rouge1_diff|â†‘  |12.4445|Â±  |1.2143|
|                                                          |       |none             |     0|rouge1_max |â†‘  |54.1696|Â±  |0.8927|
|                                                          |       |none             |     0|rouge2_acc |â†‘  | 0.4994|Â±  |0.0175|
|                                                          |       |none             |     0|rouge2_diff|â†‘  |12.4661|Â±  |1.2976|
|                                                          |       |none             |     0|rouge2_max |â†‘  |40.9144|Â±  |1.0592|
|                                                          |       |none             |     0|rougeL_acc |â†‘  | 0.5569|Â±  |0.0174|
|                                                          |       |none             |     0|rougeL_diff|â†‘  |11.9317|Â±  |1.2333|
|                                                          |       |none             |     0|rougeL_max |â†‘  |50.9560|Â±  |0.9357|
|truthfulqa_mc1                                            |      2|none             |     0|acc        |â†‘  | 0.4137|Â±  |0.0172|
|truthfulqa_mc2                                            |      3|none             |     0|acc        |â†‘  | 0.5912|Â±  |0.0158|
|winogrande                                                |      1|none             |     0|acc        |â†‘  | 0.7198|Â±  |0.0126|

|      Groups      |Version|  Filter  |n-shot|  Metric   |   |Value |   |Stderr|
|------------------|------:|----------|------|-----------|---|-----:|---|-----:|
|bbh               |      3|get-answer|      |exact_match|â†‘  |0.5738|Â±  |0.0055|
|mmlu              |      2|none      |      |acc        |â†‘  |0.6051|Â±  |0.0039|
| - humanities     |      2|none      |      |acc        |â†‘  |0.5484|Â±  |0.0067|
| - other          |      2|none      |      |acc        |â†‘  |0.6807|Â±  |0.0081|
| - social sciences|      2|none      |      |acc        |â†‘  |0.7082|Â±  |0.0080|
| - stem           |      2|none      |      |acc        |â†‘  |0.5147|Â±  |0.0086|

NousResearch_Hermes-2-Pro-Mistral-7B: 8h 27m 23s
âœ… Benchmark completed for NousResearch_Hermes-2-Pro-Mistral-7B

ðŸ”¥ Starting benchmark for deepseek-ai_deepseek-moe-16b-base
deepseek-ai_deepseek-moe-16b-base: 0h 0m 13s
âœ… Benchmark completed for deepseek-ai_deepseek-moe-16b-base

ðŸ”¥ Starting benchmark for deepseek-ai_deepseek-moe-16b-chat
ðŸ”¥ Starting benchmark for baichuan-inc_Baichuan-M1-14B-Instruct
baichuan-inc_Baichuan-M1-14B-Instruct: 0h 0m 4s
âœ… Benchmark completed for baichuan-inc_Baichuan-M1-14B-Instruct

ðŸ”¥ Starting benchmark for baichuan-inc_Baichuan2-13B-Chat
ðŸ”¥ Starting benchmark for baichuan-inc_Baichuan2-13B-Chat
ðŸ”¥ Starting benchmark for baichuan-inc_Baichuan-M1-14B-Instruct
baichuan-inc_Baichuan-M1-14B-Instruct: 0h 0m 4s
âœ… Benchmark completed for baichuan-inc_Baichuan-M1-14B-Instruct

ðŸ”¥ Starting benchmark for moonshotai_Moonlight-16B-A3B-Instruct
ðŸ”¥ Starting benchmark for moonshotai_Moonlight-16B-A3B-Instruct
moonshotai_Moonlight-16B-A3B-Instruct: 0h 0m 3s
âœ… Benchmark completed for moonshotai_Moonlight-16B-A3B-Instruct

ðŸ”¥ Starting benchmark for moonshotai_Moonlight-16B-A3B
ðŸ”¥ Starting benchmark for moonshotai_Moonlight-16B-A3B-Instruct
ðŸ”¥ Starting benchmark for Qwen_Qwen3-14B
ðŸ”¥ Starting benchmark for Qwen_Qwen2.5-14B-Instruct
ðŸ”¥ Starting benchmark for Qwen_Qwen-7B-Chat
Qwen_Qwen-7B-Chat: 0h 4m 49s
âœ… Benchmark completed for Qwen_Qwen-7B-Chat

ðŸ”¥ Starting benchmark for Qwen_Qwen-7B
Qwen_Qwen-7B: 0h 4m 53s
âœ… Benchmark completed for Qwen_Qwen-7B

ðŸ”¥ Starting benchmark for baichuan-inc_Baichuan2-13B-Chat
baichuan-inc_Baichuan2-13B-Chat: 0h 5m 31s
âœ… Benchmark completed for baichuan-inc_Baichuan2-13B-Chat

ðŸ”¥ Starting benchmark for moonshotai_Moonlight-16B-A3B-Instruct
moonshotai_Moonlight-16B-A3B-Instruct: 0h 5m 27s
âœ… Benchmark completed for moonshotai_Moonlight-16B-A3B-Instruct

ðŸ”¥ Starting benchmark for moonshotai_Moonlight-16B-A3B
moonshotai_Moonlight-16B-A3B: 0h 5m 26s
âœ… Benchmark completed for moonshotai_Moonlight-16B-A3B

ðŸ”¥ Starting benchmark for Qwen_Qwen3-14B
Qwen_Qwen3-14B: 0h 5m 11s
âœ… Benchmark completed for Qwen_Qwen3-14B

ðŸ”¥ Starting benchmark for Qwen_Qwen2.5-14B-Instruct
ðŸ”¥ Starting benchmark for openai-community_gpt2
ðŸ”¥ Starting benchmark for deepseek-ai_DeepSeek-R1-Distill-Qwen-1.5B
ðŸ”¥ Starting benchmark for openai-community_gpt2
ðŸ”¥ Starting benchmark for openai-community_gpt2
Passed argument batch_size = auto:4.0. Detecting largest batch size
Determined largest batch size: 32
Passed argument batch_size = auto:4.0. Detecting largest batch size
Determined largest batch size: 64
Passed argument batch_size = auto. Detecting largest batch size
Determined Largest batch size: 32
openai-community_gpt2: 0h 28m 26s
âœ… Benchmark completed for openai-community_gpt2

ðŸ”¥ Starting benchmark for openai-community_gpt2-medium
Passed argument batch_size = auto:4.0. Detecting largest batch size
Determined largest batch size: 28
Passed argument batch_size = auto:4.0. Detecting largest batch size
Determined largest batch size: 64
Passed argument batch_size = auto. Detecting largest batch size
Determined Largest batch size: 28
openai-community_gpt2-medium: 0h 51m 53s
âœ… Benchmark completed for openai-community_gpt2-medium

ðŸ”¥ Starting benchmark for openai-community_gpt2-large
Passed argument batch_size = auto:4.0. Detecting largest batch size
Determined largest batch size: 19
Passed argument batch_size = auto:4.0. Detecting largest batch size
Determined largest batch size: 64
Passed argument batch_size = auto. Detecting largest batch size
Determined Largest batch size: 19
openai-community_gpt2-large: 1h 27m 10s
âœ… Benchmark completed for openai-community_gpt2-large

ðŸ”¥ Starting benchmark for openai-community_gpt2-xl
Passed argument batch_size = auto:4.0. Detecting largest batch size
Determined largest batch size: 13
Passed argument batch_size = auto:4.0. Detecting largest batch size
Determined largest batch size: 64
Passed argument batch_size = auto. Detecting largest batch size
Determined Largest batch size: 13
openai-community_gpt2-xl: 2h 30m 7s
âœ… Benchmark completed for openai-community_gpt2-xl

ðŸ”¥ Starting benchmark for Qwen_Qwen2.5-Math-1.5B-Instruct
Passed argument batch_size = auto:4.0. Detecting largest batch size
Determined largest batch size: 6
Passed argument batch_size = auto:4.0. Detecting largest batch size
Determined largest batch size: 64
Passed argument batch_size = auto. Detecting largest batch size
Determined Largest batch size: 6
hf (pretrained=/mnt/data8tb/Documents/llm/llm_models/Qwen_Qwen2.5-Math-1.5B-Instruct,trust_remote_code=True), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto:4 (6,64,64,64,64)
|                          Tasks                           |Version|     Filter      |n-shot|  Metric   |   | Value |   |Stderr|
|----------------------------------------------------------|------:|-----------------|-----:|-----------|---|------:|---|-----:|
|anli_r1                                                   |      1|none             |     0|acc        |â†‘  | 0.3420|Â±  |0.0150|
|anli_r2                                                   |      1|none             |     0|acc        |â†‘  | 0.3410|Â±  |0.0150|
|anli_r3                                                   |      1|none             |     0|acc        |â†‘  | 0.3533|Â±  |0.0138|
|arc_challenge                                             |      1|none             |     0|acc        |â†‘  | 0.3336|Â±  |0.0138|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3652|Â±  |0.0141|
|bbh                                                       |      3|get-answer       |      |exact_match|â†‘  | 0.4373|Â±  |0.0052|
| - bbh_cot_fewshot_boolean_expressions                    |      4|get-answer       |     3|exact_match|â†‘  | 0.8760|Â±  |0.0209|
| - bbh_cot_fewshot_causal_judgement                       |      4|get-answer       |     3|exact_match|â†‘  | 0.4706|Â±  |0.0366|
| - bbh_cot_fewshot_date_understanding                     |      4|get-answer       |     3|exact_match|â†‘  | 0.5680|Â±  |0.0314|
| - bbh_cot_fewshot_disambiguation_qa                      |      4|get-answer       |     3|exact_match|â†‘  | 0.5280|Â±  |0.0316|
| - bbh_cot_fewshot_dyck_languages                         |      4|get-answer       |     3|exact_match|â†‘  | 0.0000|Â±  |0.0000|
| - bbh_cot_fewshot_formal_fallacies                       |      4|get-answer       |     3|exact_match|â†‘  | 0.4960|Â±  |0.0317|
| - bbh_cot_fewshot_geometric_shapes                       |      4|get-answer       |     3|exact_match|â†‘  | 0.1440|Â±  |0.0222|
| - bbh_cot_fewshot_hyperbaton                             |      4|get-answer       |     3|exact_match|â†‘  | 0.5360|Â±  |0.0316|
| - bbh_cot_fewshot_logical_deduction_five_objects         |      4|get-answer       |     3|exact_match|â†‘  | 0.2960|Â±  |0.0289|
| - bbh_cot_fewshot_logical_deduction_seven_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.2280|Â±  |0.0266|
| - bbh_cot_fewshot_logical_deduction_three_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.7040|Â±  |0.0289|
| - bbh_cot_fewshot_movie_recommendation                   |      4|get-answer       |     3|exact_match|â†‘  | 0.2840|Â±  |0.0286|
| - bbh_cot_fewshot_multistep_arithmetic_two               |      4|get-answer       |     3|exact_match|â†‘  | 0.8080|Â±  |0.0250|
| - bbh_cot_fewshot_navigate                               |      4|get-answer       |     3|exact_match|â†‘  | 0.8680|Â±  |0.0215|
| - bbh_cot_fewshot_object_counting                        |      4|get-answer       |     3|exact_match|â†‘  | 0.6800|Â±  |0.0296|
| - bbh_cot_fewshot_penguins_in_a_table                    |      4|get-answer       |     3|exact_match|â†‘  | 0.6027|Â±  |0.0406|
| - bbh_cot_fewshot_reasoning_about_colored_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.5760|Â±  |0.0313|
| - bbh_cot_fewshot_ruin_names                             |      4|get-answer       |     3|exact_match|â†‘  | 0.0480|Â±  |0.0135|
| - bbh_cot_fewshot_salient_translation_error_detection    |      4|get-answer       |     3|exact_match|â†‘  | 0.2120|Â±  |0.0259|
| - bbh_cot_fewshot_snarks                                 |      4|get-answer       |     3|exact_match|â†‘  | 0.4775|Â±  |0.0375|
| - bbh_cot_fewshot_sports_understanding                   |      4|get-answer       |     3|exact_match|â†‘  | 0.4800|Â±  |0.0317|
| - bbh_cot_fewshot_temporal_sequences                     |      4|get-answer       |     3|exact_match|â†‘  | 0.1840|Â±  |0.0246|
| - bbh_cot_fewshot_tracking_shuffled_objects_five_objects |      4|get-answer       |     3|exact_match|â†‘  | 0.2680|Â±  |0.0281|
| - bbh_cot_fewshot_tracking_shuffled_objects_seven_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.2040|Â±  |0.0255|
| - bbh_cot_fewshot_tracking_shuffled_objects_three_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.5200|Â±  |0.0317|
| - bbh_cot_fewshot_web_of_lies                            |      4|get-answer       |     3|exact_match|â†‘  | 0.8120|Â±  |0.0248|
| - bbh_cot_fewshot_word_sorting                           |      4|get-answer       |     3|exact_match|â†‘  | 0.0240|Â±  |0.0097|
|boolq                                                     |      2|none             |     0|acc        |â†‘  | 0.5694|Â±  |0.0087|
|drop                                                      |      3|none             |     0|em         |â†‘  | 0.0002|Â±  |0.0001|
|                                                          |       |none             |     0|f1         |â†‘  | 0.0231|Â±  |0.0007|
|gpqa_diamond_cot_n_shot                                   |      2|flexible-extract |     0|exact_match|â†‘  | 0.1162|Â±  |0.0228|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_cot_zeroshot                                 |      1|flexible-extract |     0|exact_match|â†‘  | 0.0960|Â±  |0.0210|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_generative_n_shot                            |      2|flexible-extract |     0|exact_match|â†‘  | 0.2172|Â±  |0.0294|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0051|Â±  |0.0051|
|gpqa_diamond_n_shot                                       |      2|none             |     0|acc        |â†‘  | 0.2828|Â±  |0.0321|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2828|Â±  |0.0321|
|gpqa_diamond_zeroshot                                     |      1|none             |     0|acc        |â†‘  | 0.3030|Â±  |0.0327|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3030|Â±  |0.0327|
|gpqa_extended_cot_n_shot                                  |      2|flexible-extract |     0|exact_match|â†‘  | 0.0971|Â±  |0.0127|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_cot_zeroshot                                |      1|flexible-extract |     0|exact_match|â†‘  | 0.1282|Â±  |0.0143|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_generative_n_shot                           |      2|flexible-extract |     0|exact_match|â†‘  | 0.2527|Â±  |0.0186|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0018|Â±  |0.0018|
|gpqa_extended_n_shot                                      |      2|none             |     0|acc        |â†‘  | 0.3168|Â±  |0.0199|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3168|Â±  |0.0199|
|gpqa_extended_zeroshot                                    |      1|none             |     0|acc        |â†‘  | 0.2821|Â±  |0.0193|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2821|Â±  |0.0193|
|gpqa_main_cot_n_shot                                      |      2|flexible-extract |     0|exact_match|â†‘  | 0.1004|Â±  |0.0142|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_cot_zeroshot                                    |      1|flexible-extract |     0|exact_match|â†‘  | 0.1094|Â±  |0.0148|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_generative_n_shot                               |      2|flexible-extract |     0|exact_match|â†‘  | 0.2433|Â±  |0.0203|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_n_shot                                          |      2|none             |     0|acc        |â†‘  | 0.3013|Â±  |0.0217|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3013|Â±  |0.0217|
|gpqa_main_zeroshot                                        |      1|none             |     0|acc        |â†‘  | 0.2835|Â±  |0.0213|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2835|Â±  |0.0213|
|gsm8k                                                     |      3|flexible-extract |     5|exact_match|â†‘  | 0.7415|Â±  |0.0121|
|                                                          |       |strict-match     |     5|exact_match|â†‘  | 0.7369|Â±  |0.0121|
|hellaswag                                                 |      1|none             |     0|acc        |â†‘  | 0.3530|Â±  |0.0048|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.4166|Â±  |0.0049|
|mmlu                                                      |      2|none             |      |acc        |â†‘  | 0.3788|Â±  |0.0041|
| - humanities                                             |      2|none             |      |acc        |â†‘  | 0.3271|Â±  |0.0068|
|  - formal_logic                                          |      1|none             |     0|acc        |â†‘  | 0.4762|Â±  |0.0447|
|  - high_school_european_history                          |      1|none             |     0|acc        |â†‘  | 0.3758|Â±  |0.0378|
|  - high_school_us_history                                |      1|none             |     0|acc        |â†‘  | 0.3873|Â±  |0.0342|
|  - high_school_world_history                             |      1|none             |     0|acc        |â†‘  | 0.3755|Â±  |0.0315|
|  - international_law                                     |      1|none             |     0|acc        |â†‘  | 0.4959|Â±  |0.0456|
|  - jurisprudence                                         |      1|none             |     0|acc        |â†‘  | 0.3241|Â±  |0.0452|
|  - logical_fallacies                                     |      1|none             |     0|acc        |â†‘  | 0.3988|Â±  |0.0385|
|  - moral_disputes                                        |      1|none             |     0|acc        |â†‘  | 0.3699|Â±  |0.0260|
|  - moral_scenarios                                       |      1|none             |     0|acc        |â†‘  | 0.2469|Â±  |0.0144|
|  - philosophy                                            |      1|none             |     0|acc        |â†‘  | 0.3826|Â±  |0.0276|
|  - prehistory                                            |      1|none             |     0|acc        |â†‘  | 0.3457|Â±  |0.0265|
|  - professional_law                                      |      1|none             |     0|acc        |â†‘  | 0.3025|Â±  |0.0117|
|  - world_religions                                       |      1|none             |     0|acc        |â†‘  | 0.2632|Â±  |0.0338|
| - other                                                  |      2|none             |      |acc        |â†‘  | 0.3746|Â±  |0.0086|
|  - business_ethics                                       |      1|none             |     0|acc        |â†‘  | 0.2900|Â±  |0.0456|
|  - clinical_knowledge                                    |      1|none             |     0|acc        |â†‘  | 0.3736|Â±  |0.0298|
|  - college_medicine                                      |      1|none             |     0|acc        |â†‘  | 0.4046|Â±  |0.0374|
|  - global_facts                                          |      1|none             |     0|acc        |â†‘  | 0.2600|Â±  |0.0441|
|  - human_aging                                           |      1|none             |     0|acc        |â†‘  | 0.3677|Â±  |0.0324|
|  - management                                            |      1|none             |     0|acc        |â†‘  | 0.4951|Â±  |0.0495|
|  - marketing                                             |      1|none             |     0|acc        |â†‘  | 0.5513|Â±  |0.0326|
|  - medical_genetics                                      |      1|none             |     0|acc        |â†‘  | 0.4700|Â±  |0.0502|
|  - miscellaneous                                         |      1|none             |     0|acc        |â†‘  | 0.3729|Â±  |0.0173|
|  - nutrition                                             |      1|none             |     0|acc        |â†‘  | 0.4281|Â±  |0.0283|
|  - professional_accounting                               |      1|none             |     0|acc        |â†‘  | 0.3014|Â±  |0.0274|
|  - professional_medicine                                 |      1|none             |     0|acc        |â†‘  | 0.2426|Â±  |0.0260|
|  - virology                                              |      1|none             |     0|acc        |â†‘  | 0.3434|Â±  |0.0370|
| - social sciences                                        |      2|none             |      |acc        |â†‘  | 0.4127|Â±  |0.0088|
|  - econometrics                                          |      1|none             |     0|acc        |â†‘  | 0.3596|Â±  |0.0451|
|  - high_school_geography                                 |      1|none             |     0|acc        |â†‘  | 0.3434|Â±  |0.0338|
|  - high_school_government_and_politics                   |      1|none             |     0|acc        |â†‘  | 0.3627|Â±  |0.0347|
|  - high_school_macroeconomics                            |      1|none             |     0|acc        |â†‘  | 0.4282|Â±  |0.0251|
|  - high_school_microeconomics                            |      1|none             |     0|acc        |â†‘  | 0.4874|Â±  |0.0325|
|  - high_school_psychology                                |      1|none             |     0|acc        |â†‘  | 0.4532|Â±  |0.0213|
|  - human_sexuality                                       |      1|none             |     0|acc        |â†‘  | 0.3893|Â±  |0.0428|
|  - professional_psychology                               |      1|none             |     0|acc        |â†‘  | 0.3578|Â±  |0.0194|
|  - public_relations                                      |      1|none             |     0|acc        |â†‘  | 0.3727|Â±  |0.0463|
|  - security_studies                                      |      1|none             |     0|acc        |â†‘  | 0.4163|Â±  |0.0316|
|  - sociology                                             |      1|none             |     0|acc        |â†‘  | 0.4925|Â±  |0.0354|
|  - us_foreign_policy                                     |      1|none             |     0|acc        |â†‘  | 0.4900|Â±  |0.0502|
| - stem                                                   |      2|none             |      |acc        |â†‘  | 0.4269|Â±  |0.0088|
|  - abstract_algebra                                      |      1|none             |     0|acc        |â†‘  | 0.4100|Â±  |0.0494|
|  - anatomy                                               |      1|none             |     0|acc        |â†‘  | 0.3185|Â±  |0.0402|
|  - astronomy                                             |      1|none             |     0|acc        |â†‘  | 0.3816|Â±  |0.0395|
|  - college_biology                                       |      1|none             |     0|acc        |â†‘  | 0.3125|Â±  |0.0388|
|  - college_chemistry                                     |      1|none             |     0|acc        |â†‘  | 0.3500|Â±  |0.0479|
|  - college_computer_science                              |      1|none             |     0|acc        |â†‘  | 0.4900|Â±  |0.0502|
|  - college_mathematics                                   |      1|none             |     0|acc        |â†‘  | 0.4400|Â±  |0.0499|
|  - college_physics                                       |      1|none             |     0|acc        |â†‘  | 0.3431|Â±  |0.0472|
|  - computer_security                                     |      1|none             |     0|acc        |â†‘  | 0.4200|Â±  |0.0496|
|  - conceptual_physics                                    |      1|none             |     0|acc        |â†‘  | 0.4638|Â±  |0.0326|
|  - electrical_engineering                                |      1|none             |     0|acc        |â†‘  | 0.4759|Â±  |0.0416|
|  - elementary_mathematics                                |      1|none             |     0|acc        |â†‘  | 0.5344|Â±  |0.0257|
|  - high_school_biology                                   |      1|none             |     0|acc        |â†‘  | 0.4387|Â±  |0.0282|
|  - high_school_chemistry                                 |      1|none             |     0|acc        |â†‘  | 0.4187|Â±  |0.0347|
|  - high_school_computer_science                          |      1|none             |     0|acc        |â†‘  | 0.5500|Â±  |0.0500|
|  - high_school_mathematics                               |      1|none             |     0|acc        |â†‘  | 0.3778|Â±  |0.0296|
|  - high_school_physics                                   |      1|none             |     0|acc        |â†‘  | 0.3510|Â±  |0.0390|
|  - high_school_statistics                                |      1|none             |     0|acc        |â†‘  | 0.4583|Â±  |0.0340|
|  - machine_learning                                      |      1|none             |     0|acc        |â†‘  | 0.3929|Â±  |0.0464|
|nq_open                                                   |      4|remove_whitespace|     0|exact_match|â†‘  | 0.0039|Â±  |0.0010|
|openbookqa                                                |      1|none             |     0|acc        |â†‘  | 0.1980|Â±  |0.0178|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2860|Â±  |0.0202|
|piqa                                                      |      1|none             |     0|acc        |â†‘  | 0.6115|Â±  |0.0114|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.6137|Â±  |0.0114|
|qnli                                                      |      1|none             |     0|acc        |â†‘  | 0.4973|Â±  |0.0068|
|sciq                                                      |      1|none             |     0|acc        |â†‘  | 0.7550|Â±  |0.0136|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.7180|Â±  |0.0142|
|triviaqa                                                  |      3|remove_whitespace|     0|exact_match|â†‘  | 0.0043|Â±  |0.0005|
|truthfulqa_gen                                            |      3|none             |     0|bleu_acc   |â†‘  | 0.3672|Â±  |0.0169|
|                                                          |       |none             |     0|bleu_diff  |â†‘  |-0.3104|Â±  |0.3272|
|                                                          |       |none             |     0|bleu_max   |â†‘  |10.0271|Â±  |0.4450|
|                                                          |       |none             |     0|rouge1_acc |â†‘  | 0.4162|Â±  |0.0173|
|                                                          |       |none             |     0|rouge1_diff|â†‘  | 0.4141|Â±  |0.5503|
|                                                          |       |none             |     0|rouge1_max |â†‘  |30.4103|Â±  |0.7236|
|                                                          |       |none             |     0|rouge2_acc |â†‘  | 0.2632|Â±  |0.0154|
|                                                          |       |none             |     0|rouge2_diff|â†‘  |-0.8895|Â±  |0.6038|
|                                                          |       |none             |     0|rouge2_max |â†‘  |17.4804|Â±  |0.7257|
|                                                          |       |none             |     0|rougeL_acc |â†‘  | 0.4002|Â±  |0.0172|
|                                                          |       |none             |     0|rougeL_diff|â†‘  | 0.0440|Â±  |0.5522|
|                                                          |       |none             |     0|rougeL_max |â†‘  |28.4832|Â±  |0.7201|
|truthfulqa_mc1                                            |      2|none             |     0|acc        |â†‘  | 0.2901|Â±  |0.0159|
|truthfulqa_mc2                                            |      3|none             |     0|acc        |â†‘  | 0.4895|Â±  |0.0159|
|winogrande                                                |      1|none             |     0|acc        |â†‘  | 0.5257|Â±  |0.0140|

|      Groups      |Version|  Filter  |n-shot|  Metric   |   |Value |   |Stderr|
|------------------|------:|----------|------|-----------|---|-----:|---|-----:|
|bbh               |      3|get-answer|      |exact_match|â†‘  |0.4373|Â±  |0.0052|
|mmlu              |      2|none      |      |acc        |â†‘  |0.3788|Â±  |0.0041|
| - humanities     |      2|none      |      |acc        |â†‘  |0.3271|Â±  |0.0068|
| - other          |      2|none      |      |acc        |â†‘  |0.3746|Â±  |0.0086|
| - social sciences|      2|none      |      |acc        |â†‘  |0.4127|Â±  |0.0088|
| - stem           |      2|none      |      |acc        |â†‘  |0.4269|Â±  |0.0088|

Qwen_Qwen2.5-Math-1.5B-Instruct: 3h 25m 33s
âœ… Benchmark completed for Qwen_Qwen2.5-Math-1.5B-Instruct

ðŸ”¥ Starting benchmark for Qwen_Qwen2.5-3B-Instruct
Passed argument batch_size = auto:4.0. Detecting largest batch size
Determined largest batch size: 2
Passed argument batch_size = auto:4.0. Detecting largest batch size
Determined largest batch size: 64
Passed argument batch_size = auto. Detecting largest batch size
Determined Largest batch size: 1
hf (pretrained=/mnt/data8tb/Documents/llm/llm_models/Qwen_Qwen2.5-3B-Instruct,trust_remote_code=True), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto:4 (2,64,64,64,64)
|                          Tasks                           |Version|     Filter      |n-shot|  Metric   |   | Value |   |Stderr|
|----------------------------------------------------------|------:|-----------------|-----:|-----------|---|------:|---|-----:|
|anli_r1                                                   |      1|none             |     0|acc        |â†‘  | 0.5620|Â±  |0.0157|
|anli_r2                                                   |      1|none             |     0|acc        |â†‘  | 0.4660|Â±  |0.0158|
|anli_r3                                                   |      1|none             |     0|acc        |â†‘  | 0.4942|Â±  |0.0144|
|arc_challenge                                             |      1|none             |     0|acc        |â†‘  | 0.4590|Â±  |0.0146|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.4821|Â±  |0.0146|
|bbh                                                       |      3|get-answer       |      |exact_match|â†‘  | 0.2491|Â±  |0.0041|
| - bbh_cot_fewshot_boolean_expressions                    |      4|get-answer       |     3|exact_match|â†‘  | 0.2320|Â±  |0.0268|
| - bbh_cot_fewshot_causal_judgement                       |      4|get-answer       |     3|exact_match|â†‘  | 0.0053|Â±  |0.0053|
| - bbh_cot_fewshot_date_understanding                     |      4|get-answer       |     3|exact_match|â†‘  | 0.4120|Â±  |0.0312|
| - bbh_cot_fewshot_disambiguation_qa                      |      4|get-answer       |     3|exact_match|â†‘  | 0.0000|Â±  |0.0000|
| - bbh_cot_fewshot_dyck_languages                         |      4|get-answer       |     3|exact_match|â†‘  | 0.0000|Â±  |0.0000|
| - bbh_cot_fewshot_formal_fallacies                       |      4|get-answer       |     3|exact_match|â†‘  | 0.0320|Â±  |0.0112|
| - bbh_cot_fewshot_geometric_shapes                       |      4|get-answer       |     3|exact_match|â†‘  | 0.2520|Â±  |0.0275|
| - bbh_cot_fewshot_hyperbaton                             |      4|get-answer       |     3|exact_match|â†‘  | 0.0000|Â±  |0.0000|
| - bbh_cot_fewshot_logical_deduction_five_objects         |      4|get-answer       |     3|exact_match|â†‘  | 0.1200|Â±  |0.0206|
| - bbh_cot_fewshot_logical_deduction_seven_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.0920|Â±  |0.0183|
| - bbh_cot_fewshot_logical_deduction_three_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.0520|Â±  |0.0141|
| - bbh_cot_fewshot_movie_recommendation                   |      4|get-answer       |     3|exact_match|â†‘  | 0.0440|Â±  |0.0130|
| - bbh_cot_fewshot_multistep_arithmetic_two               |      4|get-answer       |     3|exact_match|â†‘  | 0.0000|Â±  |0.0000|
| - bbh_cot_fewshot_navigate                               |      4|get-answer       |     3|exact_match|â†‘  | 0.8480|Â±  |0.0228|
| - bbh_cot_fewshot_object_counting                        |      4|get-answer       |     3|exact_match|â†‘  | 0.5280|Â±  |0.0316|
| - bbh_cot_fewshot_penguins_in_a_table                    |      4|get-answer       |     3|exact_match|â†‘  | 0.2329|Â±  |0.0351|
| - bbh_cot_fewshot_reasoning_about_colored_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.4040|Â±  |0.0311|
| - bbh_cot_fewshot_ruin_names                             |      4|get-answer       |     3|exact_match|â†‘  | 0.0000|Â±  |0.0000|
| - bbh_cot_fewshot_salient_translation_error_detection    |      4|get-answer       |     3|exact_match|â†‘  | 0.0160|Â±  |0.0080|
| - bbh_cot_fewshot_snarks                                 |      4|get-answer       |     3|exact_match|â†‘  | 0.0000|Â±  |0.0000|
| - bbh_cot_fewshot_sports_understanding                   |      4|get-answer       |     3|exact_match|â†‘  | 0.6800|Â±  |0.0296|
| - bbh_cot_fewshot_temporal_sequences                     |      4|get-answer       |     3|exact_match|â†‘  | 0.3720|Â±  |0.0306|
| - bbh_cot_fewshot_tracking_shuffled_objects_five_objects |      4|get-answer       |     3|exact_match|â†‘  | 0.3880|Â±  |0.0309|
| - bbh_cot_fewshot_tracking_shuffled_objects_seven_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.1160|Â±  |0.0203|
| - bbh_cot_fewshot_tracking_shuffled_objects_three_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.6400|Â±  |0.0304|
| - bbh_cot_fewshot_web_of_lies                            |      4|get-answer       |     3|exact_match|â†‘  | 1.0000|Â±  |0.0000|
| - bbh_cot_fewshot_word_sorting                           |      4|get-answer       |     3|exact_match|â†‘  | 0.1200|Â±  |0.0206|
|boolq                                                     |      2|none             |     0|acc        |â†‘  | 0.8012|Â±  |0.0070|
|drop                                                      |      3|none             |     0|em         |â†‘  | 0.0016|Â±  |0.0004|
|                                                          |       |none             |     0|f1         |â†‘  | 0.0773|Â±  |0.0014|
|gpqa_diamond_cot_n_shot                                   |      2|flexible-extract |     0|exact_match|â†‘  | 0.1010|Â±  |0.0215|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_cot_zeroshot                                 |      1|flexible-extract |     0|exact_match|â†‘  | 0.0909|Â±  |0.0205|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_generative_n_shot                            |      2|flexible-extract |     0|exact_match|â†‘  | 0.1667|Â±  |0.0266|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_n_shot                                       |      2|none             |     0|acc        |â†‘  | 0.3232|Â±  |0.0333|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3232|Â±  |0.0333|
|gpqa_diamond_zeroshot                                     |      1|none             |     0|acc        |â†‘  | 0.3081|Â±  |0.0329|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3081|Â±  |0.0329|
|gpqa_extended_cot_n_shot                                  |      2|flexible-extract |     0|exact_match|â†‘  | 0.1538|Â±  |0.0155|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_cot_zeroshot                                |      1|flexible-extract |     0|exact_match|â†‘  | 0.1081|Â±  |0.0133|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_generative_n_shot                           |      2|flexible-extract |     0|exact_match|â†‘  | 0.1996|Â±  |0.0171|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0018|Â±  |0.0018|
|gpqa_extended_n_shot                                      |      2|none             |     0|acc        |â†‘  | 0.2985|Â±  |0.0196|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2985|Â±  |0.0196|
|gpqa_extended_zeroshot                                    |      1|none             |     0|acc        |â†‘  | 0.3278|Â±  |0.0201|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3278|Â±  |0.0201|
|gpqa_main_cot_n_shot                                      |      2|flexible-extract |     0|exact_match|â†‘  | 0.1183|Â±  |0.0153|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_cot_zeroshot                                    |      1|flexible-extract |     0|exact_match|â†‘  | 0.1161|Â±  |0.0152|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_generative_n_shot                               |      2|flexible-extract |     0|exact_match|â†‘  | 0.1763|Â±  |0.0180|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_n_shot                                          |      2|none             |     0|acc        |â†‘  | 0.2812|Â±  |0.0213|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2812|Â±  |0.0213|
|gpqa_main_zeroshot                                        |      1|none             |     0|acc        |â†‘  | 0.3214|Â±  |0.0221|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3214|Â±  |0.0221|
|gsm8k                                                     |      3|flexible-extract |     5|exact_match|â†‘  | 0.6384|Â±  |0.0132|
|                                                          |       |strict-match     |     5|exact_match|â†‘  | 0.1016|Â±  |0.0083|
|hellaswag                                                 |      1|none             |     0|acc        |â†‘  | 0.5633|Â±  |0.0049|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.7491|Â±  |0.0043|
|mmlu                                                      |      2|none             |      |acc        |â†‘  | 0.6550|Â±  |0.0038|
| - humanities                                             |      2|none             |      |acc        |â†‘  | 0.5858|Â±  |0.0067|
|  - formal_logic                                          |      1|none             |     0|acc        |â†‘  | 0.4603|Â±  |0.0446|
|  - high_school_european_history                          |      1|none             |     0|acc        |â†‘  | 0.8061|Â±  |0.0309|
|  - high_school_us_history                                |      1|none             |     0|acc        |â†‘  | 0.8333|Â±  |0.0262|
|  - high_school_world_history                             |      1|none             |     0|acc        |â†‘  | 0.8523|Â±  |0.0231|
|  - international_law                                     |      1|none             |     0|acc        |â†‘  | 0.7851|Â±  |0.0375|
|  - jurisprudence                                         |      1|none             |     0|acc        |â†‘  | 0.7778|Â±  |0.0402|
|  - logical_fallacies                                     |      1|none             |     0|acc        |â†‘  | 0.7975|Â±  |0.0316|
|  - moral_disputes                                        |      1|none             |     0|acc        |â†‘  | 0.6763|Â±  |0.0252|
|  - moral_scenarios                                       |      1|none             |     0|acc        |â†‘  | 0.3374|Â±  |0.0158|
|  - philosophy                                            |      1|none             |     0|acc        |â†‘  | 0.7074|Â±  |0.0258|
|  - prehistory                                            |      1|none             |     0|acc        |â†‘  | 0.7315|Â±  |0.0247|
|  - professional_law                                      |      1|none             |     0|acc        |â†‘  | 0.4896|Â±  |0.0128|
|  - world_religions                                       |      1|none             |     0|acc        |â†‘  | 0.8187|Â±  |0.0295|
| - other                                                  |      2|none             |      |acc        |â†‘  | 0.7023|Â±  |0.0079|
|  - business_ethics                                       |      1|none             |     0|acc        |â†‘  | 0.7500|Â±  |0.0435|
|  - clinical_knowledge                                    |      1|none             |     0|acc        |â†‘  | 0.7094|Â±  |0.0279|
|  - college_medicine                                      |      1|none             |     0|acc        |â†‘  | 0.6474|Â±  |0.0364|
|  - global_facts                                          |      1|none             |     0|acc        |â†‘  | 0.3400|Â±  |0.0476|
|  - human_aging                                           |      1|none             |     0|acc        |â†‘  | 0.7040|Â±  |0.0306|
|  - management                                            |      1|none             |     0|acc        |â†‘  | 0.7864|Â±  |0.0406|
|  - marketing                                             |      1|none             |     0|acc        |â†‘  | 0.8846|Â±  |0.0209|
|  - medical_genetics                                      |      1|none             |     0|acc        |â†‘  | 0.7900|Â±  |0.0409|
|  - miscellaneous                                         |      1|none             |     0|acc        |â†‘  | 0.7982|Â±  |0.0144|
|  - nutrition                                             |      1|none             |     0|acc        |â†‘  | 0.7190|Â±  |0.0257|
|  - professional_accounting                               |      1|none             |     0|acc        |â†‘  | 0.5355|Â±  |0.0298|
|  - professional_medicine                                 |      1|none             |     0|acc        |â†‘  | 0.6360|Â±  |0.0292|
|  - virology                                              |      1|none             |     0|acc        |â†‘  | 0.4819|Â±  |0.0389|
| - social sciences                                        |      2|none             |      |acc        |â†‘  | 0.7602|Â±  |0.0076|
|  - econometrics                                          |      1|none             |     0|acc        |â†‘  | 0.4912|Â±  |0.0470|
|  - high_school_geography                                 |      1|none             |     0|acc        |â†‘  | 0.7828|Â±  |0.0294|
|  - high_school_government_and_politics                   |      1|none             |     0|acc        |â†‘  | 0.8653|Â±  |0.0246|
|  - high_school_macroeconomics                            |      1|none             |     0|acc        |â†‘  | 0.6846|Â±  |0.0236|
|  - high_school_microeconomics                            |      1|none             |     0|acc        |â†‘  | 0.7815|Â±  |0.0268|
|  - high_school_psychology                                |      1|none             |     0|acc        |â†‘  | 0.8459|Â±  |0.0155|
|  - human_sexuality                                       |      1|none             |     0|acc        |â†‘  | 0.7481|Â±  |0.0381|
|  - professional_psychology                               |      1|none             |     0|acc        |â†‘  | 0.7190|Â±  |0.0182|
|  - public_relations                                      |      1|none             |     0|acc        |â†‘  | 0.6818|Â±  |0.0446|
|  - security_studies                                      |      1|none             |     0|acc        |â†‘  | 0.7510|Â±  |0.0277|
|  - sociology                                             |      1|none             |     0|acc        |â†‘  | 0.8308|Â±  |0.0265|
|  - us_foreign_policy                                     |      1|none             |     0|acc        |â†‘  | 0.8300|Â±  |0.0378|
| - stem                                                   |      2|none             |      |acc        |â†‘  | 0.6089|Â±  |0.0085|
|  - abstract_algebra                                      |      1|none             |     0|acc        |â†‘  | 0.4900|Â±  |0.0502|
|  - anatomy                                               |      1|none             |     0|acc        |â†‘  | 0.6667|Â±  |0.0407|
|  - astronomy                                             |      1|none             |     0|acc        |â†‘  | 0.7368|Â±  |0.0358|
|  - college_biology                                       |      1|none             |     0|acc        |â†‘  | 0.7292|Â±  |0.0372|
|  - college_chemistry                                     |      1|none             |     0|acc        |â†‘  | 0.5000|Â±  |0.0503|
|  - college_computer_science                              |      1|none             |     0|acc        |â†‘  | 0.5900|Â±  |0.0494|
|  - college_mathematics                                   |      1|none             |     0|acc        |â†‘  | 0.3100|Â±  |0.0465|
|  - college_physics                                       |      1|none             |     0|acc        |â†‘  | 0.4804|Â±  |0.0497|
|  - computer_security                                     |      1|none             |     0|acc        |â†‘  | 0.7200|Â±  |0.0451|
|  - conceptual_physics                                    |      1|none             |     0|acc        |â†‘  | 0.6383|Â±  |0.0314|
|  - electrical_engineering                                |      1|none             |     0|acc        |â†‘  | 0.6483|Â±  |0.0398|
|  - elementary_mathematics                                |      1|none             |     0|acc        |â†‘  | 0.6032|Â±  |0.0252|
|  - high_school_biology                                   |      1|none             |     0|acc        |â†‘  | 0.8161|Â±  |0.0220|
|  - high_school_chemistry                                 |      1|none             |     0|acc        |â†‘  | 0.5813|Â±  |0.0347|
|  - high_school_computer_science                          |      1|none             |     0|acc        |â†‘  | 0.7500|Â±  |0.0435|
|  - high_school_mathematics                               |      1|none             |     0|acc        |â†‘  | 0.5000|Â±  |0.0305|
|  - high_school_physics                                   |      1|none             |     0|acc        |â†‘  | 0.4503|Â±  |0.0406|
|  - high_school_statistics                                |      1|none             |     0|acc        |â†‘  | 0.5880|Â±  |0.0336|
|  - machine_learning                                      |      1|none             |     0|acc        |â†‘  | 0.4911|Â±  |0.0475|
|nq_open                                                   |      4|remove_whitespace|     0|exact_match|â†‘  | 0.0083|Â±  |0.0015|
|openbookqa                                                |      1|none             |     0|acc        |â†‘  | 0.3320|Â±  |0.0211|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.4220|Â±  |0.0221|
|piqa                                                      |      1|none             |     0|acc        |â†‘  | 0.7786|Â±  |0.0097|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.7807|Â±  |0.0097|
|qnli                                                      |      1|none             |     0|acc        |â†‘  | 0.7979|Â±  |0.0054|
|sciq                                                      |      1|none             |     0|acc        |â†‘  | 0.9460|Â±  |0.0072|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.9130|Â±  |0.0089|
|triviaqa                                                  |      3|remove_whitespace|     0|exact_match|â†‘  | 0.3010|Â±  |0.0034|
|truthfulqa_gen                                            |      3|none             |     0|bleu_acc   |â†‘  | 0.4835|Â±  |0.0175|
|                                                          |       |none             |     0|bleu_diff  |â†‘  |-0.2750|Â±  |0.2661|
|                                                          |       |none             |     0|bleu_max   |â†‘  | 7.7071|Â±  |0.3537|
|                                                          |       |none             |     0|rouge1_acc |â†‘  | 0.4847|Â±  |0.0175|
|                                                          |       |none             |     0|rouge1_diff|â†‘  |-0.1394|Â±  |0.4206|
|                                                          |       |none             |     0|rouge1_max |â†‘  |26.4054|Â±  |0.5770|
|                                                          |       |none             |     0|rouge2_acc |â†‘  | 0.3880|Â±  |0.0171|
|                                                          |       |none             |     0|rouge2_diff|â†‘  |-0.7135|Â±  |0.4378|
|                                                          |       |none             |     0|rouge2_max |â†‘  |15.0737|Â±  |0.5493|
|                                                          |       |none             |     0|rougeL_acc |â†‘  | 0.4651|Â±  |0.0175|
|                                                          |       |none             |     0|rougeL_diff|â†‘  |-0.4360|Â±  |0.4149|
|                                                          |       |none             |     0|rougeL_max |â†‘  |23.4471|Â±  |0.5573|
|truthfulqa_mc1                                            |      2|none             |     0|acc        |â†‘  | 0.4162|Â±  |0.0173|
|truthfulqa_mc2                                            |      3|none             |     0|acc        |â†‘  | 0.5861|Â±  |0.0157|
|winogrande                                                |      1|none             |     0|acc        |â†‘  | 0.6930|Â±  |0.0130|

|      Groups      |Version|  Filter  |n-shot|  Metric   |   |Value |   |Stderr|
|------------------|------:|----------|------|-----------|---|-----:|---|-----:|
|bbh               |      3|get-answer|      |exact_match|â†‘  |0.2491|Â±  |0.0041|
|mmlu              |      2|none      |      |acc        |â†‘  |0.6550|Â±  |0.0038|
| - humanities     |      2|none      |      |acc        |â†‘  |0.5858|Â±  |0.0067|
| - other          |      2|none      |      |acc        |â†‘  |0.7023|Â±  |0.0079|
| - social sciences|      2|none      |      |acc        |â†‘  |0.7602|Â±  |0.0076|
| - stem           |      2|none      |      |acc        |â†‘  |0.6089|Â±  |0.0085|

Qwen_Qwen2.5-3B-Instruct: 7h 48m 19s
âœ… Benchmark completed for Qwen_Qwen2.5-3B-Instruct

ðŸ”¥ Starting benchmark for Qwen_Qwen2.5-1.5B-Instruct
Passed argument batch_size = auto:4.0. Detecting largest batch size
Determined largest batch size: 1
Passed argument batch_size = auto:4.0. Detecting largest batch size
Determined largest batch size: 64
Passed argument batch_size = auto. Detecting largest batch size
Determined Largest batch size: 1
hf (pretrained=/mnt/data8tb/Documents/llm/llm_models/Qwen_Qwen2.5-1.5B-Instruct,trust_remote_code=True), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: auto:4 (1,64,64,64,64)
|                          Tasks                           |Version|     Filter      |n-shot|  Metric   |   | Value |   |Stderr|
|----------------------------------------------------------|------:|-----------------|-----:|-----------|---|------:|---|-----:|
|anli_r1                                                   |      1|none             |     0|acc        |â†‘  | 0.4500|Â±  |0.0157|
|anli_r2                                                   |      1|none             |     0|acc        |â†‘  | 0.3940|Â±  |0.0155|
|anli_r3                                                   |      1|none             |     0|acc        |â†‘  | 0.4325|Â±  |0.0143|
|arc_challenge                                             |      1|none             |     0|acc        |â†‘  | 0.4394|Â±  |0.0145|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.4659|Â±  |0.0146|
|bbh                                                       |      3|get-answer       |      |exact_match|â†‘  | 0.3861|Â±  |0.0054|
| - bbh_cot_fewshot_boolean_expressions                    |      4|get-answer       |     3|exact_match|â†‘  | 0.8480|Â±  |0.0228|
| - bbh_cot_fewshot_causal_judgement                       |      4|get-answer       |     3|exact_match|â†‘  | 0.5080|Â±  |0.0367|
| - bbh_cot_fewshot_date_understanding                     |      4|get-answer       |     3|exact_match|â†‘  | 0.4040|Â±  |0.0311|
| - bbh_cot_fewshot_disambiguation_qa                      |      4|get-answer       |     3|exact_match|â†‘  | 0.3880|Â±  |0.0309|
| - bbh_cot_fewshot_dyck_languages                         |      4|get-answer       |     3|exact_match|â†‘  | 0.0200|Â±  |0.0089|
| - bbh_cot_fewshot_formal_fallacies                       |      4|get-answer       |     3|exact_match|â†‘  | 0.2280|Â±  |0.0266|
| - bbh_cot_fewshot_geometric_shapes                       |      4|get-answer       |     3|exact_match|â†‘  | 0.3120|Â±  |0.0294|
| - bbh_cot_fewshot_hyperbaton                             |      4|get-answer       |     3|exact_match|â†‘  | 0.5720|Â±  |0.0314|
| - bbh_cot_fewshot_logical_deduction_five_objects         |      4|get-answer       |     3|exact_match|â†‘  | 0.3040|Â±  |0.0292|
| - bbh_cot_fewshot_logical_deduction_seven_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.2280|Â±  |0.0266|
| - bbh_cot_fewshot_logical_deduction_three_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.5880|Â±  |0.0312|
| - bbh_cot_fewshot_movie_recommendation                   |      4|get-answer       |     3|exact_match|â†‘  | 0.5320|Â±  |0.0316|
| - bbh_cot_fewshot_multistep_arithmetic_two               |      4|get-answer       |     3|exact_match|â†‘  | 0.5920|Â±  |0.0311|
| - bbh_cot_fewshot_navigate                               |      4|get-answer       |     3|exact_match|â†‘  | 0.6640|Â±  |0.0299|
| - bbh_cot_fewshot_object_counting                        |      4|get-answer       |     3|exact_match|â†‘  | 0.5640|Â±  |0.0314|
| - bbh_cot_fewshot_penguins_in_a_table                    |      4|get-answer       |     3|exact_match|â†‘  | 0.4795|Â±  |0.0415|
| - bbh_cot_fewshot_reasoning_about_colored_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.5200|Â±  |0.0317|
| - bbh_cot_fewshot_ruin_names                             |      4|get-answer       |     3|exact_match|â†‘  | 0.3480|Â±  |0.0302|
| - bbh_cot_fewshot_salient_translation_error_detection    |      4|get-answer       |     3|exact_match|â†‘  | 0.3440|Â±  |0.0301|
| - bbh_cot_fewshot_snarks                                 |      4|get-answer       |     3|exact_match|â†‘  | 0.4831|Â±  |0.0376|
| - bbh_cot_fewshot_sports_understanding                   |      4|get-answer       |     3|exact_match|â†‘  | 0.6600|Â±  |0.0300|
| - bbh_cot_fewshot_temporal_sequences                     |      4|get-answer       |     3|exact_match|â†‘  | 0.1640|Â±  |0.0235|
| - bbh_cot_fewshot_tracking_shuffled_objects_five_objects |      4|get-answer       |     3|exact_match|â†‘  | 0.1440|Â±  |0.0222|
| - bbh_cot_fewshot_tracking_shuffled_objects_seven_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.1200|Â±  |0.0206|
| - bbh_cot_fewshot_tracking_shuffled_objects_three_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.4080|Â±  |0.0311|
| - bbh_cot_fewshot_web_of_lies                            |      4|get-answer       |     3|exact_match|â†‘  | 0.0560|Â±  |0.0146|
| - bbh_cot_fewshot_word_sorting                           |      4|get-answer       |     3|exact_match|â†‘  | 0.0440|Â±  |0.0130|
|boolq                                                     |      2|none             |     0|acc        |â†‘  | 0.7810|Â±  |0.0072|
|drop                                                      |      3|none             |     0|em         |â†‘  | 0.0018|Â±  |0.0004|
|                                                          |       |none             |     0|f1         |â†‘  | 0.0391|Â±  |0.0011|
|gpqa_diamond_cot_n_shot                                   |      2|flexible-extract |     0|exact_match|â†‘  | 0.1869|Â±  |0.0278|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_cot_zeroshot                                 |      1|flexible-extract |     0|exact_match|â†‘  | 0.1919|Â±  |0.0281|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_generative_n_shot                            |      2|flexible-extract |     0|exact_match|â†‘  | 0.2222|Â±  |0.0296|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_n_shot                                       |      2|none             |     0|acc        |â†‘  | 0.2929|Â±  |0.0324|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2929|Â±  |0.0324|
|gpqa_diamond_zeroshot                                     |      1|none             |     0|acc        |â†‘  | 0.2273|Â±  |0.0299|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2273|Â±  |0.0299|
|gpqa_extended_cot_n_shot                                  |      2|flexible-extract |     0|exact_match|â†‘  | 0.1941|Â±  |0.0169|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_cot_zeroshot                                |      1|flexible-extract |     0|exact_match|â†‘  | 0.1465|Â±  |0.0151|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_generative_n_shot                           |      2|flexible-extract |     0|exact_match|â†‘  | 0.2179|Â±  |0.0177|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_n_shot                                      |      2|none             |     0|acc        |â†‘  | 0.3077|Â±  |0.0198|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3077|Â±  |0.0198|
|gpqa_extended_zeroshot                                    |      1|none             |     0|acc        |â†‘  | 0.2985|Â±  |0.0196|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2985|Â±  |0.0196|
|gpqa_main_cot_n_shot                                      |      2|flexible-extract |     0|exact_match|â†‘  | 0.1674|Â±  |0.0177|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_cot_zeroshot                                    |      1|flexible-extract |     0|exact_match|â†‘  | 0.1496|Â±  |0.0169|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_generative_n_shot                               |      2|flexible-extract |     0|exact_match|â†‘  | 0.2232|Â±  |0.0197|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_n_shot                                          |      2|none             |     0|acc        |â†‘  | 0.3237|Â±  |0.0221|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3237|Â±  |0.0221|
|gpqa_main_zeroshot                                        |      1|none             |     0|acc        |â†‘  | 0.2857|Â±  |0.0214|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2857|Â±  |0.0214|
|gsm8k                                                     |      3|flexible-extract |     5|exact_match|â†‘  | 0.5201|Â±  |0.0138|
|                                                          |       |strict-match     |     5|exact_match|â†‘  | 0.3025|Â±  |0.0127|
|hellaswag                                                 |      1|none             |     0|acc        |â†‘  | 0.5087|Â±  |0.0050|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.6827|Â±  |0.0046|
|mmlu                                                      |      2|none             |      |acc        |â†‘  | 0.6003|Â±  |0.0039|
| - humanities                                             |      2|none             |      |acc        |â†‘  | 0.5409|Â±  |0.0068|
|  - formal_logic                                          |      1|none             |     0|acc        |â†‘  | 0.5000|Â±  |0.0447|
|  - high_school_european_history                          |      1|none             |     0|acc        |â†‘  | 0.7576|Â±  |0.0335|
|  - high_school_us_history                                |      1|none             |     0|acc        |â†‘  | 0.7255|Â±  |0.0313|
|  - high_school_world_history                             |      1|none             |     0|acc        |â†‘  | 0.7722|Â±  |0.0273|
|  - international_law                                     |      1|none             |     0|acc        |â†‘  | 0.7355|Â±  |0.0403|
|  - jurisprudence                                         |      1|none             |     0|acc        |â†‘  | 0.7963|Â±  |0.0389|
|  - logical_fallacies                                     |      1|none             |     0|acc        |â†‘  | 0.7669|Â±  |0.0332|
|  - moral_disputes                                        |      1|none             |     0|acc        |â†‘  | 0.6474|Â±  |0.0257|
|  - moral_scenarios                                       |      1|none             |     0|acc        |â†‘  | 0.2972|Â±  |0.0153|
|  - philosophy                                            |      1|none             |     0|acc        |â†‘  | 0.6656|Â±  |0.0268|
|  - prehistory                                            |      1|none             |     0|acc        |â†‘  | 0.6728|Â±  |0.0261|
|  - professional_law                                      |      1|none             |     0|acc        |â†‘  | 0.4394|Â±  |0.0127|
|  - world_religions                                       |      1|none             |     0|acc        |â†‘  | 0.8012|Â±  |0.0306|
| - other                                                  |      2|none             |      |acc        |â†‘  | 0.6453|Â±  |0.0083|
|  - business_ethics                                       |      1|none             |     0|acc        |â†‘  | 0.6600|Â±  |0.0476|
|  - clinical_knowledge                                    |      1|none             |     0|acc        |â†‘  | 0.6642|Â±  |0.0291|
|  - college_medicine                                      |      1|none             |     0|acc        |â†‘  | 0.6705|Â±  |0.0358|
|  - global_facts                                          |      1|none             |     0|acc        |â†‘  | 0.2100|Â±  |0.0409|
|  - human_aging                                           |      1|none             |     0|acc        |â†‘  | 0.6278|Â±  |0.0324|
|  - management                                            |      1|none             |     0|acc        |â†‘  | 0.8155|Â±  |0.0384|
|  - marketing                                             |      1|none             |     0|acc        |â†‘  | 0.8376|Â±  |0.0242|
|  - medical_genetics                                      |      1|none             |     0|acc        |â†‘  | 0.6600|Â±  |0.0476|
|  - miscellaneous                                         |      1|none             |     0|acc        |â†‘  | 0.7241|Â±  |0.0160|
|  - nutrition                                             |      1|none             |     0|acc        |â†‘  | 0.6732|Â±  |0.0269|
|  - professional_accounting                               |      1|none             |     0|acc        |â†‘  | 0.4610|Â±  |0.0297|
|  - professional_medicine                                 |      1|none             |     0|acc        |â†‘  | 0.6029|Â±  |0.0297|
|  - virology                                              |      1|none             |     0|acc        |â†‘  | 0.4398|Â±  |0.0386|
| - social sciences                                        |      2|none             |      |acc        |â†‘  | 0.7085|Â±  |0.0080|
|  - econometrics                                          |      1|none             |     0|acc        |â†‘  | 0.4474|Â±  |0.0468|
|  - high_school_geography                                 |      1|none             |     0|acc        |â†‘  | 0.7677|Â±  |0.0301|
|  - high_school_government_and_politics                   |      1|none             |     0|acc        |â†‘  | 0.7979|Â±  |0.0290|
|  - high_school_macroeconomics                            |      1|none             |     0|acc        |â†‘  | 0.6769|Â±  |0.0237|
|  - high_school_microeconomics                            |      1|none             |     0|acc        |â†‘  | 0.7185|Â±  |0.0292|
|  - high_school_psychology                                |      1|none             |     0|acc        |â†‘  | 0.8275|Â±  |0.0162|
|  - human_sexuality                                       |      1|none             |     0|acc        |â†‘  | 0.7328|Â±  |0.0388|
|  - professional_psychology                               |      1|none             |     0|acc        |â†‘  | 0.6013|Â±  |0.0198|
|  - public_relations                                      |      1|none             |     0|acc        |â†‘  | 0.5909|Â±  |0.0471|
|  - security_studies                                      |      1|none             |     0|acc        |â†‘  | 0.6980|Â±  |0.0294|
|  - sociology                                             |      1|none             |     0|acc        |â†‘  | 0.8060|Â±  |0.0280|
|  - us_foreign_policy                                     |      1|none             |     0|acc        |â†‘  | 0.7500|Â±  |0.0435|
| - stem                                                   |      2|none             |      |acc        |â†‘  | 0.5392|Â±  |0.0086|
|  - abstract_algebra                                      |      1|none             |     0|acc        |â†‘  | 0.3700|Â±  |0.0485|
|  - anatomy                                               |      1|none             |     0|acc        |â†‘  | 0.5333|Â±  |0.0431|
|  - astronomy                                             |      1|none             |     0|acc        |â†‘  | 0.7105|Â±  |0.0369|
|  - college_biology                                       |      1|none             |     0|acc        |â†‘  | 0.6528|Â±  |0.0398|
|  - college_chemistry                                     |      1|none             |     0|acc        |â†‘  | 0.3300|Â±  |0.0473|
|  - college_computer_science                              |      1|none             |     0|acc        |â†‘  | 0.5000|Â±  |0.0503|
|  - college_mathematics                                   |      1|none             |     0|acc        |â†‘  | 0.4100|Â±  |0.0494|
|  - college_physics                                       |      1|none             |     0|acc        |â†‘  | 0.4608|Â±  |0.0496|
|  - computer_security                                     |      1|none             |     0|acc        |â†‘  | 0.7300|Â±  |0.0446|
|  - conceptual_physics                                    |      1|none             |     0|acc        |â†‘  | 0.5787|Â±  |0.0323|
|  - electrical_engineering                                |      1|none             |     0|acc        |â†‘  | 0.6069|Â±  |0.0407|
|  - elementary_mathematics                                |      1|none             |     0|acc        |â†‘  | 0.4947|Â±  |0.0257|
|  - high_school_biology                                   |      1|none             |     0|acc        |â†‘  | 0.7613|Â±  |0.0243|
|  - high_school_chemistry                                 |      1|none             |     0|acc        |â†‘  | 0.4926|Â±  |0.0352|
|  - high_school_computer_science                          |      1|none             |     0|acc        |â†‘  | 0.6700|Â±  |0.0473|
|  - high_school_mathematics                               |      1|none             |     0|acc        |â†‘  | 0.4148|Â±  |0.0300|
|  - high_school_physics                                   |      1|none             |     0|acc        |â†‘  | 0.3709|Â±  |0.0394|
|  - high_school_statistics                                |      1|none             |     0|acc        |â†‘  | 0.5463|Â±  |0.0340|
|  - machine_learning                                      |      1|none             |     0|acc        |â†‘  | 0.4018|Â±  |0.0465|
|nq_open                                                   |      4|remove_whitespace|     0|exact_match|â†‘  | 0.0421|Â±  |0.0033|
|openbookqa                                                |      1|none             |     0|acc        |â†‘  | 0.3160|Â±  |0.0208|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.4040|Â±  |0.0220|
|piqa                                                      |      1|none             |     0|acc        |â†‘  | 0.7633|Â±  |0.0099|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.7595|Â±  |0.0100|
|qnli                                                      |      1|none             |     0|acc        |â†‘  | 0.5660|Â±  |0.0067|
|sciq                                                      |      1|none             |     0|acc        |â†‘  | 0.9490|Â±  |0.0070|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.9400|Â±  |0.0075|
|triviaqa                                                  |      3|remove_whitespace|     0|exact_match|â†‘  | 0.2801|Â±  |0.0034|
|truthfulqa_gen                                            |      3|none             |     0|bleu_acc   |â†‘  | 0.3966|Â±  |0.0171|
|                                                          |       |none             |     0|bleu_diff  |â†‘  |-1.7922|Â±  |0.4018|
|                                                          |       |none             |     0|bleu_max   |â†‘  |12.2414|Â±  |0.5454|
|                                                          |       |none             |     0|rouge1_acc |â†‘  | 0.4272|Â±  |0.0173|
|                                                          |       |none             |     0|rouge1_diff|â†‘  |-3.2519|Â±  |0.6342|
|                                                          |       |none             |     0|rouge1_max |â†‘  |32.3672|Â±  |0.7715|
|                                                          |       |none             |     0|rouge2_acc |â†‘  | 0.2742|Â±  |0.0156|
|                                                          |       |none             |     0|rouge2_diff|â†‘  |-3.5921|Â±  |0.6461|
|                                                          |       |none             |     0|rouge2_max |â†‘  |17.5177|Â±  |0.7544|
|                                                          |       |none             |     0|rougeL_acc |â†‘  | 0.4235|Â±  |0.0173|
|                                                          |       |none             |     0|rougeL_diff|â†‘  |-3.5600|Â±  |0.6371|
|                                                          |       |none             |     0|rougeL_max |â†‘  |29.5881|Â±  |0.7560|
|truthfulqa_mc1                                            |      2|none             |     0|acc        |â†‘  | 0.3121|Â±  |0.0162|
|truthfulqa_mc2                                            |      3|none             |     0|acc        |â†‘  | 0.4657|Â±  |0.0150|
|winogrande                                                |      1|none             |     0|acc        |â†‘  | 0.6290|Â±  |0.0136|

|      Groups      |Version|  Filter  |n-shot|  Metric   |   |Value |   |Stderr|
|------------------|------:|----------|------|-----------|---|-----:|---|-----:|
|bbh               |      3|get-answer|      |exact_match|â†‘  |0.3861|Â±  |0.0054|
|mmlu              |      2|none      |      |acc        |â†‘  |0.6003|Â±  |0.0039|
| - humanities     |      2|none      |      |acc        |â†‘  |0.5409|Â±  |0.0068|
| - other          |      2|none      |      |acc        |â†‘  |0.6453|Â±  |0.0083|
| - social sciences|      2|none      |      |acc        |â†‘  |0.7085|Â±  |0.0080|
| - stem           |      2|none      |      |acc        |â†‘  |0.5392|Â±  |0.0086|

Qwen_Qwen2.5-1.5B-Instruct: 5h 38m 25s
âœ… Benchmark completed for Qwen_Qwen2.5-1.5B-Instruct

ðŸ”¥ Starting benchmark for Qwen_Qwen2.5-0.5B-Instruct
Passed argument batch_size = auto:4.0. Detecting largest batch size
Determined largest batch size: 2
Passed argument batch_size = auto:4.0. Detecting largest batch size
Determined largest batch size: 64
ðŸ”¥ Starting benchmark for deepseek-ai_deepseek-moe-16b-base
ðŸ”¥ Starting benchmark for deepseek-ai_deepseek-moe-16b-base
deepseek-ai_deepseek-moe-16b-base: 0h 5m 31s
âœ… Benchmark completed for deepseek-ai_deepseek-moe-16b-base

ðŸ”¥ Starting benchmark for Deepseek-ai_deepseek-moe-16b-chat
Deepseek-ai_deepseek-moe-16b-chat: 0h 0m 4s
âœ… Benchmark completed for Deepseek-ai_deepseek-moe-16b-chat

ðŸ”¥ Starting benchmark for Qwen_Qwen-7B-Chat
Qwen_Qwen-7B-Chat: 0h 5m 5s
âœ… Benchmark completed for Qwen_Qwen-7B-Chat

ðŸ”¥ Starting benchmark for Qwen_Qwen-7B
Qwen_Qwen-7B: 0h 5m 8s
âœ… Benchmark completed for Qwen_Qwen-7B

ðŸ”¥ Starting benchmark for baichuan-inc_Baichuan2-13B-Chat
baichuan-inc_Baichuan2-13B-Chat: 1h 19m 49s
âœ… Benchmark completed for baichuan-inc_Baichuan2-13B-Chat

ðŸ”¥ Starting benchmark for Qwen_Qwen2.5-14B-Instruct
hf (pretrained=/mnt/data8tb/Documents/llm/llm_models/Qwen_Qwen2.5-14B-Instruct,trust_remote_code=True,load_in_8bit=True,device_map=auto), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 1
|                          Tasks                           |Version|     Filter      |n-shot|  Metric   |   | Value |   |Stderr|
|----------------------------------------------------------|------:|-----------------|-----:|-----------|---|------:|---|-----:|
|anli_r1                                                   |      1|none             |     0|acc        |â†‘  | 0.7210|Â±  |0.0142|
|anli_r2                                                   |      1|none             |     0|acc        |â†‘  | 0.6340|Â±  |0.0152|
|anli_r3                                                   |      1|none             |     0|acc        |â†‘  | 0.6175|Â±  |0.0140|
|arc_challenge                                             |      1|none             |     0|acc        |â†‘  | 0.6067|Â±  |0.0143|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.6152|Â±  |0.0142|
|bbh                                                       |      3|get-answer       |      |exact_match|â†‘  | 0.1069|Â±  |0.0032|
| - bbh_cot_fewshot_boolean_expressions                    |      4|get-answer       |     3|exact_match|â†‘  | 0.0000|Â±  |0.0000|
| - bbh_cot_fewshot_causal_judgement                       |      4|get-answer       |     3|exact_match|â†‘  | 0.2995|Â±  |0.0336|
| - bbh_cot_fewshot_date_understanding                     |      4|get-answer       |     3|exact_match|â†‘  | 0.3280|Â±  |0.0298|
| - bbh_cot_fewshot_disambiguation_qa                      |      4|get-answer       |     3|exact_match|â†‘  | 0.0560|Â±  |0.0146|
| - bbh_cot_fewshot_dyck_languages                         |      4|get-answer       |     3|exact_match|â†‘  | 0.0680|Â±  |0.0160|
| - bbh_cot_fewshot_formal_fallacies                       |      4|get-answer       |     3|exact_match|â†‘  | 0.0000|Â±  |0.0000|
| - bbh_cot_fewshot_geometric_shapes                       |      4|get-answer       |     3|exact_match|â†‘  | 0.0120|Â±  |0.0069|
| - bbh_cot_fewshot_hyperbaton                             |      4|get-answer       |     3|exact_match|â†‘  | 0.1400|Â±  |0.0220|
| - bbh_cot_fewshot_logical_deduction_five_objects         |      4|get-answer       |     3|exact_match|â†‘  | 0.0120|Â±  |0.0069|
| - bbh_cot_fewshot_logical_deduction_seven_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.0000|Â±  |0.0000|
| - bbh_cot_fewshot_logical_deduction_three_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.0280|Â±  |0.0105|
| - bbh_cot_fewshot_movie_recommendation                   |      4|get-answer       |     3|exact_match|â†‘  | 0.1480|Â±  |0.0225|
| - bbh_cot_fewshot_multistep_arithmetic_two               |      4|get-answer       |     3|exact_match|â†‘  | 0.0000|Â±  |0.0000|
| - bbh_cot_fewshot_navigate                               |      4|get-answer       |     3|exact_match|â†‘  | 0.0000|Â±  |0.0000|
| - bbh_cot_fewshot_object_counting                        |      4|get-answer       |     3|exact_match|â†‘  | 0.5400|Â±  |0.0316|
| - bbh_cot_fewshot_penguins_in_a_table                    |      4|get-answer       |     3|exact_match|â†‘  | 0.1027|Â±  |0.0252|
| - bbh_cot_fewshot_reasoning_about_colored_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.4600|Â±  |0.0316|
| - bbh_cot_fewshot_ruin_names                             |      4|get-answer       |     3|exact_match|â†‘  | 0.0120|Â±  |0.0069|
| - bbh_cot_fewshot_salient_translation_error_detection    |      4|get-answer       |     3|exact_match|â†‘  | 0.6000|Â±  |0.0310|
| - bbh_cot_fewshot_snarks                                 |      4|get-answer       |     3|exact_match|â†‘  | 0.0056|Â±  |0.0056|
| - bbh_cot_fewshot_sports_understanding                   |      4|get-answer       |     3|exact_match|â†‘  | 0.0320|Â±  |0.0112|
| - bbh_cot_fewshot_temporal_sequences                     |      4|get-answer       |     3|exact_match|â†‘  | 0.0000|Â±  |0.0000|
| - bbh_cot_fewshot_tracking_shuffled_objects_five_objects |      4|get-answer       |     3|exact_match|â†‘  | 0.0120|Â±  |0.0069|
| - bbh_cot_fewshot_tracking_shuffled_objects_seven_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.0000|Â±  |0.0000|
| - bbh_cot_fewshot_tracking_shuffled_objects_three_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.0040|Â±  |0.0040|
| - bbh_cot_fewshot_web_of_lies                            |      4|get-answer       |     3|exact_match|â†‘  | 0.0000|Â±  |0.0000|
| - bbh_cot_fewshot_word_sorting                           |      4|get-answer       |     3|exact_match|â†‘  | 0.0440|Â±  |0.0130|
|boolq                                                     |      2|none             |     0|acc        |â†‘  | 0.8862|Â±  |0.0056|
|drop                                                      |      3|none             |     0|em         |â†‘  | 0.0002|Â±  |0.0001|
|                                                          |       |none             |     0|f1         |â†‘  | 0.0713|Â±  |0.0012|
|gpqa_diamond_cot_n_shot                                   |      2|flexible-extract |     0|exact_match|â†‘  | 0.1414|Â±  |0.0248|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_cot_zeroshot                                 |      1|flexible-extract |     0|exact_match|â†‘  | 0.1111|Â±  |0.0224|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_generative_n_shot                            |      2|flexible-extract |     0|exact_match|â†‘  | 0.2424|Â±  |0.0305|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0101|Â±  |0.0071|
|gpqa_diamond_n_shot                                       |      2|none             |     0|acc        |â†‘  | 0.4091|Â±  |0.0350|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.4091|Â±  |0.0350|
|gpqa_diamond_zeroshot                                     |      1|none             |     0|acc        |â†‘  | 0.3434|Â±  |0.0338|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3434|Â±  |0.0338|
|gpqa_extended_cot_n_shot                                  |      2|flexible-extract |     0|exact_match|â†‘  | 0.1593|Â±  |0.0157|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_cot_zeroshot                                |      1|flexible-extract |     0|exact_match|â†‘  | 0.1337|Â±  |0.0146|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_generative_n_shot                           |      2|flexible-extract |     0|exact_match|â†‘  | 0.2930|Â±  |0.0195|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0037|Â±  |0.0026|
|gpqa_extended_n_shot                                      |      2|none             |     0|acc        |â†‘  | 0.3755|Â±  |0.0207|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3755|Â±  |0.0207|
|gpqa_extended_zeroshot                                    |      1|none             |     0|acc        |â†‘  | 0.3810|Â±  |0.0208|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3810|Â±  |0.0208|
|gpqa_main_cot_n_shot                                      |      2|flexible-extract |     0|exact_match|â†‘  | 0.1295|Â±  |0.0159|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_cot_zeroshot                                    |      1|flexible-extract |     0|exact_match|â†‘  | 0.1161|Â±  |0.0152|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_generative_n_shot                               |      2|flexible-extract |     0|exact_match|â†‘  | 0.2812|Â±  |0.0213|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0022|Â±  |0.0022|
|gpqa_main_n_shot                                          |      2|none             |     0|acc        |â†‘  | 0.4107|Â±  |0.0233|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.4107|Â±  |0.0233|
|gpqa_main_zeroshot                                        |      1|none             |     0|acc        |â†‘  | 0.3549|Â±  |0.0226|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3549|Â±  |0.0226|
|gsm8k                                                     |      3|flexible-extract |     5|exact_match|â†‘  | 0.4390|Â±  |0.0137|
|                                                          |       |strict-match     |     5|exact_match|â†‘  | 0.7923|Â±  |0.0112|
|hellaswag                                                 |      1|none             |     0|acc        |â†‘  | 0.6527|Â±  |0.0048|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.8420|Â±  |0.0036|
|mmlu                                                      |      2|none             |      |acc        |â†‘  | 0.7831|Â±  |0.0033|
| - humanities                                             |      2|none             |      |acc        |â†‘  | 0.7214|Â±  |0.0062|
|  - formal_logic                                          |      1|none             |     0|acc        |â†‘  | 0.6349|Â±  |0.0431|
|  - high_school_european_history                          |      1|none             |     0|acc        |â†‘  | 0.8606|Â±  |0.0270|
|  - high_school_us_history                                |      1|none             |     0|acc        |â†‘  | 0.9118|Â±  |0.0199|
|  - high_school_world_history                             |      1|none             |     0|acc        |â†‘  | 0.9072|Â±  |0.0189|
|  - international_law                                     |      1|none             |     0|acc        |â†‘  | 0.9008|Â±  |0.0273|
|  - jurisprudence                                         |      1|none             |     0|acc        |â†‘  | 0.8519|Â±  |0.0343|
|  - logical_fallacies                                     |      1|none             |     0|acc        |â†‘  | 0.8834|Â±  |0.0252|
|  - moral_disputes                                        |      1|none             |     0|acc        |â†‘  | 0.7977|Â±  |0.0216|
|  - moral_scenarios                                       |      1|none             |     0|acc        |â†‘  | 0.6525|Â±  |0.0159|
|  - philosophy                                            |      1|none             |     0|acc        |â†‘  | 0.8199|Â±  |0.0218|
|  - prehistory                                            |      1|none             |     0|acc        |â†‘  | 0.8858|Â±  |0.0177|
|  - professional_law                                      |      1|none             |     0|acc        |â†‘  | 0.5678|Â±  |0.0127|
|  - world_religions                                       |      1|none             |     0|acc        |â†‘  | 0.8947|Â±  |0.0235|
| - other                                                  |      2|none             |      |acc        |â†‘  | 0.8104|Â±  |0.0068|
|  - business_ethics                                       |      1|none             |     0|acc        |â†‘  | 0.7900|Â±  |0.0409|
|  - clinical_knowledge                                    |      1|none             |     0|acc        |â†‘  | 0.8340|Â±  |0.0229|
|  - college_medicine                                      |      1|none             |     0|acc        |â†‘  | 0.7399|Â±  |0.0335|
|  - global_facts                                          |      1|none             |     0|acc        |â†‘  | 0.5700|Â±  |0.0498|
|  - human_aging                                           |      1|none             |     0|acc        |â†‘  | 0.7803|Â±  |0.0278|
|  - management                                            |      1|none             |     0|acc        |â†‘  | 0.8835|Â±  |0.0318|
|  - marketing                                             |      1|none             |     0|acc        |â†‘  | 0.9274|Â±  |0.0170|
|  - medical_genetics                                      |      1|none             |     0|acc        |â†‘  | 0.9000|Â±  |0.0302|
|  - miscellaneous                                         |      1|none             |     0|acc        |â†‘  | 0.9055|Â±  |0.0105|
|  - nutrition                                             |      1|none             |     0|acc        |â†‘  | 0.8235|Â±  |0.0218|
|  - professional_accounting                               |      1|none             |     0|acc        |â†‘  | 0.6348|Â±  |0.0287|
|  - professional_medicine                                 |      1|none             |     0|acc        |â†‘  | 0.8456|Â±  |0.0220|
|  - virology                                              |      1|none             |     0|acc        |â†‘  | 0.5482|Â±  |0.0387|
| - social sciences                                        |      2|none             |      |acc        |â†‘  | 0.8635|Â±  |0.0061|
|  - econometrics                                          |      1|none             |     0|acc        |â†‘  | 0.6842|Â±  |0.0437|
|  - high_school_geography                                 |      1|none             |     0|acc        |â†‘  | 0.9141|Â±  |0.0200|
|  - high_school_government_and_politics                   |      1|none             |     0|acc        |â†‘  | 0.9534|Â±  |0.0152|
|  - high_school_macroeconomics                            |      1|none             |     0|acc        |â†‘  | 0.8590|Â±  |0.0176|
|  - high_school_microeconomics                            |      1|none             |     0|acc        |â†‘  | 0.9034|Â±  |0.0192|
|  - high_school_psychology                                |      1|none             |     0|acc        |â†‘  | 0.9046|Â±  |0.0126|
|  - human_sexuality                                       |      1|none             |     0|acc        |â†‘  | 0.8626|Â±  |0.0302|
|  - professional_psychology                               |      1|none             |     0|acc        |â†‘  | 0.8137|Â±  |0.0158|
|  - public_relations                                      |      1|none             |     0|acc        |â†‘  | 0.7727|Â±  |0.0401|
|  - security_studies                                      |      1|none             |     0|acc        |â†‘  | 0.8327|Â±  |0.0239|
|  - sociology                                             |      1|none             |     0|acc        |â†‘  | 0.8955|Â±  |0.0216|
|  - us_foreign_policy                                     |      1|none             |     0|acc        |â†‘  | 0.9100|Â±  |0.0288|
| - stem                                                   |      2|none             |      |acc        |â†‘  | 0.7697|Â±  |0.0073|
|  - abstract_algebra                                      |      1|none             |     0|acc        |â†‘  | 0.6300|Â±  |0.0485|
|  - anatomy                                               |      1|none             |     0|acc        |â†‘  | 0.7630|Â±  |0.0367|
|  - astronomy                                             |      1|none             |     0|acc        |â†‘  | 0.9079|Â±  |0.0235|
|  - college_biology                                       |      1|none             |     0|acc        |â†‘  | 0.8958|Â±  |0.0255|
|  - college_chemistry                                     |      1|none             |     0|acc        |â†‘  | 0.5300|Â±  |0.0502|
|  - college_computer_science                              |      1|none             |     0|acc        |â†‘  | 0.6900|Â±  |0.0465|
|  - college_mathematics                                   |      1|none             |     0|acc        |â†‘  | 0.5900|Â±  |0.0494|
|  - college_physics                                       |      1|none             |     0|acc        |â†‘  | 0.6373|Â±  |0.0478|
|  - computer_security                                     |      1|none             |     0|acc        |â†‘  | 0.8000|Â±  |0.0402|
|  - conceptual_physics                                    |      1|none             |     0|acc        |â†‘  | 0.8340|Â±  |0.0243|
|  - electrical_engineering                                |      1|none             |     0|acc        |â†‘  | 0.7448|Â±  |0.0363|
|  - elementary_mathematics                                |      1|none             |     0|acc        |â†‘  | 0.8624|Â±  |0.0177|
|  - high_school_biology                                   |      1|none             |     0|acc        |â†‘  | 0.9065|Â±  |0.0166|
|  - high_school_chemistry                                 |      1|none             |     0|acc        |â†‘  | 0.6897|Â±  |0.0326|
|  - high_school_computer_science                          |      1|none             |     0|acc        |â†‘  | 0.9000|Â±  |0.0302|
|  - high_school_mathematics                               |      1|none             |     0|acc        |â†‘  | 0.6444|Â±  |0.0292|
|  - high_school_physics                                   |      1|none             |     0|acc        |â†‘  | 0.7351|Â±  |0.0360|
|  - high_school_statistics                                |      1|none             |     0|acc        |â†‘  | 0.7824|Â±  |0.0281|
|  - machine_learning                                      |      1|none             |     0|acc        |â†‘  | 0.6518|Â±  |0.0452|
|nq_open                                                   |      4|remove_whitespace|     0|exact_match|â†‘  | 0.0615|Â±  |0.0040|
|openbookqa                                                |      1|none             |     0|acc        |â†‘  | 0.3700|Â±  |0.0216|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.4760|Â±  |0.0224|
|piqa                                                      |      1|none             |     0|acc        |â†‘  | 0.8058|Â±  |0.0092|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.8172|Â±  |0.0090|
|qnli                                                      |      1|none             |     0|acc        |â†‘  | 0.8539|Â±  |0.0048|
|sciq                                                      |      1|none             |     0|acc        |â†‘  | 0.9640|Â±  |0.0059|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.9290|Â±  |0.0081|
|triviaqa                                                  |      3|remove_whitespace|     0|exact_match|â†‘  | 0.0393|Â±  |0.0015|
|truthfulqa_gen                                            |      3|none             |     0|bleu_acc   |â†‘  | 0.5141|Â±  |0.0175|
|                                                          |       |none             |     0|bleu_diff  |â†‘  | 0.9069|Â±  |0.2407|
|                                                          |       |none             |     0|bleu_max   |â†‘  | 7.7993|Â±  |0.4042|
|                                                          |       |none             |     0|rouge1_acc |â†‘  | 0.5386|Â±  |0.0175|
|                                                          |       |none             |     0|rouge1_diff|â†‘  | 1.7284|Â±  |0.4077|
|                                                          |       |none             |     0|rouge1_max |â†‘  |25.5442|Â±  |0.6360|
|                                                          |       |none             |     0|rouge2_acc |â†‘  | 0.4541|Â±  |0.0174|
|                                                          |       |none             |     0|rouge2_diff|â†‘  | 1.1688|Â±  |0.4042|
|                                                          |       |none             |     0|rouge2_max |â†‘  |14.7681|Â±  |0.5874|
|                                                          |       |none             |     0|rougeL_acc |â†‘  | 0.5239|Â±  |0.0175|
|                                                          |       |none             |     0|rougeL_diff|â†‘  | 1.4904|Â±  |0.3881|
|                                                          |       |none             |     0|rougeL_max |â†‘  |22.4469|Â±  |0.6199|
|truthfulqa_mc1                                            |      2|none             |     0|acc        |â†‘  | 0.5104|Â±  |0.0175|
|truthfulqa_mc2                                            |      3|none             |     0|acc        |â†‘  | 0.6830|Â±  |0.0150|
|winogrande                                                |      1|none             |     0|acc        |â†‘  | 0.7545|Â±  |0.0121|

|      Groups      |Version|  Filter  |n-shot|  Metric   |   |Value |   |Stderr|
|------------------|------:|----------|------|-----------|---|-----:|---|-----:|
|bbh               |      3|get-answer|      |exact_match|â†‘  |0.1069|Â±  |0.0032|
|mmlu              |      2|none      |      |acc        |â†‘  |0.7831|Â±  |0.0033|
| - humanities     |      2|none      |      |acc        |â†‘  |0.7214|Â±  |0.0062|
| - other          |      2|none      |      |acc        |â†‘  |0.8104|Â±  |0.0068|
| - social sciences|      2|none      |      |acc        |â†‘  |0.8635|Â±  |0.0061|
| - stem           |      2|none      |      |acc        |â†‘  |0.7697|Â±  |0.0073|

Qwen_Qwen2.5-14B-Instruct: 52h 44m 39s
âœ… Benchmark completed for Qwen_Qwen2.5-14B-Instruct

ðŸ”¥ Starting benchmark for Qwen_Qwen3-14B
hf (pretrained=/mnt/data8tb/Documents/llm/llm_models/Qwen_Qwen3-14B,trust_remote_code=True,load_in_8bit=True,device_map=auto), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 1
|                          Tasks                           |Version|     Filter      |n-shot|  Metric   |   | Value |   |Stderr|
|----------------------------------------------------------|------:|-----------------|-----:|-----------|---|------:|---|-----:|
|anli_r1                                                   |      1|none             |     0|acc        |â†‘  | 0.6460|Â±  |0.0151|
|anli_r2                                                   |      1|none             |     0|acc        |â†‘  | 0.5700|Â±  |0.0157|
|anli_r3                                                   |      1|none             |     0|acc        |â†‘  | 0.5567|Â±  |0.0143|
|arc_challenge                                             |      1|none             |     0|acc        |â†‘  | 0.5870|Â±  |0.0144|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.6007|Â±  |0.0143|
|bbh                                                       |      3|get-answer       |      |exact_match|â†‘  | 0.4330|Â±  |0.0048|
| - bbh_cot_fewshot_boolean_expressions                    |      4|get-answer       |     3|exact_match|â†‘  | 0.9760|Â±  |0.0097|
| - bbh_cot_fewshot_causal_judgement                       |      4|get-answer       |     3|exact_match|â†‘  | 0.0588|Â±  |0.0173|
| - bbh_cot_fewshot_date_understanding                     |      4|get-answer       |     3|exact_match|â†‘  | 0.4760|Â±  |0.0316|
| - bbh_cot_fewshot_disambiguation_qa                      |      4|get-answer       |     3|exact_match|â†‘  | 0.0000|Â±  |0.0000|
| - bbh_cot_fewshot_dyck_languages                         |      4|get-answer       |     3|exact_match|â†‘  | 0.3720|Â±  |0.0306|
| - bbh_cot_fewshot_formal_fallacies                       |      4|get-answer       |     3|exact_match|â†‘  | 0.7880|Â±  |0.0259|
| - bbh_cot_fewshot_geometric_shapes                       |      4|get-answer       |     3|exact_match|â†‘  | 0.2000|Â±  |0.0253|
| - bbh_cot_fewshot_hyperbaton                             |      4|get-answer       |     3|exact_match|â†‘  | 0.4000|Â±  |0.0310|
| - bbh_cot_fewshot_logical_deduction_five_objects         |      4|get-answer       |     3|exact_match|â†‘  | 0.5120|Â±  |0.0317|
| - bbh_cot_fewshot_logical_deduction_seven_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.0920|Â±  |0.0183|
| - bbh_cot_fewshot_logical_deduction_three_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.9240|Â±  |0.0168|
| - bbh_cot_fewshot_movie_recommendation                   |      4|get-answer       |     3|exact_match|â†‘  | 0.7840|Â±  |0.0261|
| - bbh_cot_fewshot_multistep_arithmetic_two               |      4|get-answer       |     3|exact_match|â†‘  | 0.0000|Â±  |0.0000|
| - bbh_cot_fewshot_navigate                               |      4|get-answer       |     3|exact_match|â†‘  | 0.6760|Â±  |0.0297|
| - bbh_cot_fewshot_object_counting                        |      4|get-answer       |     3|exact_match|â†‘  | 0.7200|Â±  |0.0285|
| - bbh_cot_fewshot_penguins_in_a_table                    |      4|get-answer       |     3|exact_match|â†‘  | 0.5068|Â±  |0.0415|
| - bbh_cot_fewshot_reasoning_about_colored_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.2000|Â±  |0.0253|
| - bbh_cot_fewshot_ruin_names                             |      4|get-answer       |     3|exact_match|â†‘  | 0.3560|Â±  |0.0303|
| - bbh_cot_fewshot_salient_translation_error_detection    |      4|get-answer       |     3|exact_match|â†‘  | 0.0080|Â±  |0.0056|
| - bbh_cot_fewshot_snarks                                 |      4|get-answer       |     3|exact_match|â†‘  | 0.2809|Â±  |0.0338|
| - bbh_cot_fewshot_sports_understanding                   |      4|get-answer       |     3|exact_match|â†‘  | 0.8640|Â±  |0.0217|
| - bbh_cot_fewshot_temporal_sequences                     |      4|get-answer       |     3|exact_match|â†‘  | 0.8640|Â±  |0.0217|
| - bbh_cot_fewshot_tracking_shuffled_objects_five_objects |      4|get-answer       |     3|exact_match|â†‘  | 0.6720|Â±  |0.0298|
| - bbh_cot_fewshot_tracking_shuffled_objects_seven_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.1120|Â±  |0.0200|
| - bbh_cot_fewshot_tracking_shuffled_objects_three_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.1080|Â±  |0.0197|
| - bbh_cot_fewshot_web_of_lies                            |      4|get-answer       |     3|exact_match|â†‘  | 0.3760|Â±  |0.0307|
| - bbh_cot_fewshot_word_sorting                           |      4|get-answer       |     3|exact_match|â†‘  | 0.2560|Â±  |0.0277|
|boolq                                                     |      2|none             |     0|acc        |â†‘  | 0.8917|Â±  |0.0054|
|drop                                                      |      3|none             |     0|em         |â†‘  | 0.0035|Â±  |0.0006|
|                                                          |       |none             |     0|f1         |â†‘  | 0.0904|Â±  |0.0018|
|gpqa_diamond_cot_n_shot                                   |      2|flexible-extract |     0|exact_match|â†‘  | 0.0758|Â±  |0.0189|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_cot_zeroshot                                 |      1|flexible-extract |     0|exact_match|â†‘  | 0.0606|Â±  |0.0170|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_generative_n_shot                            |      2|flexible-extract |     0|exact_match|â†‘  | 0.3939|Â±  |0.0348|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_n_shot                                       |      2|none             |     0|acc        |â†‘  | 0.3586|Â±  |0.0342|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3586|Â±  |0.0342|
|gpqa_diamond_zeroshot                                     |      1|none             |     0|acc        |â†‘  | 0.3838|Â±  |0.0346|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3838|Â±  |0.0346|
|gpqa_extended_cot_n_shot                                  |      2|flexible-extract |     0|exact_match|â†‘  | 0.1062|Â±  |0.0132|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_cot_zeroshot                                |      1|flexible-extract |     0|exact_match|â†‘  | 0.0586|Â±  |0.0101|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_generative_n_shot                           |      2|flexible-extract |     0|exact_match|â†‘  | 0.3590|Â±  |0.0205|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_n_shot                                      |      2|none             |     0|acc        |â†‘  | 0.3974|Â±  |0.0210|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3974|Â±  |0.0210|
|gpqa_extended_zeroshot                                    |      1|none             |     0|acc        |â†‘  | 0.3828|Â±  |0.0208|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3828|Â±  |0.0208|
|gpqa_main_cot_n_shot                                      |      2|flexible-extract |     0|exact_match|â†‘  | 0.1205|Â±  |0.0154|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_cot_zeroshot                                    |      1|flexible-extract |     0|exact_match|â†‘  | 0.0781|Â±  |0.0127|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_generative_n_shot                               |      2|flexible-extract |     0|exact_match|â†‘  | 0.3415|Â±  |0.0224|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_n_shot                                          |      2|none             |     0|acc        |â†‘  | 0.3750|Â±  |0.0229|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3750|Â±  |0.0229|
|gpqa_main_zeroshot                                        |      1|none             |     0|acc        |â†‘  | 0.3973|Â±  |0.0231|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3973|Â±  |0.0231|
|gsm8k                                                     |      3|flexible-extract |     5|exact_match|â†‘  | 0.8135|Â±  |0.0107|
|                                                          |       |strict-match     |     5|exact_match|â†‘  | 0.8984|Â±  |0.0083|
|hellaswag                                                 |      1|none             |     0|acc        |â†‘  | 0.6084|Â±  |0.0049|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.7877|Â±  |0.0041|
|mmlu                                                      |      2|none             |      |acc        |â†‘  | 0.7695|Â±  |0.0034|
| - humanities                                             |      2|none             |      |acc        |â†‘  | 0.6778|Â±  |0.0065|
|  - formal_logic                                          |      1|none             |     0|acc        |â†‘  | 0.6667|Â±  |0.0422|
|  - high_school_european_history                          |      1|none             |     0|acc        |â†‘  | 0.8303|Â±  |0.0293|
|  - high_school_us_history                                |      1|none             |     0|acc        |â†‘  | 0.9069|Â±  |0.0204|
|  - high_school_world_history                             |      1|none             |     0|acc        |â†‘  | 0.8565|Â±  |0.0228|
|  - international_law                                     |      1|none             |     0|acc        |â†‘  | 0.8430|Â±  |0.0332|
|  - jurisprudence                                         |      1|none             |     0|acc        |â†‘  | 0.8611|Â±  |0.0334|
|  - logical_fallacies                                     |      1|none             |     0|acc        |â†‘  | 0.8773|Â±  |0.0258|
|  - moral_disputes                                        |      1|none             |     0|acc        |â†‘  | 0.7775|Â±  |0.0224|
|  - moral_scenarios                                       |      1|none             |     0|acc        |â†‘  | 0.5665|Â±  |0.0166|
|  - philosophy                                            |      1|none             |     0|acc        |â†‘  | 0.7556|Â±  |0.0244|
|  - prehistory                                            |      1|none             |     0|acc        |â†‘  | 0.8333|Â±  |0.0207|
|  - professional_law                                      |      1|none             |     0|acc        |â†‘  | 0.5293|Â±  |0.0127|
|  - world_religions                                       |      1|none             |     0|acc        |â†‘  | 0.8713|Â±  |0.0257|
| - other                                                  |      2|none             |      |acc        |â†‘  | 0.8011|Â±  |0.0069|
|  - business_ethics                                       |      1|none             |     0|acc        |â†‘  | 0.8100|Â±  |0.0394|
|  - clinical_knowledge                                    |      1|none             |     0|acc        |â†‘  | 0.8151|Â±  |0.0239|
|  - college_medicine                                      |      1|none             |     0|acc        |â†‘  | 0.7919|Â±  |0.0310|
|  - global_facts                                          |      1|none             |     0|acc        |â†‘  | 0.5200|Â±  |0.0502|
|  - human_aging                                           |      1|none             |     0|acc        |â†‘  | 0.7444|Â±  |0.0293|
|  - management                                            |      1|none             |     0|acc        |â†‘  | 0.8350|Â±  |0.0368|
|  - marketing                                             |      1|none             |     0|acc        |â†‘  | 0.9231|Â±  |0.0175|
|  - medical_genetics                                      |      1|none             |     0|acc        |â†‘  | 0.8200|Â±  |0.0386|
|  - miscellaneous                                         |      1|none             |     0|acc        |â†‘  | 0.8838|Â±  |0.0115|
|  - nutrition                                             |      1|none             |     0|acc        |â†‘  | 0.8170|Â±  |0.0221|
|  - professional_accounting                               |      1|none             |     0|acc        |â†‘  | 0.6454|Â±  |0.0285|
|  - professional_medicine                                 |      1|none             |     0|acc        |â†‘  | 0.8640|Â±  |0.0208|
|  - virology                                              |      1|none             |     0|acc        |â†‘  | 0.5663|Â±  |0.0386|
| - social sciences                                        |      2|none             |      |acc        |â†‘  | 0.8586|Â±  |0.0062|
|  - econometrics                                          |      1|none             |     0|acc        |â†‘  | 0.6842|Â±  |0.0437|
|  - high_school_geography                                 |      1|none             |     0|acc        |â†‘  | 0.8889|Â±  |0.0224|
|  - high_school_government_and_politics                   |      1|none             |     0|acc        |â†‘  | 0.9275|Â±  |0.0187|
|  - high_school_macroeconomics                            |      1|none             |     0|acc        |â†‘  | 0.8615|Â±  |0.0175|
|  - high_school_microeconomics                            |      1|none             |     0|acc        |â†‘  | 0.9412|Â±  |0.0153|
|  - high_school_psychology                                |      1|none             |     0|acc        |â†‘  | 0.9266|Â±  |0.0112|
|  - human_sexuality                                       |      1|none             |     0|acc        |â†‘  | 0.8397|Â±  |0.0322|
|  - professional_psychology                               |      1|none             |     0|acc        |â†‘  | 0.8072|Â±  |0.0160|
|  - public_relations                                      |      1|none             |     0|acc        |â†‘  | 0.7455|Â±  |0.0417|
|  - security_studies                                      |      1|none             |     0|acc        |â†‘  | 0.7918|Â±  |0.0260|
|  - sociology                                             |      1|none             |     0|acc        |â†‘  | 0.8706|Â±  |0.0237|
|  - us_foreign_policy                                     |      1|none             |     0|acc        |â†‘  | 0.8900|Â±  |0.0314|
| - stem                                                   |      2|none             |      |acc        |â†‘  | 0.7881|Â±  |0.0070|
|  - abstract_algebra                                      |      1|none             |     0|acc        |â†‘  | 0.6300|Â±  |0.0485|
|  - anatomy                                               |      1|none             |     0|acc        |â†‘  | 0.8000|Â±  |0.0346|
|  - astronomy                                             |      1|none             |     0|acc        |â†‘  | 0.8750|Â±  |0.0269|
|  - college_biology                                       |      1|none             |     0|acc        |â†‘  | 0.9028|Â±  |0.0248|
|  - college_chemistry                                     |      1|none             |     0|acc        |â†‘  | 0.5800|Â±  |0.0496|
|  - college_computer_science                              |      1|none             |     0|acc        |â†‘  | 0.6700|Â±  |0.0473|
|  - college_mathematics                                   |      1|none             |     0|acc        |â†‘  | 0.6700|Â±  |0.0473|
|  - college_physics                                       |      1|none             |     0|acc        |â†‘  | 0.6569|Â±  |0.0472|
|  - computer_security                                     |      1|none             |     0|acc        |â†‘  | 0.8500|Â±  |0.0359|
|  - conceptual_physics                                    |      1|none             |     0|acc        |â†‘  | 0.8723|Â±  |0.0218|
|  - electrical_engineering                                |      1|none             |     0|acc        |â†‘  | 0.8276|Â±  |0.0315|
|  - elementary_mathematics                                |      1|none             |     0|acc        |â†‘  | 0.8545|Â±  |0.0182|
|  - high_school_biology                                   |      1|none             |     0|acc        |â†‘  | 0.9387|Â±  |0.0136|
|  - high_school_chemistry                                 |      1|none             |     0|acc        |â†‘  | 0.7833|Â±  |0.0290|
|  - high_school_computer_science                          |      1|none             |     0|acc        |â†‘  | 0.8900|Â±  |0.0314|
|  - high_school_mathematics                               |      1|none             |     0|acc        |â†‘  | 0.5963|Â±  |0.0299|
|  - high_school_physics                                   |      1|none             |     0|acc        |â†‘  | 0.7550|Â±  |0.0351|
|  - high_school_statistics                                |      1|none             |     0|acc        |â†‘  | 0.7917|Â±  |0.0277|
|  - machine_learning                                      |      1|none             |     0|acc        |â†‘  | 0.6607|Â±  |0.0449|
|nq_open                                                   |      4|remove_whitespace|     0|exact_match|â†‘  | 0.0922|Â±  |0.0048|
|openbookqa                                                |      1|none             |     0|acc        |â†‘  | 0.3420|Â±  |0.0212|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.4600|Â±  |0.0223|
|piqa                                                      |      1|none             |     0|acc        |â†‘  | 0.7916|Â±  |0.0095|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.7949|Â±  |0.0094|
|qnli                                                      |      1|none             |     0|acc        |â†‘  | 0.8442|Â±  |0.0049|
|sciq                                                      |      1|none             |     0|acc        |â†‘  | 0.9770|Â±  |0.0047|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.9660|Â±  |0.0057|
|triviaqa                                                  |      3|remove_whitespace|     0|exact_match|â†‘  | 0.4075|Â±  |0.0037|
|truthfulqa_gen                                            |      3|none             |     0|bleu_acc   |â†‘  | 0.6744|Â±  |0.0164|
|                                                          |       |none             |     0|bleu_diff  |â†‘  |20.9239|Â±  |1.1232|
|                                                          |       |none             |     0|bleu_max   |â†‘  |38.9157|Â±  |0.8760|
|                                                          |       |none             |     0|rouge1_acc |â†‘  | 0.6818|Â±  |0.0163|
|                                                          |       |none             |     0|rouge1_diff|â†‘  |29.7410|Â±  |1.5930|
|                                                          |       |none             |     0|rouge1_max |â†‘  |65.2284|Â±  |0.9593|
|                                                          |       |none             |     0|rouge2_acc |â†‘  | 0.6267|Â±  |0.0169|
|                                                          |       |none             |     0|rouge2_diff|â†‘  |30.9999|Â±  |1.7090|
|                                                          |       |none             |     0|rouge2_max |â†‘  |54.1666|Â±  |1.1923|
|                                                          |       |none             |     0|rougeL_acc |â†‘  | 0.6756|Â±  |0.0164|
|                                                          |       |none             |     0|rougeL_diff|â†‘  |29.8027|Â±  |1.6052|
|                                                          |       |none             |     0|rougeL_max |â†‘  |63.4625|Â±  |0.9960|
|truthfulqa_mc1                                            |      2|none             |     0|acc        |â†‘  | 0.4064|Â±  |0.0172|
|truthfulqa_mc2                                            |      3|none             |     0|acc        |â†‘  | 0.5894|Â±  |0.0154|
|winogrande                                                |      1|none             |     0|acc        |â†‘  | 0.7206|Â±  |0.0126|

|      Groups      |Version|  Filter  |n-shot|  Metric   |   |Value |   |Stderr|
|------------------|------:|----------|------|-----------|---|-----:|---|-----:|
|bbh               |      3|get-answer|      |exact_match|â†‘  |0.4330|Â±  |0.0048|
|mmlu              |      2|none      |      |acc        |â†‘  |0.7695|Â±  |0.0034|
| - humanities     |      2|none      |      |acc        |â†‘  |0.6778|Â±  |0.0065|
| - other          |      2|none      |      |acc        |â†‘  |0.8011|Â±  |0.0069|
| - social sciences|      2|none      |      |acc        |â†‘  |0.8586|Â±  |0.0062|
| - stem           |      2|none      |      |acc        |â†‘  |0.7881|Â±  |0.0070|

Qwen_Qwen3-14B: 29h 46m 1s
âœ… Benchmark completed for Qwen_Qwen3-14B

ðŸ”¥ Starting benchmark for Qwen_Qwen2.5-1.5B-Instruct
ðŸ”¥ Starting benchmark for Qwen_Qwen2.5-1.5B-Instruct
Qwen_Qwen2.5-1.5B-Instruct: 0h 5m 18s
âœ… Benchmark completed for Qwen_Qwen2.5-1.5B-Instruct

ðŸ”¥ Starting benchmark for Qwen_Qwen2.5-0.5B-Instruct
Qwen_Qwen2.5-0.5B-Instruct: 0h 5m 0s
âœ… Benchmark completed for Qwen_Qwen2.5-0.5B-Instruct

ðŸ”¥ Starting benchmark for deepseek-ai_DeepSeek-R1-Distill-Qwen-1.5B
deepseek-ai_DeepSeek-R1-Distill-Qwen-1.5B: 0h 5m 19s
âœ… Benchmark completed for deepseek-ai_DeepSeek-R1-Distill-Qwen-1.5B

ðŸ”¥ Starting benchmark for Qwen_Qwen3-1.7B
Qwen_Qwen3-1.7B: 0h 5m 18s
âœ… Benchmark completed for Qwen_Qwen3-1.7B

ðŸ”¥ Starting benchmark for Qwen_Qwen3-0.6B
Qwen_Qwen3-0.6B: 0h 5m 0s
âœ… Benchmark completed for Qwen_Qwen3-0.6B

ðŸ”¥ Starting benchmark for Qwen_Qwen3-4B
Qwen_Qwen3-4B: 0h 5m 7s
âœ… Benchmark completed for Qwen_Qwen3-4B

ðŸ”¥ Starting benchmark for Qwen_Qwen2.5-1.5B-Instruct
Qwen_Qwen2.5-1.5B-Instruct: 0h 0m 3s
âœ… Benchmark completed for Qwen_Qwen2.5-1.5B-Instruct

ðŸ”¥ Starting benchmark for Qwen_Qwen2.5-0.5B-Instruct
Qwen_Qwen2.5-0.5B-Instruct: 0h 0m 3s
âœ… Benchmark completed for Qwen_Qwen2.5-0.5B-Instruct

ðŸ”¥ Starting benchmark for deepseek-ai_DeepSeek-R1-Distill-Qwen-1.5B
ðŸ”¥ Starting benchmark for Qwen_Qwen2.5-1.5B-Instruct
Passed argument batch_size = auto:1. Detecting largest batch size
Determined largest batch size: 1
ðŸ”¥ Starting benchmark for Qwen_Qwen2.5-1.5B-Instruct
hf (pretrained=/mnt/data8tb/Documents/llm/llm_models/Qwen_Qwen2.5-1.5B-Instruct,trust_remote_code=True,device_map=auto), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 6
|                          Tasks                           |Version|     Filter      |n-shot|  Metric   |   | Value |   |Stderr|
|----------------------------------------------------------|------:|-----------------|-----:|-----------|---|------:|---|-----:|
|anli_r1                                                   |      1|none             |     0|acc        |â†‘  | 0.4480|Â±  |0.0157|
|anli_r2                                                   |      1|none             |     0|acc        |â†‘  | 0.3920|Â±  |0.0154|
|anli_r3                                                   |      1|none             |     0|acc        |â†‘  | 0.4317|Â±  |0.0143|
|arc_challenge                                             |      1|none             |     0|acc        |â†‘  | 0.4352|Â±  |0.0145|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.4684|Â±  |0.0146|
|bbh                                                       |      3|get-answer       |      |exact_match|â†‘  | 0.3692|Â±  |0.0054|
| - bbh_cot_fewshot_boolean_expressions                    |      4|get-answer       |     3|exact_match|â†‘  | 0.8600|Â±  |0.0220|
| - bbh_cot_fewshot_causal_judgement                       |      4|get-answer       |     3|exact_match|â†‘  | 0.4813|Â±  |0.0366|
| - bbh_cot_fewshot_date_understanding                     |      4|get-answer       |     3|exact_match|â†‘  | 0.2920|Â±  |0.0288|
| - bbh_cot_fewshot_disambiguation_qa                      |      4|get-answer       |     3|exact_match|â†‘  | 0.4000|Â±  |0.0310|
| - bbh_cot_fewshot_dyck_languages                         |      4|get-answer       |     3|exact_match|â†‘  | 0.0160|Â±  |0.0080|
| - bbh_cot_fewshot_formal_fallacies                       |      4|get-answer       |     3|exact_match|â†‘  | 0.1920|Â±  |0.0250|
| - bbh_cot_fewshot_geometric_shapes                       |      4|get-answer       |     3|exact_match|â†‘  | 0.3160|Â±  |0.0295|
| - bbh_cot_fewshot_hyperbaton                             |      4|get-answer       |     3|exact_match|â†‘  | 0.5720|Â±  |0.0314|
| - bbh_cot_fewshot_logical_deduction_five_objects         |      4|get-answer       |     3|exact_match|â†‘  | 0.2560|Â±  |0.0277|
| - bbh_cot_fewshot_logical_deduction_seven_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.1800|Â±  |0.0243|
| - bbh_cot_fewshot_logical_deduction_three_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.5240|Â±  |0.0316|
| - bbh_cot_fewshot_movie_recommendation                   |      4|get-answer       |     3|exact_match|â†‘  | 0.4720|Â±  |0.0316|
| - bbh_cot_fewshot_multistep_arithmetic_two               |      4|get-answer       |     3|exact_match|â†‘  | 0.6040|Â±  |0.0310|
| - bbh_cot_fewshot_navigate                               |      4|get-answer       |     3|exact_match|â†‘  | 0.6520|Â±  |0.0302|
| - bbh_cot_fewshot_object_counting                        |      4|get-answer       |     3|exact_match|â†‘  | 0.4760|Â±  |0.0316|
| - bbh_cot_fewshot_penguins_in_a_table                    |      4|get-answer       |     3|exact_match|â†‘  | 0.5068|Â±  |0.0415|
| - bbh_cot_fewshot_reasoning_about_colored_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.4600|Â±  |0.0316|
| - bbh_cot_fewshot_ruin_names                             |      4|get-answer       |     3|exact_match|â†‘  | 0.3320|Â±  |0.0298|
| - bbh_cot_fewshot_salient_translation_error_detection    |      4|get-answer       |     3|exact_match|â†‘  | 0.3440|Â±  |0.0301|
| - bbh_cot_fewshot_snarks                                 |      4|get-answer       |     3|exact_match|â†‘  | 0.4438|Â±  |0.0373|
| - bbh_cot_fewshot_sports_understanding                   |      4|get-answer       |     3|exact_match|â†‘  | 0.6360|Â±  |0.0305|
| - bbh_cot_fewshot_temporal_sequences                     |      4|get-answer       |     3|exact_match|â†‘  | 0.1120|Â±  |0.0200|
| - bbh_cot_fewshot_tracking_shuffled_objects_five_objects |      4|get-answer       |     3|exact_match|â†‘  | 0.1520|Â±  |0.0228|
| - bbh_cot_fewshot_tracking_shuffled_objects_seven_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.1040|Â±  |0.0193|
| - bbh_cot_fewshot_tracking_shuffled_objects_three_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.3920|Â±  |0.0309|
| - bbh_cot_fewshot_web_of_lies                            |      4|get-answer       |     3|exact_match|â†‘  | 0.2560|Â±  |0.0277|
| - bbh_cot_fewshot_word_sorting                           |      4|get-answer       |     3|exact_match|â†‘  | 0.0440|Â±  |0.0130|
|boolq                                                     |      2|none             |     0|acc        |â†‘  | 0.7813|Â±  |0.0072|
|drop                                                      |      3|none             |     0|em         |â†‘  | 0.0017|Â±  |0.0004|
|                                                          |       |none             |     0|f1         |â†‘  | 0.0391|Â±  |0.0011|
|gpqa_diamond_cot_n_shot                                   |      2|flexible-extract |     0|exact_match|â†‘  | 0.1566|Â±  |0.0259|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_cot_zeroshot                                 |      1|flexible-extract |     0|exact_match|â†‘  | 0.1768|Â±  |0.0272|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_generative_n_shot                            |      2|flexible-extract |     0|exact_match|â†‘  | 0.2222|Â±  |0.0296|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_n_shot                                       |      2|none             |     0|acc        |â†‘  | 0.2929|Â±  |0.0324|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2929|Â±  |0.0324|
|gpqa_diamond_zeroshot                                     |      1|none             |     0|acc        |â†‘  | 0.2323|Â±  |0.0301|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2323|Â±  |0.0301|
|gpqa_extended_cot_n_shot                                  |      2|flexible-extract |     0|exact_match|â†‘  | 0.1630|Â±  |0.0158|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_cot_zeroshot                                |      1|flexible-extract |     0|exact_match|â†‘  | 0.1520|Â±  |0.0154|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_generative_n_shot                           |      2|flexible-extract |     0|exact_match|â†‘  | 0.2271|Â±  |0.0179|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_n_shot                                      |      2|none             |     0|acc        |â†‘  | 0.3114|Â±  |0.0198|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3114|Â±  |0.0198|
|gpqa_extended_zeroshot                                    |      1|none             |     0|acc        |â†‘  | 0.2985|Â±  |0.0196|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2985|Â±  |0.0196|
|gpqa_main_cot_n_shot                                      |      2|flexible-extract |     0|exact_match|â†‘  | 0.1987|Â±  |0.0189|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_cot_zeroshot                                    |      1|flexible-extract |     0|exact_match|â†‘  | 0.1228|Â±  |0.0155|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_generative_n_shot                               |      2|flexible-extract |     0|exact_match|â†‘  | 0.2165|Â±  |0.0195|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_n_shot                                          |      2|none             |     0|acc        |â†‘  | 0.3259|Â±  |0.0222|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3259|Â±  |0.0222|
|gpqa_main_zeroshot                                        |      1|none             |     0|acc        |â†‘  | 0.2835|Â±  |0.0213|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2835|Â±  |0.0213|
|gsm8k                                                     |      3|flexible-extract |     5|exact_match|â†‘  | 0.5095|Â±  |0.0138|
|                                                          |       |strict-match     |     5|exact_match|â†‘  | 0.3192|Â±  |0.0128|
|hellaswag                                                 |      1|none             |     0|acc        |â†‘  | 0.5080|Â±  |0.0050|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.6829|Â±  |0.0046|
|mmlu                                                      |      2|none             |      |acc        |â†‘  | 0.6006|Â±  |0.0039|
| - humanities                                             |      2|none             |      |acc        |â†‘  | 0.5422|Â±  |0.0068|
|  - formal_logic                                          |      1|none             |     0|acc        |â†‘  | 0.5238|Â±  |0.0447|
|  - high_school_european_history                          |      1|none             |     0|acc        |â†‘  | 0.7576|Â±  |0.0335|
|  - high_school_us_history                                |      1|none             |     0|acc        |â†‘  | 0.7255|Â±  |0.0313|
|  - high_school_world_history                             |      1|none             |     0|acc        |â†‘  | 0.7722|Â±  |0.0273|
|  - international_law                                     |      1|none             |     0|acc        |â†‘  | 0.7438|Â±  |0.0398|
|  - jurisprudence                                         |      1|none             |     0|acc        |â†‘  | 0.7870|Â±  |0.0396|
|  - logical_fallacies                                     |      1|none             |     0|acc        |â†‘  | 0.7669|Â±  |0.0332|
|  - moral_disputes                                        |      1|none             |     0|acc        |â†‘  | 0.6532|Â±  |0.0256|
|  - moral_scenarios                                       |      1|none             |     0|acc        |â†‘  | 0.3006|Â±  |0.0153|
|  - philosophy                                            |      1|none             |     0|acc        |â†‘  | 0.6656|Â±  |0.0268|
|  - prehistory                                            |      1|none             |     0|acc        |â†‘  | 0.6728|Â±  |0.0261|
|  - professional_law                                      |      1|none             |     0|acc        |â†‘  | 0.4394|Â±  |0.0127|
|  - world_religions                                       |      1|none             |     0|acc        |â†‘  | 0.7895|Â±  |0.0313|
| - other                                                  |      2|none             |      |acc        |â†‘  | 0.6460|Â±  |0.0083|
|  - business_ethics                                       |      1|none             |     0|acc        |â†‘  | 0.6600|Â±  |0.0476|
|  - clinical_knowledge                                    |      1|none             |     0|acc        |â†‘  | 0.6679|Â±  |0.0290|
|  - college_medicine                                      |      1|none             |     0|acc        |â†‘  | 0.6705|Â±  |0.0358|
|  - global_facts                                          |      1|none             |     0|acc        |â†‘  | 0.2100|Â±  |0.0409|
|  - human_aging                                           |      1|none             |     0|acc        |â†‘  | 0.6278|Â±  |0.0324|
|  - management                                            |      1|none             |     0|acc        |â†‘  | 0.8058|Â±  |0.0392|
|  - marketing                                             |      1|none             |     0|acc        |â†‘  | 0.8376|Â±  |0.0242|
|  - medical_genetics                                      |      1|none             |     0|acc        |â†‘  | 0.6700|Â±  |0.0473|
|  - miscellaneous                                         |      1|none             |     0|acc        |â†‘  | 0.7241|Â±  |0.0160|
|  - nutrition                                             |      1|none             |     0|acc        |â†‘  | 0.6732|Â±  |0.0269|
|  - professional_accounting                               |      1|none             |     0|acc        |â†‘  | 0.4645|Â±  |0.0298|
|  - professional_medicine                                 |      1|none             |     0|acc        |â†‘  | 0.6029|Â±  |0.0297|
|  - virology                                              |      1|none             |     0|acc        |â†‘  | 0.4398|Â±  |0.0386|
| - social sciences                                        |      2|none             |      |acc        |â†‘  | 0.7065|Â±  |0.0080|
|  - econometrics                                          |      1|none             |     0|acc        |â†‘  | 0.4474|Â±  |0.0468|
|  - high_school_geography                                 |      1|none             |     0|acc        |â†‘  | 0.7576|Â±  |0.0305|
|  - high_school_government_and_politics                   |      1|none             |     0|acc        |â†‘  | 0.8083|Â±  |0.0284|
|  - high_school_macroeconomics                            |      1|none             |     0|acc        |â†‘  | 0.6718|Â±  |0.0238|
|  - high_school_microeconomics                            |      1|none             |     0|acc        |â†‘  | 0.7143|Â±  |0.0293|
|  - high_school_psychology                                |      1|none             |     0|acc        |â†‘  | 0.8220|Â±  |0.0164|
|  - human_sexuality                                       |      1|none             |     0|acc        |â†‘  | 0.7328|Â±  |0.0388|
|  - professional_psychology                               |      1|none             |     0|acc        |â†‘  | 0.5997|Â±  |0.0198|
|  - public_relations                                      |      1|none             |     0|acc        |â†‘  | 0.5909|Â±  |0.0471|
|  - security_studies                                      |      1|none             |     0|acc        |â†‘  | 0.6980|Â±  |0.0294|
|  - sociology                                             |      1|none             |     0|acc        |â†‘  | 0.8109|Â±  |0.0277|
|  - us_foreign_policy                                     |      1|none             |     0|acc        |â†‘  | 0.7500|Â±  |0.0435|
| - stem                                                   |      2|none             |      |acc        |â†‘  | 0.5395|Â±  |0.0086|
|  - abstract_algebra                                      |      1|none             |     0|acc        |â†‘  | 0.3600|Â±  |0.0482|
|  - anatomy                                               |      1|none             |     0|acc        |â†‘  | 0.5333|Â±  |0.0431|
|  - astronomy                                             |      1|none             |     0|acc        |â†‘  | 0.7105|Â±  |0.0369|
|  - college_biology                                       |      1|none             |     0|acc        |â†‘  | 0.6389|Â±  |0.0402|
|  - college_chemistry                                     |      1|none             |     0|acc        |â†‘  | 0.3400|Â±  |0.0476|
|  - college_computer_science                              |      1|none             |     0|acc        |â†‘  | 0.5000|Â±  |0.0503|
|  - college_mathematics                                   |      1|none             |     0|acc        |â†‘  | 0.4100|Â±  |0.0494|
|  - college_physics                                       |      1|none             |     0|acc        |â†‘  | 0.4608|Â±  |0.0496|
|  - computer_security                                     |      1|none             |     0|acc        |â†‘  | 0.7300|Â±  |0.0446|
|  - conceptual_physics                                    |      1|none             |     0|acc        |â†‘  | 0.5787|Â±  |0.0323|
|  - electrical_engineering                                |      1|none             |     0|acc        |â†‘  | 0.6069|Â±  |0.0407|
|  - elementary_mathematics                                |      1|none             |     0|acc        |â†‘  | 0.4947|Â±  |0.0257|
|  - high_school_biology                                   |      1|none             |     0|acc        |â†‘  | 0.7613|Â±  |0.0243|
|  - high_school_chemistry                                 |      1|none             |     0|acc        |â†‘  | 0.4975|Â±  |0.0352|
|  - high_school_computer_science                          |      1|none             |     0|acc        |â†‘  | 0.6800|Â±  |0.0469|
|  - high_school_mathematics                               |      1|none             |     0|acc        |â†‘  | 0.4222|Â±  |0.0301|
|  - high_school_physics                                   |      1|none             |     0|acc        |â†‘  | 0.3709|Â±  |0.0394|
|  - high_school_statistics                                |      1|none             |     0|acc        |â†‘  | 0.5417|Â±  |0.0340|
|  - machine_learning                                      |      1|none             |     0|acc        |â†‘  | 0.4018|Â±  |0.0465|
|nq_open                                                   |      4|remove_whitespace|     0|exact_match|â†‘  | 0.0416|Â±  |0.0033|
|openbookqa                                                |      1|none             |     0|acc        |â†‘  | 0.3200|Â±  |0.0209|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.4060|Â±  |0.0220|
|piqa                                                      |      1|none             |     0|acc        |â†‘  | 0.7628|Â±  |0.0099|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.7584|Â±  |0.0100|
|qnli                                                      |      1|none             |     0|acc        |â†‘  | 0.5667|Â±  |0.0067|
|sciq                                                      |      1|none             |     0|acc        |â†‘  | 0.9490|Â±  |0.0070|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.9390|Â±  |0.0076|
|triviaqa                                                  |      3|remove_whitespace|     0|exact_match|â†‘  | 0.2826|Â±  |0.0034|
|truthfulqa_gen                                            |      3|none             |     0|bleu_acc   |â†‘  | 0.4027|Â±  |0.0172|
|                                                          |       |none             |     0|bleu_diff  |â†‘  |-1.9015|Â±  |0.4040|
|                                                          |       |none             |     0|bleu_max   |â†‘  |11.9071|Â±  |0.5278|
|                                                          |       |none             |     0|rouge1_acc |â†‘  | 0.4406|Â±  |0.0174|
|                                                          |       |none             |     0|rouge1_diff|â†‘  |-3.1237|Â±  |0.6369|
|                                                          |       |none             |     0|rouge1_max |â†‘  |32.1186|Â±  |0.7534|
|                                                          |       |none             |     0|rouge2_acc |â†‘  | 0.2815|Â±  |0.0157|
|                                                          |       |none             |     0|rouge2_diff|â†‘  |-3.7671|Â±  |0.6452|
|                                                          |       |none             |     0|rouge2_max |â†‘  |16.9153|Â±  |0.7371|
|                                                          |       |none             |     0|rougeL_acc |â†‘  | 0.4321|Â±  |0.0173|
|                                                          |       |none             |     0|rougeL_diff|â†‘  |-3.3951|Â±  |0.6399|
|                                                          |       |none             |     0|rougeL_max |â†‘  |29.3596|Â±  |0.7370|
|truthfulqa_mc1                                            |      2|none             |     0|acc        |â†‘  | 0.3121|Â±  |0.0162|
|truthfulqa_mc2                                            |      3|none             |     0|acc        |â†‘  | 0.4657|Â±  |0.0150|
|winogrande                                                |      1|none             |     0|acc        |â†‘  | 0.6275|Â±  |0.0136|

|      Groups      |Version|  Filter  |n-shot|  Metric   |   |Value |   |Stderr|
|------------------|------:|----------|------|-----------|---|-----:|---|-----:|
|bbh               |      3|get-answer|      |exact_match|â†‘  |0.3692|Â±  |0.0054|
|mmlu              |      2|none      |      |acc        |â†‘  |0.6006|Â±  |0.0039|
| - humanities     |      2|none      |      |acc        |â†‘  |0.5422|Â±  |0.0068|
| - other          |      2|none      |      |acc        |â†‘  |0.6460|Â±  |0.0083|
| - social sciences|      2|none      |      |acc        |â†‘  |0.7065|Â±  |0.0080|
| - stem           |      2|none      |      |acc        |â†‘  |0.5395|Â±  |0.0086|

Qwen_Qwen2.5-1.5B-Instruct: 3h 20m 46s
âœ… Benchmark completed for Qwen_Qwen2.5-1.5B-Instruct

ðŸ”¥ Starting benchmark for Qwen_Qwen2.5-0.5B-Instruct
hf (pretrained=/mnt/data8tb/Documents/llm/llm_models/Qwen_Qwen2.5-0.5B-Instruct,trust_remote_code=True,device_map=auto), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 6
|                          Tasks                           |Version|     Filter      |n-shot|  Metric   |   | Value |   |Stderr|
|----------------------------------------------------------|------:|-----------------|-----:|-----------|---|------:|---|-----:|
|anli_r1                                                   |      1|none             |     0|acc        |â†‘  | 0.3240|Â±  |0.0148|
|anli_r2                                                   |      1|none             |     0|acc        |â†‘  | 0.3420|Â±  |0.0150|
|anli_r3                                                   |      1|none             |     0|acc        |â†‘  | 0.3475|Â±  |0.0138|
|arc_challenge                                             |      1|none             |     0|acc        |â†‘  | 0.3020|Â±  |0.0134|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3370|Â±  |0.0138|
|bbh                                                       |      3|get-answer       |      |exact_match|â†‘  | 0.2138|Â±  |0.0046|
| - bbh_cot_fewshot_boolean_expressions                    |      4|get-answer       |     3|exact_match|â†‘  | 0.6440|Â±  |0.0303|
| - bbh_cot_fewshot_causal_judgement                       |      4|get-answer       |     3|exact_match|â†‘  | 0.0160|Â±  |0.0092|
| - bbh_cot_fewshot_date_understanding                     |      4|get-answer       |     3|exact_match|â†‘  | 0.2240|Â±  |0.0264|
| - bbh_cot_fewshot_disambiguation_qa                      |      4|get-answer       |     3|exact_match|â†‘  | 0.1280|Â±  |0.0212|
| - bbh_cot_fewshot_dyck_languages                         |      4|get-answer       |     3|exact_match|â†‘  | 0.0000|Â±  |0.0000|
| - bbh_cot_fewshot_formal_fallacies                       |      4|get-answer       |     3|exact_match|â†‘  | 0.1960|Â±  |0.0252|
| - bbh_cot_fewshot_geometric_shapes                       |      4|get-answer       |     3|exact_match|â†‘  | 0.0680|Â±  |0.0160|
| - bbh_cot_fewshot_hyperbaton                             |      4|get-answer       |     3|exact_match|â†‘  | 0.3920|Â±  |0.0309|
| - bbh_cot_fewshot_logical_deduction_five_objects         |      4|get-answer       |     3|exact_match|â†‘  | 0.1600|Â±  |0.0232|
| - bbh_cot_fewshot_logical_deduction_seven_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.0720|Â±  |0.0164|
| - bbh_cot_fewshot_logical_deduction_three_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.3120|Â±  |0.0294|
| - bbh_cot_fewshot_movie_recommendation                   |      4|get-answer       |     3|exact_match|â†‘  | 0.1800|Â±  |0.0243|
| - bbh_cot_fewshot_multistep_arithmetic_two               |      4|get-answer       |     3|exact_match|â†‘  | 0.2320|Â±  |0.0268|
| - bbh_cot_fewshot_navigate                               |      4|get-answer       |     3|exact_match|â†‘  | 0.5480|Â±  |0.0315|
| - bbh_cot_fewshot_object_counting                        |      4|get-answer       |     3|exact_match|â†‘  | 0.4760|Â±  |0.0316|
| - bbh_cot_fewshot_penguins_in_a_table                    |      4|get-answer       |     3|exact_match|â†‘  | 0.1233|Â±  |0.0273|
| - bbh_cot_fewshot_reasoning_about_colored_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.1000|Â±  |0.0190|
| - bbh_cot_fewshot_ruin_names                             |      4|get-answer       |     3|exact_match|â†‘  | 0.1720|Â±  |0.0239|
| - bbh_cot_fewshot_salient_translation_error_detection    |      4|get-answer       |     3|exact_match|â†‘  | 0.0240|Â±  |0.0097|
| - bbh_cot_fewshot_snarks                                 |      4|get-answer       |     3|exact_match|â†‘  | 0.2528|Â±  |0.0327|
| - bbh_cot_fewshot_sports_understanding                   |      4|get-answer       |     3|exact_match|â†‘  | 0.4880|Â±  |0.0317|
| - bbh_cot_fewshot_temporal_sequences                     |      4|get-answer       |     3|exact_match|â†‘  | 0.2640|Â±  |0.0279|
| - bbh_cot_fewshot_tracking_shuffled_objects_five_objects |      4|get-answer       |     3|exact_match|â†‘  | 0.1360|Â±  |0.0217|
| - bbh_cot_fewshot_tracking_shuffled_objects_seven_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.1160|Â±  |0.0203|
| - bbh_cot_fewshot_tracking_shuffled_objects_three_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.3280|Â±  |0.0298|
| - bbh_cot_fewshot_web_of_lies                            |      4|get-answer       |     3|exact_match|â†‘  | 0.0200|Â±  |0.0089|
| - bbh_cot_fewshot_word_sorting                           |      4|get-answer       |     3|exact_match|â†‘  | 0.0240|Â±  |0.0097|
|boolq                                                     |      2|none             |     0|acc        |â†‘  | 0.6768|Â±  |0.0082|
|drop                                                      |      3|none             |     0|em         |â†‘  | 0.0003|Â±  |0.0002|
|                                                          |       |none             |     0|f1         |â†‘  | 0.0286|Â±  |0.0008|
|gpqa_diamond_cot_n_shot                                   |      2|flexible-extract |     0|exact_match|â†‘  | 0.1263|Â±  |0.0237|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_cot_zeroshot                                 |      1|flexible-extract |     0|exact_match|â†‘  | 0.1768|Â±  |0.0272|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0051|Â±  |0.0051|
|gpqa_diamond_generative_n_shot                            |      2|flexible-extract |     0|exact_match|â†‘  | 0.1970|Â±  |0.0283|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_n_shot                                       |      2|none             |     0|acc        |â†‘  | 0.2323|Â±  |0.0301|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2323|Â±  |0.0301|
|gpqa_diamond_zeroshot                                     |      1|none             |     0|acc        |â†‘  | 0.2626|Â±  |0.0314|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2626|Â±  |0.0314|
|gpqa_extended_cot_n_shot                                  |      2|flexible-extract |     0|exact_match|â†‘  | 0.1447|Â±  |0.0151|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_cot_zeroshot                                |      1|flexible-extract |     0|exact_match|â†‘  | 0.1868|Â±  |0.0167|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0018|Â±  |0.0018|
|gpqa_extended_generative_n_shot                           |      2|flexible-extract |     0|exact_match|â†‘  | 0.2015|Â±  |0.0172|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0037|Â±  |0.0026|
|gpqa_extended_n_shot                                      |      2|none             |     0|acc        |â†‘  | 0.2601|Â±  |0.0188|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2601|Â±  |0.0188|
|gpqa_extended_zeroshot                                    |      1|none             |     0|acc        |â†‘  | 0.2802|Â±  |0.0192|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2802|Â±  |0.0192|
|gpqa_main_cot_n_shot                                      |      2|flexible-extract |     0|exact_match|â†‘  | 0.1585|Â±  |0.0173|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_cot_zeroshot                                    |      1|flexible-extract |     0|exact_match|â†‘  | 0.1585|Â±  |0.0173|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0022|Â±  |0.0022|
|gpqa_main_generative_n_shot                               |      2|flexible-extract |     0|exact_match|â†‘  | 0.1808|Â±  |0.0182|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_n_shot                                          |      2|none             |     0|acc        |â†‘  | 0.2701|Â±  |0.0210|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2701|Â±  |0.0210|
|gpqa_main_zeroshot                                        |      1|none             |     0|acc        |â†‘  | 0.2679|Â±  |0.0209|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2679|Â±  |0.0209|
|gsm8k                                                     |      3|flexible-extract |     5|exact_match|â†‘  | 0.3169|Â±  |0.0128|
|                                                          |       |strict-match     |     5|exact_match|â†‘  | 0.2077|Â±  |0.0112|
|hellaswag                                                 |      1|none             |     0|acc        |â†‘  | 0.4049|Â±  |0.0049|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.5241|Â±  |0.0050|
|mmlu                                                      |      2|none             |      |acc        |â†‘  | 0.4576|Â±  |0.0041|
| - humanities                                             |      2|none             |      |acc        |â†‘  | 0.4219|Â±  |0.0069|
|  - formal_logic                                          |      1|none             |     0|acc        |â†‘  | 0.3175|Â±  |0.0416|
|  - high_school_european_history                          |      1|none             |     0|acc        |â†‘  | 0.6000|Â±  |0.0383|
|  - high_school_us_history                                |      1|none             |     0|acc        |â†‘  | 0.5294|Â±  |0.0350|
|  - high_school_world_history                             |      1|none             |     0|acc        |â†‘  | 0.6076|Â±  |0.0318|
|  - international_law                                     |      1|none             |     0|acc        |â†‘  | 0.7438|Â±  |0.0398|
|  - jurisprudence                                         |      1|none             |     0|acc        |â†‘  | 0.6019|Â±  |0.0473|
|  - logical_fallacies                                     |      1|none             |     0|acc        |â†‘  | 0.4724|Â±  |0.0392|
|  - moral_disputes                                        |      1|none             |     0|acc        |â†‘  | 0.5318|Â±  |0.0269|
|  - moral_scenarios                                       |      1|none             |     0|acc        |â†‘  | 0.2380|Â±  |0.0142|
|  - philosophy                                            |      1|none             |     0|acc        |â†‘  | 0.4759|Â±  |0.0284|
|  - prehistory                                            |      1|none             |     0|acc        |â†‘  | 0.5432|Â±  |0.0277|
|  - professional_law                                      |      1|none             |     0|acc        |â†‘  | 0.3520|Â±  |0.0122|
|  - world_religions                                       |      1|none             |     0|acc        |â†‘  | 0.5906|Â±  |0.0377|
| - other                                                  |      2|none             |      |acc        |â†‘  | 0.5082|Â±  |0.0088|
|  - business_ethics                                       |      1|none             |     0|acc        |â†‘  | 0.5300|Â±  |0.0502|
|  - clinical_knowledge                                    |      1|none             |     0|acc        |â†‘  | 0.5094|Â±  |0.0308|
|  - college_medicine                                      |      1|none             |     0|acc        |â†‘  | 0.4509|Â±  |0.0379|
|  - global_facts                                          |      1|none             |     0|acc        |â†‘  | 0.3100|Â±  |0.0465|
|  - human_aging                                           |      1|none             |     0|acc        |â†‘  | 0.5426|Â±  |0.0334|
|  - management                                            |      1|none             |     0|acc        |â†‘  | 0.5728|Â±  |0.0490|
|  - marketing                                             |      1|none             |     0|acc        |â†‘  | 0.7393|Â±  |0.0288|
|  - medical_genetics                                      |      1|none             |     0|acc        |â†‘  | 0.5200|Â±  |0.0502|
|  - miscellaneous                                         |      1|none             |     0|acc        |â†‘  | 0.5556|Â±  |0.0178|
|  - nutrition                                             |      1|none             |     0|acc        |â†‘  | 0.5882|Â±  |0.0282|
|  - professional_accounting                               |      1|none             |     0|acc        |â†‘  | 0.3191|Â±  |0.0278|
|  - professional_medicine                                 |      1|none             |     0|acc        |â†‘  | 0.3676|Â±  |0.0293|
|  - virology                                              |      1|none             |     0|acc        |â†‘  | 0.4337|Â±  |0.0386|
| - social sciences                                        |      2|none             |      |acc        |â†‘  | 0.5301|Â±  |0.0089|
|  - econometrics                                          |      1|none             |     0|acc        |â†‘  | 0.2895|Â±  |0.0427|
|  - high_school_geography                                 |      1|none             |     0|acc        |â†‘  | 0.5657|Â±  |0.0353|
|  - high_school_government_and_politics                   |      1|none             |     0|acc        |â†‘  | 0.5544|Â±  |0.0359|
|  - high_school_macroeconomics                            |      1|none             |     0|acc        |â†‘  | 0.4410|Â±  |0.0252|
|  - high_school_microeconomics                            |      1|none             |     0|acc        |â†‘  | 0.4748|Â±  |0.0324|
|  - high_school_psychology                                |      1|none             |     0|acc        |â†‘  | 0.6183|Â±  |0.0208|
|  - human_sexuality                                       |      1|none             |     0|acc        |â†‘  | 0.5573|Â±  |0.0436|
|  - professional_psychology                               |      1|none             |     0|acc        |â†‘  | 0.4608|Â±  |0.0202|
|  - public_relations                                      |      1|none             |     0|acc        |â†‘  | 0.5273|Â±  |0.0478|
|  - security_studies                                      |      1|none             |     0|acc        |â†‘  | 0.5633|Â±  |0.0318|
|  - sociology                                             |      1|none             |     0|acc        |â†‘  | 0.6667|Â±  |0.0333|
|  - us_foreign_policy                                     |      1|none             |     0|acc        |â†‘  | 0.7200|Â±  |0.0451|
| - stem                                                   |      2|none             |      |acc        |â†‘  | 0.3901|Â±  |0.0085|
|  - abstract_algebra                                      |      1|none             |     0|acc        |â†‘  | 0.3300|Â±  |0.0473|
|  - anatomy                                               |      1|none             |     0|acc        |â†‘  | 0.4000|Â±  |0.0423|
|  - astronomy                                             |      1|none             |     0|acc        |â†‘  | 0.4737|Â±  |0.0406|
|  - college_biology                                       |      1|none             |     0|acc        |â†‘  | 0.4444|Â±  |0.0416|
|  - college_chemistry                                     |      1|none             |     0|acc        |â†‘  | 0.2900|Â±  |0.0456|
|  - college_computer_science                              |      1|none             |     0|acc        |â†‘  | 0.3400|Â±  |0.0476|
|  - college_mathematics                                   |      1|none             |     0|acc        |â†‘  | 0.2700|Â±  |0.0446|
|  - college_physics                                       |      1|none             |     0|acc        |â†‘  | 0.2745|Â±  |0.0444|
|  - computer_security                                     |      1|none             |     0|acc        |â†‘  | 0.6900|Â±  |0.0465|
|  - conceptual_physics                                    |      1|none             |     0|acc        |â†‘  | 0.3830|Â±  |0.0318|
|  - electrical_engineering                                |      1|none             |     0|acc        |â†‘  | 0.5103|Â±  |0.0417|
|  - elementary_mathematics                                |      1|none             |     0|acc        |â†‘  | 0.3307|Â±  |0.0242|
|  - high_school_biology                                   |      1|none             |     0|acc        |â†‘  | 0.5355|Â±  |0.0284|
|  - high_school_chemistry                                 |      1|none             |     0|acc        |â†‘  | 0.4138|Â±  |0.0347|
|  - high_school_computer_science                          |      1|none             |     0|acc        |â†‘  | 0.4400|Â±  |0.0499|
|  - high_school_mathematics                               |      1|none             |     0|acc        |â†‘  | 0.3000|Â±  |0.0279|
|  - high_school_physics                                   |      1|none             |     0|acc        |â†‘  | 0.2517|Â±  |0.0354|
|  - high_school_statistics                                |      1|none             |     0|acc        |â†‘  | 0.3333|Â±  |0.0321|
|  - machine_learning                                      |      1|none             |     0|acc        |â†‘  | 0.4107|Â±  |0.0467|
|nq_open                                                   |      4|remove_whitespace|     0|exact_match|â†‘  | 0.0205|Â±  |0.0024|
|openbookqa                                                |      1|none             |     0|acc        |â†‘  | 0.2440|Â±  |0.0192|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3460|Â±  |0.0213|
|piqa                                                      |      1|none             |     0|acc        |â†‘  | 0.7062|Â±  |0.0106|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.7040|Â±  |0.0107|
|qnli                                                      |      1|none             |     0|acc        |â†‘  | 0.5369|Â±  |0.0067|
|sciq                                                      |      1|none             |     0|acc        |â†‘  | 0.9190|Â±  |0.0086|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.8830|Â±  |0.0102|
|triviaqa                                                  |      3|remove_whitespace|     0|exact_match|â†‘  | 0.1342|Â±  |0.0025|
|truthfulqa_gen                                            |      3|none             |     0|bleu_acc   |â†‘  | 0.3219|Â±  |0.0164|
|                                                          |       |none             |     0|bleu_diff  |â†‘  |-0.2967|Â±  |0.0886|
|                                                          |       |none             |     0|bleu_max   |â†‘  | 2.9561|Â±  |0.1554|
|                                                          |       |none             |     0|rouge1_acc |â†‘  | 0.3770|Â±  |0.0170|
|                                                          |       |none             |     0|rouge1_diff|â†‘  |-0.1350|Â±  |0.2257|
|                                                          |       |none             |     0|rouge1_max |â†‘  |12.8150|Â±  |0.3731|
|                                                          |       |none             |     0|rouge2_acc |â†‘  | 0.2154|Â±  |0.0144|
|                                                          |       |none             |     0|rouge2_diff|â†‘  |-0.5713|Â±  |0.1837|
|                                                          |       |none             |     0|rouge2_max |â†‘  | 5.3013|Â±  |0.2882|
|                                                          |       |none             |     0|rougeL_acc |â†‘  | 0.3537|Â±  |0.0167|
|                                                          |       |none             |     0|rougeL_diff|â†‘  |-0.4059|Â±  |0.2150|
|                                                          |       |none             |     0|rougeL_max |â†‘  |11.6973|Â±  |0.3485|
|truthfulqa_mc1                                            |      2|none             |     0|acc        |â†‘  | 0.2717|Â±  |0.0156|
|truthfulqa_mc2                                            |      3|none             |     0|acc        |â†‘  | 0.4184|Â±  |0.0146|
|winogrande                                                |      1|none             |     0|acc        |â†‘  | 0.5564|Â±  |0.0140|

|      Groups      |Version|  Filter  |n-shot|  Metric   |   |Value |   |Stderr|
|------------------|------:|----------|------|-----------|---|-----:|---|-----:|
|bbh               |      3|get-answer|      |exact_match|â†‘  |0.2138|Â±  |0.0046|
|mmlu              |      2|none      |      |acc        |â†‘  |0.4576|Â±  |0.0041|
| - humanities     |      2|none      |      |acc        |â†‘  |0.4219|Â±  |0.0069|
| - other          |      2|none      |      |acc        |â†‘  |0.5082|Â±  |0.0088|
| - social sciences|      2|none      |      |acc        |â†‘  |0.5301|Â±  |0.0089|
| - stem           |      2|none      |      |acc        |â†‘  |0.3901|Â±  |0.0085|

Qwen_Qwen2.5-0.5B-Instruct: 2h 34m 22s
âœ… Benchmark completed for Qwen_Qwen2.5-0.5B-Instruct

ðŸ”¥ Starting benchmark for deepseek-ai_DeepSeek-R1-Distill-Qwen-1.5B
hf (pretrained=/mnt/data8tb/Documents/llm/llm_models/deepseek-ai_DeepSeek-R1-Distill-Qwen-1.5B,trust_remote_code=True,device_map=auto), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 6
|                          Tasks                           |Version|     Filter      |n-shot|  Metric   |   | Value |   |Stderr|
|----------------------------------------------------------|------:|-----------------|-----:|-----------|---|------:|---|-----:|
|anli_r1                                                   |      1|none             |     0|acc        |â†‘  | 0.3560|Â±  |0.0151|
|anli_r2                                                   |      1|none             |     0|acc        |â†‘  | 0.3620|Â±  |0.0152|
|anli_r3                                                   |      1|none             |     0|acc        |â†‘  | 0.3625|Â±  |0.0139|
|arc_challenge                                             |      1|none             |     0|acc        |â†‘  | 0.3422|Â±  |0.0139|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3464|Â±  |0.0139|
|bbh                                                       |      3|get-answer       |      |exact_match|â†‘  | 0.4059|Â±  |0.0051|
| - bbh_cot_fewshot_boolean_expressions                    |      4|get-answer       |     3|exact_match|â†‘  | 0.8600|Â±  |0.0220|
| - bbh_cot_fewshot_causal_judgement                       |      4|get-answer       |     3|exact_match|â†‘  | 0.3850|Â±  |0.0357|
| - bbh_cot_fewshot_date_understanding                     |      4|get-answer       |     3|exact_match|â†‘  | 0.4520|Â±  |0.0315|
| - bbh_cot_fewshot_disambiguation_qa                      |      4|get-answer       |     3|exact_match|â†‘  | 0.2960|Â±  |0.0289|
| - bbh_cot_fewshot_dyck_languages                         |      4|get-answer       |     3|exact_match|â†‘  | 0.0160|Â±  |0.0080|
| - bbh_cot_fewshot_formal_fallacies                       |      4|get-answer       |     3|exact_match|â†‘  | 0.1600|Â±  |0.0232|
| - bbh_cot_fewshot_geometric_shapes                       |      4|get-answer       |     3|exact_match|â†‘  | 0.2560|Â±  |0.0277|
| - bbh_cot_fewshot_hyperbaton                             |      4|get-answer       |     3|exact_match|â†‘  | 0.3600|Â±  |0.0304|
| - bbh_cot_fewshot_logical_deduction_five_objects         |      4|get-answer       |     3|exact_match|â†‘  | 0.1520|Â±  |0.0228|
| - bbh_cot_fewshot_logical_deduction_seven_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.0080|Â±  |0.0056|
| - bbh_cot_fewshot_logical_deduction_three_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.6280|Â±  |0.0306|
| - bbh_cot_fewshot_movie_recommendation                   |      4|get-answer       |     3|exact_match|â†‘  | 0.5080|Â±  |0.0317|
| - bbh_cot_fewshot_multistep_arithmetic_two               |      4|get-answer       |     3|exact_match|â†‘  | 0.9280|Â±  |0.0164|
| - bbh_cot_fewshot_navigate                               |      4|get-answer       |     3|exact_match|â†‘  | 0.7840|Â±  |0.0261|
| - bbh_cot_fewshot_object_counting                        |      4|get-answer       |     3|exact_match|â†‘  | 0.6320|Â±  |0.0306|
| - bbh_cot_fewshot_penguins_in_a_table                    |      4|get-answer       |     3|exact_match|â†‘  | 0.4932|Â±  |0.0415|
| - bbh_cot_fewshot_reasoning_about_colored_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.5560|Â±  |0.0315|
| - bbh_cot_fewshot_ruin_names                             |      4|get-answer       |     3|exact_match|â†‘  | 0.2440|Â±  |0.0272|
| - bbh_cot_fewshot_salient_translation_error_detection    |      4|get-answer       |     3|exact_match|â†‘  | 0.1280|Â±  |0.0212|
| - bbh_cot_fewshot_snarks                                 |      4|get-answer       |     3|exact_match|â†‘  | 0.1798|Â±  |0.0289|
| - bbh_cot_fewshot_sports_understanding                   |      4|get-answer       |     3|exact_match|â†‘  | 0.4760|Â±  |0.0316|
| - bbh_cot_fewshot_temporal_sequences                     |      4|get-answer       |     3|exact_match|â†‘  | 0.1760|Â±  |0.0241|
| - bbh_cot_fewshot_tracking_shuffled_objects_five_objects |      4|get-answer       |     3|exact_match|â†‘  | 0.4240|Â±  |0.0313|
| - bbh_cot_fewshot_tracking_shuffled_objects_seven_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.1800|Â±  |0.0243|
| - bbh_cot_fewshot_tracking_shuffled_objects_three_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.6280|Â±  |0.0306|
| - bbh_cot_fewshot_web_of_lies                            |      4|get-answer       |     3|exact_match|â†‘  | 0.9200|Â±  |0.0172|
| - bbh_cot_fewshot_word_sorting                           |      4|get-answer       |     3|exact_match|â†‘  | 0.0960|Â±  |0.0187|
|boolq                                                     |      2|none             |     0|acc        |â†‘  | 0.6801|Â±  |0.0082|
|drop                                                      |      3|none             |     0|em         |â†‘  | 0.0008|Â±  |0.0003|
|                                                          |       |none             |     0|f1         |â†‘  | 0.0507|Â±  |0.0013|
|gpqa_diamond_cot_n_shot                                   |      2|flexible-extract |     0|exact_match|â†‘  | 0.0606|Â±  |0.0170|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_cot_zeroshot                                 |      1|flexible-extract |     0|exact_match|â†‘  | 0.0606|Â±  |0.0170|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_generative_n_shot                            |      2|flexible-extract |     0|exact_match|â†‘  | 0.1364|Â±  |0.0245|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_n_shot                                       |      2|none             |     0|acc        |â†‘  | 0.2727|Â±  |0.0317|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2727|Â±  |0.0317|
|gpqa_diamond_zeroshot                                     |      1|none             |     0|acc        |â†‘  | 0.2727|Â±  |0.0317|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2727|Â±  |0.0317|
|gpqa_extended_cot_n_shot                                  |      2|flexible-extract |     0|exact_match|â†‘  | 0.0788|Â±  |0.0115|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_cot_zeroshot                                |      1|flexible-extract |     0|exact_match|â†‘  | 0.0824|Â±  |0.0118|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_generative_n_shot                           |      2|flexible-extract |     0|exact_match|â†‘  | 0.1740|Â±  |0.0162|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_n_shot                                      |      2|none             |     0|acc        |â†‘  | 0.2418|Â±  |0.0183|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2418|Â±  |0.0183|
|gpqa_extended_zeroshot                                    |      1|none             |     0|acc        |â†‘  | 0.3205|Â±  |0.0200|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3205|Â±  |0.0200|
|gpqa_main_cot_n_shot                                      |      2|flexible-extract |     0|exact_match|â†‘  | 0.0714|Â±  |0.0122|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_cot_zeroshot                                    |      1|flexible-extract |     0|exact_match|â†‘  | 0.0759|Â±  |0.0125|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_generative_n_shot                               |      2|flexible-extract |     0|exact_match|â†‘  | 0.1451|Â±  |0.0167|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_n_shot                                          |      2|none             |     0|acc        |â†‘  | 0.2612|Â±  |0.0208|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2612|Â±  |0.0208|
|gpqa_main_zeroshot                                        |      1|none             |     0|acc        |â†‘  | 0.2723|Â±  |0.0211|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2723|Â±  |0.0211|
|gsm8k                                                     |      3|flexible-extract |     5|exact_match|â†‘  | 0.7074|Â±  |0.0125|
|                                                          |       |strict-match     |     5|exact_match|â†‘  | 0.7013|Â±  |0.0126|
|hellaswag                                                 |      1|none             |     0|acc        |â†‘  | 0.3633|Â±  |0.0048|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.4467|Â±  |0.0050|
|mmlu                                                      |      2|none             |      |acc        |â†‘  | 0.3606|Â±  |0.0040|
| - humanities                                             |      2|none             |      |acc        |â†‘  | 0.3135|Â±  |0.0067|
|  - formal_logic                                          |      1|none             |     0|acc        |â†‘  | 0.3968|Â±  |0.0438|
|  - high_school_european_history                          |      1|none             |     0|acc        |â†‘  | 0.3515|Â±  |0.0373|
|  - high_school_us_history                                |      1|none             |     0|acc        |â†‘  | 0.3235|Â±  |0.0328|
|  - high_school_world_history                             |      1|none             |     0|acc        |â†‘  | 0.4135|Â±  |0.0321|
|  - international_law                                     |      1|none             |     0|acc        |â†‘  | 0.4463|Â±  |0.0454|
|  - jurisprudence                                         |      1|none             |     0|acc        |â†‘  | 0.4259|Â±  |0.0478|
|  - logical_fallacies                                     |      1|none             |     0|acc        |â†‘  | 0.4110|Â±  |0.0387|
|  - moral_disputes                                        |      1|none             |     0|acc        |â†‘  | 0.3555|Â±  |0.0258|
|  - moral_scenarios                                       |      1|none             |     0|acc        |â†‘  | 0.2447|Â±  |0.0144|
|  - philosophy                                            |      1|none             |     0|acc        |â†‘  | 0.4051|Â±  |0.0279|
|  - prehistory                                            |      1|none             |     0|acc        |â†‘  | 0.3765|Â±  |0.0270|
|  - professional_law                                      |      1|none             |     0|acc        |â†‘  | 0.2595|Â±  |0.0112|
|  - world_religions                                       |      1|none             |     0|acc        |â†‘  | 0.2807|Â±  |0.0345|
| - other                                                  |      2|none             |      |acc        |â†‘  | 0.3859|Â±  |0.0087|
|  - business_ethics                                       |      1|none             |     0|acc        |â†‘  | 0.4300|Â±  |0.0498|
|  - clinical_knowledge                                    |      1|none             |     0|acc        |â†‘  | 0.3811|Â±  |0.0299|
|  - college_medicine                                      |      1|none             |     0|acc        |â†‘  | 0.3468|Â±  |0.0363|
|  - global_facts                                          |      1|none             |     0|acc        |â†‘  | 0.3300|Â±  |0.0473|
|  - human_aging                                           |      1|none             |     0|acc        |â†‘  | 0.3767|Â±  |0.0325|
|  - management                                            |      1|none             |     0|acc        |â†‘  | 0.5146|Â±  |0.0495|
|  - marketing                                             |      1|none             |     0|acc        |â†‘  | 0.5726|Â±  |0.0324|
|  - medical_genetics                                      |      1|none             |     0|acc        |â†‘  | 0.3900|Â±  |0.0490|
|  - miscellaneous                                         |      1|none             |     0|acc        |â†‘  | 0.3921|Â±  |0.0175|
|  - nutrition                                             |      1|none             |     0|acc        |â†‘  | 0.4118|Â±  |0.0282|
|  - professional_accounting                               |      1|none             |     0|acc        |â†‘  | 0.2943|Â±  |0.0272|
|  - professional_medicine                                 |      1|none             |     0|acc        |â†‘  | 0.2721|Â±  |0.0270|
|  - virology                                              |      1|none             |     0|acc        |â†‘  | 0.3735|Â±  |0.0377|
| - social sciences                                        |      2|none             |      |acc        |â†‘  | 0.4027|Â±  |0.0088|
|  - econometrics                                          |      1|none             |     0|acc        |â†‘  | 0.3070|Â±  |0.0434|
|  - high_school_geography                                 |      1|none             |     0|acc        |â†‘  | 0.3535|Â±  |0.0341|
|  - high_school_government_and_politics                   |      1|none             |     0|acc        |â†‘  | 0.3938|Â±  |0.0353|
|  - high_school_macroeconomics                            |      1|none             |     0|acc        |â†‘  | 0.3821|Â±  |0.0246|
|  - high_school_microeconomics                            |      1|none             |     0|acc        |â†‘  | 0.4580|Â±  |0.0324|
|  - high_school_psychology                                |      1|none             |     0|acc        |â†‘  | 0.4624|Â±  |0.0214|
|  - human_sexuality                                       |      1|none             |     0|acc        |â†‘  | 0.4351|Â±  |0.0435|
|  - professional_psychology                               |      1|none             |     0|acc        |â†‘  | 0.3317|Â±  |0.0190|
|  - public_relations                                      |      1|none             |     0|acc        |â†‘  | 0.3818|Â±  |0.0465|
|  - security_studies                                      |      1|none             |     0|acc        |â†‘  | 0.4367|Â±  |0.0318|
|  - sociology                                             |      1|none             |     0|acc        |â†‘  | 0.4527|Â±  |0.0352|
|  - us_foreign_policy                                     |      1|none             |     0|acc        |â†‘  | 0.4800|Â±  |0.0502|
| - stem                                                   |      2|none             |      |acc        |â†‘  | 0.3650|Â±  |0.0085|
|  - abstract_algebra                                      |      1|none             |     0|acc        |â†‘  | 0.3000|Â±  |0.0461|
|  - anatomy                                               |      1|none             |     0|acc        |â†‘  | 0.3037|Â±  |0.0397|
|  - astronomy                                             |      1|none             |     0|acc        |â†‘  | 0.4013|Â±  |0.0399|
|  - college_biology                                       |      1|none             |     0|acc        |â†‘  | 0.3056|Â±  |0.0385|
|  - college_chemistry                                     |      1|none             |     0|acc        |â†‘  | 0.3100|Â±  |0.0465|
|  - college_computer_science                              |      1|none             |     0|acc        |â†‘  | 0.3900|Â±  |0.0490|
|  - college_mathematics                                   |      1|none             |     0|acc        |â†‘  | 0.3800|Â±  |0.0488|
|  - college_physics                                       |      1|none             |     0|acc        |â†‘  | 0.2941|Â±  |0.0453|
|  - computer_security                                     |      1|none             |     0|acc        |â†‘  | 0.3600|Â±  |0.0482|
|  - conceptual_physics                                    |      1|none             |     0|acc        |â†‘  | 0.4043|Â±  |0.0321|
|  - electrical_engineering                                |      1|none             |     0|acc        |â†‘  | 0.4345|Â±  |0.0413|
|  - elementary_mathematics                                |      1|none             |     0|acc        |â†‘  | 0.4471|Â±  |0.0256|
|  - high_school_biology                                   |      1|none             |     0|acc        |â†‘  | 0.4258|Â±  |0.0281|
|  - high_school_chemistry                                 |      1|none             |     0|acc        |â†‘  | 0.3202|Â±  |0.0328|
|  - high_school_computer_science                          |      1|none             |     0|acc        |â†‘  | 0.3900|Â±  |0.0490|
|  - high_school_mathematics                               |      1|none             |     0|acc        |â†‘  | 0.3519|Â±  |0.0291|
|  - high_school_physics                                   |      1|none             |     0|acc        |â†‘  | 0.1987|Â±  |0.0326|
|  - high_school_statistics                                |      1|none             |     0|acc        |â†‘  | 0.3472|Â±  |0.0325|
|  - machine_learning                                      |      1|none             |     0|acc        |â†‘  | 0.3393|Â±  |0.0449|
|nq_open                                                   |      4|remove_whitespace|     0|exact_match|â†‘  | 0.0064|Â±  |0.0013|
|openbookqa                                                |      1|none             |     0|acc        |â†‘  | 0.1980|Â±  |0.0178|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3080|Â±  |0.0207|
|piqa                                                      |      1|none             |     0|acc        |â†‘  | 0.6513|Â±  |0.0111|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.6578|Â±  |0.0111|
|qnli                                                      |      1|none             |     0|acc        |â†‘  | 0.5054|Â±  |0.0068|
|sciq                                                      |      1|none             |     0|acc        |â†‘  | 0.8990|Â±  |0.0095|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.8450|Â±  |0.0115|
|triviaqa                                                  |      3|remove_whitespace|     0|exact_match|â†‘  | 0.0090|Â±  |0.0007|
|truthfulqa_gen                                            |      3|none             |     0|bleu_acc   |â†‘  | 0.3268|Â±  |0.0164|
|                                                          |       |none             |     0|bleu_diff  |â†‘  |-2.4557|Â±  |0.4017|
|                                                          |       |none             |     0|bleu_max   |â†‘  |12.6086|Â±  |0.4908|
|                                                          |       |none             |     0|rouge1_acc |â†‘  | 0.3476|Â±  |0.0167|
|                                                          |       |none             |     0|rouge1_diff|â†‘  |-3.9220|Â±  |0.5617|
|                                                          |       |none             |     0|rouge1_max |â†‘  |34.8664|Â±  |0.7473|
|                                                          |       |none             |     0|rouge2_acc |â†‘  | 0.2289|Â±  |0.0147|
|                                                          |       |none             |     0|rouge2_diff|â†‘  |-5.0782|Â±  |0.6278|
|                                                          |       |none             |     0|rouge2_max |â†‘  |20.7313|Â±  |0.7617|
|                                                          |       |none             |     0|rougeL_acc |â†‘  | 0.3329|Â±  |0.0165|
|                                                          |       |none             |     0|rougeL_diff|â†‘  |-4.0907|Â±  |0.5646|
|                                                          |       |none             |     0|rougeL_max |â†‘  |32.3187|Â±  |0.7355|
|truthfulqa_mc1                                            |      2|none             |     0|acc        |â†‘  | 0.2938|Â±  |0.0159|
|truthfulqa_mc2                                            |      3|none             |     0|acc        |â†‘  | 0.4517|Â±  |0.0155|
|winogrande                                                |      1|none             |     0|acc        |â†‘  | 0.5493|Â±  |0.0140|

|      Groups      |Version|  Filter  |n-shot|  Metric   |   |Value |   |Stderr|
|------------------|------:|----------|------|-----------|---|-----:|---|-----:|
|bbh               |      3|get-answer|      |exact_match|â†‘  |0.4059|Â±  |0.0051|
|mmlu              |      2|none      |      |acc        |â†‘  |0.3606|Â±  |0.0040|
| - humanities     |      2|none      |      |acc        |â†‘  |0.3135|Â±  |0.0067|
| - other          |      2|none      |      |acc        |â†‘  |0.3859|Â±  |0.0087|
| - social sciences|      2|none      |      |acc        |â†‘  |0.4027|Â±  |0.0088|
| - stem           |      2|none      |      |acc        |â†‘  |0.3650|Â±  |0.0085|

deepseek-ai_DeepSeek-R1-Distill-Qwen-1.5B: 3h 41m 4s
âœ… Benchmark completed for deepseek-ai_DeepSeek-R1-Distill-Qwen-1.5B

ðŸ”¥ Starting benchmark for Qwen_Qwen3-1.7B
hf (pretrained=/mnt/data8tb/Documents/llm/llm_models/Qwen_Qwen3-1.7B,trust_remote_code=True,device_map=auto), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 6
|                          Tasks                           |Version|     Filter      |n-shot|  Metric   |   | Value |   |Stderr|
|----------------------------------------------------------|------:|-----------------|-----:|-----------|---|------:|---|-----:|
|anli_r1                                                   |      1|none             |     0|acc        |â†‘  | 0.4100|Â±  |0.0156|
|anli_r2                                                   |      1|none             |     0|acc        |â†‘  | 0.4040|Â±  |0.0155|
|anli_r3                                                   |      1|none             |     0|acc        |â†‘  | 0.4342|Â±  |0.0143|
|arc_challenge                                             |      1|none             |     0|acc        |â†‘  | 0.3985|Â±  |0.0143|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.4343|Â±  |0.0145|
|bbh                                                       |      3|get-answer       |      |exact_match|â†‘  | 0.4826|Â±  |0.0048|
| - bbh_cot_fewshot_boolean_expressions                    |      4|get-answer       |     3|exact_match|â†‘  | 0.9120|Â±  |0.0180|
| - bbh_cot_fewshot_causal_judgement                       |      4|get-answer       |     3|exact_match|â†‘  | 0.4920|Â±  |0.0367|
| - bbh_cot_fewshot_date_understanding                     |      4|get-answer       |     3|exact_match|â†‘  | 0.6280|Â±  |0.0306|
| - bbh_cot_fewshot_disambiguation_qa                      |      4|get-answer       |     3|exact_match|â†‘  | 0.2840|Â±  |0.0286|
| - bbh_cot_fewshot_dyck_languages                         |      4|get-answer       |     3|exact_match|â†‘  | 0.0520|Â±  |0.0141|
| - bbh_cot_fewshot_formal_fallacies                       |      4|get-answer       |     3|exact_match|â†‘  | 0.1800|Â±  |0.0243|
| - bbh_cot_fewshot_geometric_shapes                       |      4|get-answer       |     3|exact_match|â†‘  | 0.3040|Â±  |0.0292|
| - bbh_cot_fewshot_hyperbaton                             |      4|get-answer       |     3|exact_match|â†‘  | 0.6840|Â±  |0.0295|
| - bbh_cot_fewshot_logical_deduction_five_objects         |      4|get-answer       |     3|exact_match|â†‘  | 0.1200|Â±  |0.0206|
| - bbh_cot_fewshot_logical_deduction_seven_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.0000|Â±  |0.0000|
| - bbh_cot_fewshot_logical_deduction_three_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.7880|Â±  |0.0259|
| - bbh_cot_fewshot_movie_recommendation                   |      4|get-answer       |     3|exact_match|â†‘  | 0.5720|Â±  |0.0314|
| - bbh_cot_fewshot_multistep_arithmetic_two               |      4|get-answer       |     3|exact_match|â†‘  | 0.9640|Â±  |0.0118|
| - bbh_cot_fewshot_navigate                               |      4|get-answer       |     3|exact_match|â†‘  | 0.8880|Â±  |0.0200|
| - bbh_cot_fewshot_object_counting                        |      4|get-answer       |     3|exact_match|â†‘  | 0.7560|Â±  |0.0272|
| - bbh_cot_fewshot_penguins_in_a_table                    |      4|get-answer       |     3|exact_match|â†‘  | 0.5890|Â±  |0.0409|
| - bbh_cot_fewshot_reasoning_about_colored_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.5960|Â±  |0.0311|
| - bbh_cot_fewshot_ruin_names                             |      4|get-answer       |     3|exact_match|â†‘  | 0.5000|Â±  |0.0317|
| - bbh_cot_fewshot_salient_translation_error_detection    |      4|get-answer       |     3|exact_match|â†‘  | 0.0840|Â±  |0.0176|
| - bbh_cot_fewshot_snarks                                 |      4|get-answer       |     3|exact_match|â†‘  | 0.4101|Â±  |0.0370|
| - bbh_cot_fewshot_sports_understanding                   |      4|get-answer       |     3|exact_match|â†‘  | 0.7600|Â±  |0.0271|
| - bbh_cot_fewshot_temporal_sequences                     |      4|get-answer       |     3|exact_match|â†‘  | 0.2920|Â±  |0.0288|
| - bbh_cot_fewshot_tracking_shuffled_objects_five_objects |      4|get-answer       |     3|exact_match|â†‘  | 0.2080|Â±  |0.0257|
| - bbh_cot_fewshot_tracking_shuffled_objects_seven_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.0000|Â±  |0.0000|
| - bbh_cot_fewshot_tracking_shuffled_objects_three_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.8120|Â±  |0.0248|
| - bbh_cot_fewshot_web_of_lies                            |      4|get-answer       |     3|exact_match|â†‘  | 1.0000|Â±  |0.0000|
| - bbh_cot_fewshot_word_sorting                           |      4|get-answer       |     3|exact_match|â†‘  | 0.1800|Â±  |0.0243|
|boolq                                                     |      2|none             |     0|acc        |â†‘  | 0.7765|Â±  |0.0073|
|drop                                                      |      3|none             |     0|em         |â†‘  | 0.0031|Â±  |0.0006|
|                                                          |       |none             |     0|f1         |â†‘  | 0.0753|Â±  |0.0018|
|gpqa_diamond_cot_n_shot                                   |      2|flexible-extract |     0|exact_match|â†‘  | 0.0758|Â±  |0.0189|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0051|Â±  |0.0051|
|gpqa_diamond_cot_zeroshot                                 |      1|flexible-extract |     0|exact_match|â†‘  | 0.0253|Â±  |0.0112|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_generative_n_shot                            |      2|flexible-extract |     0|exact_match|â†‘  | 0.1263|Â±  |0.0237|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_n_shot                                       |      2|none             |     0|acc        |â†‘  | 0.2475|Â±  |0.0307|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2475|Â±  |0.0307|
|gpqa_diamond_zeroshot                                     |      1|none             |     0|acc        |â†‘  | 0.3232|Â±  |0.0333|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3232|Â±  |0.0333|
|gpqa_extended_cot_n_shot                                  |      2|flexible-extract |     0|exact_match|â†‘  | 0.0824|Â±  |0.0118|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0018|Â±  |0.0018|
|gpqa_extended_cot_zeroshot                                |      1|flexible-extract |     0|exact_match|â†‘  | 0.0934|Â±  |0.0125|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0055|Â±  |0.0032|
|gpqa_extended_generative_n_shot                           |      2|flexible-extract |     0|exact_match|â†‘  | 0.1722|Â±  |0.0162|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_n_shot                                      |      2|none             |     0|acc        |â†‘  | 0.2949|Â±  |0.0195|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2949|Â±  |0.0195|
|gpqa_extended_zeroshot                                    |      1|none             |     0|acc        |â†‘  | 0.2967|Â±  |0.0196|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2967|Â±  |0.0196|
|gpqa_main_cot_n_shot                                      |      2|flexible-extract |     0|exact_match|â†‘  | 0.0893|Â±  |0.0135|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_cot_zeroshot                                    |      1|flexible-extract |     0|exact_match|â†‘  | 0.0804|Â±  |0.0129|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0045|Â±  |0.0032|
|gpqa_main_generative_n_shot                               |      2|flexible-extract |     0|exact_match|â†‘  | 0.1473|Â±  |0.0168|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_n_shot                                          |      2|none             |     0|acc        |â†‘  | 0.2612|Â±  |0.0208|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2612|Â±  |0.0208|
|gpqa_main_zeroshot                                        |      1|none             |     0|acc        |â†‘  | 0.2902|Â±  |0.0215|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2902|Â±  |0.0215|
|gsm8k                                                     |      3|flexible-extract |     5|exact_match|â†‘  | 0.6922|Â±  |0.0127|
|                                                          |       |strict-match     |     5|exact_match|â†‘  | 0.6899|Â±  |0.0127|
|hellaswag                                                 |      1|none             |     0|acc        |â†‘  | 0.4606|Â±  |0.0050|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.6038|Â±  |0.0049|
|mmlu                                                      |      2|none             |      |acc        |â†‘  | 0.5538|Â±  |0.0040|
| - humanities                                             |      2|none             |      |acc        |â†‘  | 0.4854|Â±  |0.0069|
|  - formal_logic                                          |      1|none             |     0|acc        |â†‘  | 0.4841|Â±  |0.0447|
|  - high_school_european_history                          |      1|none             |     0|acc        |â†‘  | 0.6727|Â±  |0.0366|
|  - high_school_us_history                                |      1|none             |     0|acc        |â†‘  | 0.6618|Â±  |0.0332|
|  - high_school_world_history                             |      1|none             |     0|acc        |â†‘  | 0.7046|Â±  |0.0297|
|  - international_law                                     |      1|none             |     0|acc        |â†‘  | 0.6364|Â±  |0.0439|
|  - jurisprudence                                         |      1|none             |     0|acc        |â†‘  | 0.6852|Â±  |0.0449|
|  - logical_fallacies                                     |      1|none             |     0|acc        |â†‘  | 0.7055|Â±  |0.0358|
|  - moral_disputes                                        |      1|none             |     0|acc        |â†‘  | 0.6069|Â±  |0.0263|
|  - moral_scenarios                                       |      1|none             |     0|acc        |â†‘  | 0.2425|Â±  |0.0143|
|  - philosophy                                            |      1|none             |     0|acc        |â†‘  | 0.6141|Â±  |0.0276|
|  - prehistory                                            |      1|none             |     0|acc        |â†‘  | 0.6080|Â±  |0.0272|
|  - professional_law                                      |      1|none             |     0|acc        |â†‘  | 0.3924|Â±  |0.0125|
|  - world_religions                                       |      1|none             |     0|acc        |â†‘  | 0.7427|Â±  |0.0335|
| - other                                                  |      2|none             |      |acc        |â†‘  | 0.5993|Â±  |0.0085|
|  - business_ethics                                       |      1|none             |     0|acc        |â†‘  | 0.5900|Â±  |0.0494|
|  - clinical_knowledge                                    |      1|none             |     0|acc        |â†‘  | 0.6000|Â±  |0.0302|
|  - college_medicine                                      |      1|none             |     0|acc        |â†‘  | 0.5896|Â±  |0.0375|
|  - global_facts                                          |      1|none             |     0|acc        |â†‘  | 0.2300|Â±  |0.0423|
|  - human_aging                                           |      1|none             |     0|acc        |â†‘  | 0.5695|Â±  |0.0332|
|  - management                                            |      1|none             |     0|acc        |â†‘  | 0.6699|Â±  |0.0466|
|  - marketing                                             |      1|none             |     0|acc        |â†‘  | 0.8248|Â±  |0.0249|
|  - medical_genetics                                      |      1|none             |     0|acc        |â†‘  | 0.6400|Â±  |0.0482|
|  - miscellaneous                                         |      1|none             |     0|acc        |â†‘  | 0.6871|Â±  |0.0166|
|  - nutrition                                             |      1|none             |     0|acc        |â†‘  | 0.5850|Â±  |0.0282|
|  - professional_accounting                               |      1|none             |     0|acc        |â†‘  | 0.4149|Â±  |0.0294|
|  - professional_medicine                                 |      1|none             |     0|acc        |â†‘  | 0.5515|Â±  |0.0302|
|  - virology                                              |      1|none             |     0|acc        |â†‘  | 0.4940|Â±  |0.0389|
| - social sciences                                        |      2|none             |      |acc        |â†‘  | 0.6341|Â±  |0.0085|
|  - econometrics                                          |      1|none             |     0|acc        |â†‘  | 0.4825|Â±  |0.0470|
|  - high_school_geography                                 |      1|none             |     0|acc        |â†‘  | 0.6919|Â±  |0.0329|
|  - high_school_government_and_politics                   |      1|none             |     0|acc        |â†‘  | 0.6995|Â±  |0.0331|
|  - high_school_macroeconomics                            |      1|none             |     0|acc        |â†‘  | 0.5410|Â±  |0.0253|
|  - high_school_microeconomics                            |      1|none             |     0|acc        |â†‘  | 0.6345|Â±  |0.0313|
|  - high_school_psychology                                |      1|none             |     0|acc        |â†‘  | 0.7725|Â±  |0.0180|
|  - human_sexuality                                       |      1|none             |     0|acc        |â†‘  | 0.6641|Â±  |0.0414|
|  - professional_psychology                               |      1|none             |     0|acc        |â†‘  | 0.5523|Â±  |0.0201|
|  - public_relations                                      |      1|none             |     0|acc        |â†‘  | 0.5455|Â±  |0.0477|
|  - security_studies                                      |      1|none             |     0|acc        |â†‘  | 0.5918|Â±  |0.0315|
|  - sociology                                             |      1|none             |     0|acc        |â†‘  | 0.6816|Â±  |0.0329|
|  - us_foreign_policy                                     |      1|none             |     0|acc        |â†‘  | 0.7400|Â±  |0.0441|
| - stem                                                   |      2|none             |      |acc        |â†‘  | 0.5325|Â±  |0.0087|
|  - abstract_algebra                                      |      1|none             |     0|acc        |â†‘  | 0.3900|Â±  |0.0490|
|  - anatomy                                               |      1|none             |     0|acc        |â†‘  | 0.5259|Â±  |0.0431|
|  - astronomy                                             |      1|none             |     0|acc        |â†‘  | 0.6382|Â±  |0.0391|
|  - college_biology                                       |      1|none             |     0|acc        |â†‘  | 0.6806|Â±  |0.0390|
|  - college_chemistry                                     |      1|none             |     0|acc        |â†‘  | 0.4200|Â±  |0.0496|
|  - college_computer_science                              |      1|none             |     0|acc        |â†‘  | 0.4300|Â±  |0.0498|
|  - college_mathematics                                   |      1|none             |     0|acc        |â†‘  | 0.3900|Â±  |0.0490|
|  - college_physics                                       |      1|none             |     0|acc        |â†‘  | 0.3333|Â±  |0.0469|
|  - computer_security                                     |      1|none             |     0|acc        |â†‘  | 0.7100|Â±  |0.0456|
|  - conceptual_physics                                    |      1|none             |     0|acc        |â†‘  | 0.6511|Â±  |0.0312|
|  - electrical_engineering                                |      1|none             |     0|acc        |â†‘  | 0.6000|Â±  |0.0408|
|  - elementary_mathematics                                |      1|none             |     0|acc        |â†‘  | 0.5238|Â±  |0.0257|
|  - high_school_biology                                   |      1|none             |     0|acc        |â†‘  | 0.6806|Â±  |0.0265|
|  - high_school_chemistry                                 |      1|none             |     0|acc        |â†‘  | 0.5320|Â±  |0.0351|
|  - high_school_computer_science                          |      1|none             |     0|acc        |â†‘  | 0.6700|Â±  |0.0473|
|  - high_school_mathematics                               |      1|none             |     0|acc        |â†‘  | 0.3481|Â±  |0.0290|
|  - high_school_physics                                   |      1|none             |     0|acc        |â†‘  | 0.4106|Â±  |0.0402|
|  - high_school_statistics                                |      1|none             |     0|acc        |â†‘  | 0.5463|Â±  |0.0340|
|  - machine_learning                                      |      1|none             |     0|acc        |â†‘  | 0.4196|Â±  |0.0468|
|nq_open                                                   |      4|remove_whitespace|     0|exact_match|â†‘  | 0.0222|Â±  |0.0025|
|openbookqa                                                |      1|none             |     0|acc        |â†‘  | 0.2820|Â±  |0.0201|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3760|Â±  |0.0217|
|piqa                                                      |      1|none             |     0|acc        |â†‘  | 0.7242|Â±  |0.0104|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.7203|Â±  |0.0105|
|qnli                                                      |      1|none             |     0|acc        |â†‘  | 0.5105|Â±  |0.0068|
|sciq                                                      |      1|none             |     0|acc        |â†‘  | 0.9310|Â±  |0.0080|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.9140|Â±  |0.0089|
|triviaqa                                                  |      3|remove_whitespace|     0|exact_match|â†‘  | 0.1350|Â±  |0.0026|
|truthfulqa_gen                                            |      3|none             |     0|bleu_acc   |â†‘  | 0.4688|Â±  |0.0175|
|                                                          |       |none             |     0|bleu_diff  |â†‘  | 2.7742|Â±  |0.9353|
|                                                          |       |none             |     0|bleu_max   |â†‘  |26.4045|Â±  |0.8442|
|                                                          |       |none             |     0|rouge1_acc |â†‘  | 0.4627|Â±  |0.0175|
|                                                          |       |none             |     0|rouge1_diff|â†‘  | 4.5361|Â±  |1.2612|
|                                                          |       |none             |     0|rouge1_max |â†‘  |50.0513|Â±  |0.9614|
|                                                          |       |none             |     0|rouge2_acc |â†‘  | 0.3354|Â±  |0.0165|
|                                                          |       |none             |     0|rouge2_diff|â†‘  | 2.8338|Â±  |1.3839|
|                                                          |       |none             |     0|rouge2_max |â†‘  |34.8104|Â±  |1.1528|
|                                                          |       |none             |     0|rougeL_acc |â†‘  | 0.4529|Â±  |0.0174|
|                                                          |       |none             |     0|rougeL_diff|â†‘  | 4.3615|Â±  |1.2613|
|                                                          |       |none             |     0|rougeL_max |â†‘  |47.8592|Â±  |0.9740|
|truthfulqa_mc1                                            |      2|none             |     0|acc        |â†‘  | 0.2950|Â±  |0.0160|
|truthfulqa_mc2                                            |      3|none             |     0|acc        |â†‘  | 0.4588|Â±  |0.0155|
|winogrande                                                |      1|none             |     0|acc        |â†‘  | 0.6085|Â±  |0.0137|

|      Groups      |Version|  Filter  |n-shot|  Metric   |   |Value |   |Stderr|
|------------------|------:|----------|------|-----------|---|-----:|---|-----:|
|bbh               |      3|get-answer|      |exact_match|â†‘  |0.4826|Â±  |0.0048|
|mmlu              |      2|none      |      |acc        |â†‘  |0.5538|Â±  |0.0040|
| - humanities     |      2|none      |      |acc        |â†‘  |0.4854|Â±  |0.0069|
| - other          |      2|none      |      |acc        |â†‘  |0.5993|Â±  |0.0085|
| - social sciences|      2|none      |      |acc        |â†‘  |0.6341|Â±  |0.0085|
| - stem           |      2|none      |      |acc        |â†‘  |0.5325|Â±  |0.0087|

Qwen_Qwen3-1.7B: 4h 25m 25s
âœ… Benchmark completed for Qwen_Qwen3-1.7B

ðŸ”¥ Starting benchmark for Qwen_Qwen3-0.6B
hf (pretrained=/mnt/data8tb/Documents/llm/llm_models/Qwen_Qwen3-0.6B,trust_remote_code=True,device_map=auto), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 6
|                          Tasks                           |Version|     Filter      |n-shot|  Metric   |   | Value |   |Stderr|
|----------------------------------------------------------|------:|-----------------|-----:|-----------|---|------:|---|-----:|
|anli_r1                                                   |      1|none             |     0|acc        |â†‘  | 0.3430|Â±  |0.0150|
|anli_r2                                                   |      1|none             |     0|acc        |â†‘  | 0.3190|Â±  |0.0147|
|anli_r3                                                   |      1|none             |     0|acc        |â†‘  | 0.3442|Â±  |0.0137|
|arc_challenge                                             |      1|none             |     0|acc        |â†‘  | 0.3123|Â±  |0.0135|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3422|Â±  |0.0139|
|bbh                                                       |      3|get-answer       |      |exact_match|â†‘  | 0.4148|Â±  |0.0053|
| - bbh_cot_fewshot_boolean_expressions                    |      4|get-answer       |     3|exact_match|â†‘  | 0.7560|Â±  |0.0272|
| - bbh_cot_fewshot_causal_judgement                       |      4|get-answer       |     3|exact_match|â†‘  | 0.3529|Â±  |0.0350|
| - bbh_cot_fewshot_date_understanding                     |      4|get-answer       |     3|exact_match|â†‘  | 0.5800|Â±  |0.0313|
| - bbh_cot_fewshot_disambiguation_qa                      |      4|get-answer       |     3|exact_match|â†‘  | 0.3160|Â±  |0.0295|
| - bbh_cot_fewshot_dyck_languages                         |      4|get-answer       |     3|exact_match|â†‘  | 0.0000|Â±  |0.0000|
| - bbh_cot_fewshot_formal_fallacies                       |      4|get-answer       |     3|exact_match|â†‘  | 0.3280|Â±  |0.0298|
| - bbh_cot_fewshot_geometric_shapes                       |      4|get-answer       |     3|exact_match|â†‘  | 0.3960|Â±  |0.0310|
| - bbh_cot_fewshot_hyperbaton                             |      4|get-answer       |     3|exact_match|â†‘  | 0.6160|Â±  |0.0308|
| - bbh_cot_fewshot_logical_deduction_five_objects         |      4|get-answer       |     3|exact_match|â†‘  | 0.2280|Â±  |0.0266|
| - bbh_cot_fewshot_logical_deduction_seven_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.0920|Â±  |0.0183|
| - bbh_cot_fewshot_logical_deduction_three_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.5520|Â±  |0.0315|
| - bbh_cot_fewshot_movie_recommendation                   |      4|get-answer       |     3|exact_match|â†‘  | 0.4520|Â±  |0.0315|
| - bbh_cot_fewshot_multistep_arithmetic_two               |      4|get-answer       |     3|exact_match|â†‘  | 0.8240|Â±  |0.0241|
| - bbh_cot_fewshot_navigate                               |      4|get-answer       |     3|exact_match|â†‘  | 0.5800|Â±  |0.0313|
| - bbh_cot_fewshot_object_counting                        |      4|get-answer       |     3|exact_match|â†‘  | 0.6640|Â±  |0.0299|
| - bbh_cot_fewshot_penguins_in_a_table                    |      4|get-answer       |     3|exact_match|â†‘  | 0.4726|Â±  |0.0415|
| - bbh_cot_fewshot_reasoning_about_colored_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.6000|Â±  |0.0310|
| - bbh_cot_fewshot_ruin_names                             |      4|get-answer       |     3|exact_match|â†‘  | 0.2120|Â±  |0.0259|
| - bbh_cot_fewshot_salient_translation_error_detection    |      4|get-answer       |     3|exact_match|â†‘  | 0.2360|Â±  |0.0269|
| - bbh_cot_fewshot_snarks                                 |      4|get-answer       |     3|exact_match|â†‘  | 0.2921|Â±  |0.0342|
| - bbh_cot_fewshot_sports_understanding                   |      4|get-answer       |     3|exact_match|â†‘  | 0.4880|Â±  |0.0317|
| - bbh_cot_fewshot_temporal_sequences                     |      4|get-answer       |     3|exact_match|â†‘  | 0.1280|Â±  |0.0212|
| - bbh_cot_fewshot_tracking_shuffled_objects_five_objects |      4|get-answer       |     3|exact_match|â†‘  | 0.2840|Â±  |0.0286|
| - bbh_cot_fewshot_tracking_shuffled_objects_seven_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.1240|Â±  |0.0209|
| - bbh_cot_fewshot_tracking_shuffled_objects_three_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.5640|Â±  |0.0314|
| - bbh_cot_fewshot_web_of_lies                            |      4|get-answer       |     3|exact_match|â†‘  | 0.9840|Â±  |0.0080|
| - bbh_cot_fewshot_word_sorting                           |      4|get-answer       |     3|exact_match|â†‘  | 0.0520|Â±  |0.0141|
|boolq                                                     |      2|none             |     0|acc        |â†‘  | 0.6391|Â±  |0.0084|
|drop                                                      |      3|none             |     0|em         |â†‘  | 0.0007|Â±  |0.0003|
|                                                          |       |none             |     0|f1         |â†‘  | 0.0605|Â±  |0.0013|
|gpqa_diamond_cot_n_shot                                   |      2|flexible-extract |     0|exact_match|â†‘  | 0.1768|Â±  |0.0272|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0101|Â±  |0.0071|
|gpqa_diamond_cot_zeroshot                                 |      1|flexible-extract |     0|exact_match|â†‘  | 0.1010|Â±  |0.0215|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_generative_n_shot                            |      2|flexible-extract |     0|exact_match|â†‘  | 0.2222|Â±  |0.0296|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0101|Â±  |0.0071|
|gpqa_diamond_n_shot                                       |      2|none             |     0|acc        |â†‘  | 0.2626|Â±  |0.0314|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2626|Â±  |0.0314|
|gpqa_diamond_zeroshot                                     |      1|none             |     0|acc        |â†‘  | 0.2677|Â±  |0.0315|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2677|Â±  |0.0315|
|gpqa_extended_cot_n_shot                                  |      2|flexible-extract |     0|exact_match|â†‘  | 0.1996|Â±  |0.0171|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0110|Â±  |0.0045|
|gpqa_extended_cot_zeroshot                                |      1|flexible-extract |     0|exact_match|â†‘  | 0.1245|Â±  |0.0141|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0018|Â±  |0.0018|
|gpqa_extended_generative_n_shot                           |      2|flexible-extract |     0|exact_match|â†‘  | 0.2674|Â±  |0.0190|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0311|Â±  |0.0074|
|gpqa_extended_n_shot                                      |      2|none             |     0|acc        |â†‘  | 0.2674|Â±  |0.0190|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2674|Â±  |0.0190|
|gpqa_extended_zeroshot                                    |      1|none             |     0|acc        |â†‘  | 0.3022|Â±  |0.0197|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3022|Â±  |0.0197|
|gpqa_main_cot_n_shot                                      |      2|flexible-extract |     0|exact_match|â†‘  | 0.1897|Â±  |0.0185|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0067|Â±  |0.0039|
|gpqa_main_cot_zeroshot                                    |      1|flexible-extract |     0|exact_match|â†‘  | 0.1027|Â±  |0.0144|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0045|Â±  |0.0032|
|gpqa_main_generative_n_shot                               |      2|flexible-extract |     0|exact_match|â†‘  | 0.2277|Â±  |0.0198|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0134|Â±  |0.0054|
|gpqa_main_n_shot                                          |      2|none             |     0|acc        |â†‘  | 0.2723|Â±  |0.0211|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2723|Â±  |0.0211|
|gpqa_main_zeroshot                                        |      1|none             |     0|acc        |â†‘  | 0.2701|Â±  |0.0210|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.2701|Â±  |0.0210|
|gsm8k                                                     |      3|flexible-extract |     5|exact_match|â†‘  | 0.4109|Â±  |0.0136|
|                                                          |       |strict-match     |     5|exact_match|â†‘  | 0.4124|Â±  |0.0136|
|hellaswag                                                 |      1|none             |     0|acc        |â†‘  | 0.3763|Â±  |0.0048|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.4719|Â±  |0.0050|
|mmlu                                                      |      2|none             |      |acc        |â†‘  | 0.4013|Â±  |0.0040|
| - humanities                                             |      2|none             |      |acc        |â†‘  | 0.3654|Â±  |0.0068|
|  - formal_logic                                          |      1|none             |     0|acc        |â†‘  | 0.4206|Â±  |0.0442|
|  - high_school_european_history                          |      1|none             |     0|acc        |â†‘  | 0.5455|Â±  |0.0389|
|  - high_school_us_history                                |      1|none             |     0|acc        |â†‘  | 0.5000|Â±  |0.0351|
|  - high_school_world_history                             |      1|none             |     0|acc        |â†‘  | 0.5907|Â±  |0.0320|
|  - international_law                                     |      1|none             |     0|acc        |â†‘  | 0.5620|Â±  |0.0453|
|  - jurisprudence                                         |      1|none             |     0|acc        |â†‘  | 0.4167|Â±  |0.0477|
|  - logical_fallacies                                     |      1|none             |     0|acc        |â†‘  | 0.4724|Â±  |0.0392|
|  - moral_disputes                                        |      1|none             |     0|acc        |â†‘  | 0.3208|Â±  |0.0251|
|  - moral_scenarios                                       |      1|none             |     0|acc        |â†‘  | 0.2425|Â±  |0.0143|
|  - philosophy                                            |      1|none             |     0|acc        |â†‘  | 0.4116|Â±  |0.0280|
|  - prehistory                                            |      1|none             |     0|acc        |â†‘  | 0.4321|Â±  |0.0276|
|  - professional_law                                      |      1|none             |     0|acc        |â†‘  | 0.2986|Â±  |0.0117|
|  - world_religions                                       |      1|none             |     0|acc        |â†‘  | 0.5263|Â±  |0.0383|
| - other                                                  |      2|none             |      |acc        |â†‘  | 0.4245|Â±  |0.0087|
|  - business_ethics                                       |      1|none             |     0|acc        |â†‘  | 0.4300|Â±  |0.0498|
|  - clinical_knowledge                                    |      1|none             |     0|acc        |â†‘  | 0.3283|Â±  |0.0289|
|  - college_medicine                                      |      1|none             |     0|acc        |â†‘  | 0.2890|Â±  |0.0346|
|  - global_facts                                          |      1|none             |     0|acc        |â†‘  | 0.2400|Â±  |0.0429|
|  - human_aging                                           |      1|none             |     0|acc        |â†‘  | 0.4664|Â±  |0.0335|
|  - management                                            |      1|none             |     0|acc        |â†‘  | 0.5340|Â±  |0.0494|
|  - marketing                                             |      1|none             |     0|acc        |â†‘  | 0.6325|Â±  |0.0316|
|  - medical_genetics                                      |      1|none             |     0|acc        |â†‘  | 0.3900|Â±  |0.0490|
|  - miscellaneous                                         |      1|none             |     0|acc        |â†‘  | 0.4891|Â±  |0.0179|
|  - nutrition                                             |      1|none             |     0|acc        |â†‘  | 0.4641|Â±  |0.0286|
|  - professional_accounting                               |      1|none             |     0|acc        |â†‘  | 0.2908|Â±  |0.0271|
|  - professional_medicine                                 |      1|none             |     0|acc        |â†‘  | 0.3235|Â±  |0.0284|
|  - virology                                              |      1|none             |     0|acc        |â†‘  | 0.4458|Â±  |0.0387|
| - social sciences                                        |      2|none             |      |acc        |â†‘  | 0.4777|Â±  |0.0089|
|  - econometrics                                          |      1|none             |     0|acc        |â†‘  | 0.2895|Â±  |0.0427|
|  - high_school_geography                                 |      1|none             |     0|acc        |â†‘  | 0.4697|Â±  |0.0356|
|  - high_school_government_and_politics                   |      1|none             |     0|acc        |â†‘  | 0.5233|Â±  |0.0360|
|  - high_school_macroeconomics                            |      1|none             |     0|acc        |â†‘  | 0.4128|Â±  |0.0250|
|  - high_school_microeconomics                            |      1|none             |     0|acc        |â†‘  | 0.4160|Â±  |0.0320|
|  - high_school_psychology                                |      1|none             |     0|acc        |â†‘  | 0.5615|Â±  |0.0213|
|  - human_sexuality                                       |      1|none             |     0|acc        |â†‘  | 0.5038|Â±  |0.0439|
|  - professional_psychology                               |      1|none             |     0|acc        |â†‘  | 0.4085|Â±  |0.0199|
|  - public_relations                                      |      1|none             |     0|acc        |â†‘  | 0.4545|Â±  |0.0477|
|  - security_studies                                      |      1|none             |     0|acc        |â†‘  | 0.5102|Â±  |0.0320|
|  - sociology                                             |      1|none             |     0|acc        |â†‘  | 0.6418|Â±  |0.0339|
|  - us_foreign_policy                                     |      1|none             |     0|acc        |â†‘  | 0.5700|Â±  |0.0498|
| - stem                                                   |      2|none             |      |acc        |â†‘  | 0.3574|Â±  |0.0084|
|  - abstract_algebra                                      |      1|none             |     0|acc        |â†‘  | 0.2900|Â±  |0.0456|
|  - anatomy                                               |      1|none             |     0|acc        |â†‘  | 0.3704|Â±  |0.0417|
|  - astronomy                                             |      1|none             |     0|acc        |â†‘  | 0.4474|Â±  |0.0405|
|  - college_biology                                       |      1|none             |     0|acc        |â†‘  | 0.4583|Â±  |0.0417|
|  - college_chemistry                                     |      1|none             |     0|acc        |â†‘  | 0.3300|Â±  |0.0473|
|  - college_computer_science                              |      1|none             |     0|acc        |â†‘  | 0.2600|Â±  |0.0441|
|  - college_mathematics                                   |      1|none             |     0|acc        |â†‘  | 0.3300|Â±  |0.0473|
|  - college_physics                                       |      1|none             |     0|acc        |â†‘  | 0.2647|Â±  |0.0439|
|  - computer_security                                     |      1|none             |     0|acc        |â†‘  | 0.6000|Â±  |0.0492|
|  - conceptual_physics                                    |      1|none             |     0|acc        |â†‘  | 0.3745|Â±  |0.0316|
|  - electrical_engineering                                |      1|none             |     0|acc        |â†‘  | 0.4345|Â±  |0.0413|
|  - elementary_mathematics                                |      1|none             |     0|acc        |â†‘  | 0.3571|Â±  |0.0247|
|  - high_school_biology                                   |      1|none             |     0|acc        |â†‘  | 0.4419|Â±  |0.0283|
|  - high_school_chemistry                                 |      1|none             |     0|acc        |â†‘  | 0.3300|Â±  |0.0331|
|  - high_school_computer_science                          |      1|none             |     0|acc        |â†‘  | 0.4300|Â±  |0.0498|
|  - high_school_mathematics                               |      1|none             |     0|acc        |â†‘  | 0.2815|Â±  |0.0274|
|  - high_school_physics                                   |      1|none             |     0|acc        |â†‘  | 0.2252|Â±  |0.0341|
|  - high_school_statistics                                |      1|none             |     0|acc        |â†‘  | 0.2361|Â±  |0.0290|
|  - machine_learning                                      |      1|none             |     0|acc        |â†‘  | 0.3661|Â±  |0.0457|
|nq_open                                                   |      4|remove_whitespace|     0|exact_match|â†‘  | 0.0205|Â±  |0.0024|
|openbookqa                                                |      1|none             |     0|acc        |â†‘  | 0.2160|Â±  |0.0184|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3200|Â±  |0.0209|
|piqa                                                      |      1|none             |     0|acc        |â†‘  | 0.6752|Â±  |0.0109|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.6752|Â±  |0.0109|
|qnli                                                      |      1|none             |     0|acc        |â†‘  | 0.4961|Â±  |0.0068|
|sciq                                                      |      1|none             |     0|acc        |â†‘  | 0.8720|Â±  |0.0106|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.8330|Â±  |0.0118|
|triviaqa                                                  |      3|remove_whitespace|     0|exact_match|â†‘  | 0.0193|Â±  |0.0010|
|truthfulqa_gen                                            |      3|none             |     0|bleu_acc   |â†‘  | 0.3305|Â±  |0.0165|
|                                                          |       |none             |     0|bleu_diff  |â†‘  |-4.2316|Â±  |0.5836|
|                                                          |       |none             |     0|bleu_max   |â†‘  |17.1879|Â±  |0.6583|
|                                                          |       |none             |     0|rouge1_acc |â†‘  | 0.2987|Â±  |0.0160|
|                                                          |       |none             |     0|rouge1_diff|â†‘  |-7.1428|Â±  |0.6809|
|                                                          |       |none             |     0|rouge1_max |â†‘  |37.0632|Â±  |0.8923|
|                                                          |       |none             |     0|rouge2_acc |â†‘  | 0.2166|Â±  |0.0144|
|                                                          |       |none             |     0|rouge2_diff|â†‘  |-7.9206|Â±  |0.7858|
|                                                          |       |none             |     0|rouge2_max |â†‘  |21.7683|Â±  |0.8976|
|                                                          |       |none             |     0|rougeL_acc |â†‘  | 0.2938|Â±  |0.0159|
|                                                          |       |none             |     0|rougeL_diff|â†‘  |-7.4867|Â±  |0.6710|
|                                                          |       |none             |     0|rougeL_max |â†‘  |34.4220|Â±  |0.8733|
|truthfulqa_mc1                                            |      2|none             |     0|acc        |â†‘  | 0.2705|Â±  |0.0156|
|truthfulqa_mc2                                            |      3|none             |     0|acc        |â†‘  | 0.4277|Â±  |0.0145|
|winogrande                                                |      1|none             |     0|acc        |â†‘  | 0.5517|Â±  |0.0140|

|      Groups      |Version|  Filter  |n-shot|  Metric   |   |Value |   |Stderr|
|------------------|------:|----------|------|-----------|---|-----:|---|-----:|
|bbh               |      3|get-answer|      |exact_match|â†‘  |0.4148|Â±  |0.0053|
|mmlu              |      2|none      |      |acc        |â†‘  |0.4013|Â±  |0.0040|
| - humanities     |      2|none      |      |acc        |â†‘  |0.3654|Â±  |0.0068|
| - other          |      2|none      |      |acc        |â†‘  |0.4245|Â±  |0.0087|
| - social sciences|      2|none      |      |acc        |â†‘  |0.4777|Â±  |0.0089|
| - stem           |      2|none      |      |acc        |â†‘  |0.3574|Â±  |0.0084|

Qwen_Qwen3-0.6B: 3h 45m 57s
âœ… Benchmark completed for Qwen_Qwen3-0.6B

ðŸ”¥ Starting benchmark for Qwen_Qwen3-4B
hf (pretrained=/mnt/data8tb/Documents/llm/llm_models/Qwen_Qwen3-4B,trust_remote_code=True,device_map=auto), gen_kwargs: (None), limit: None, num_fewshot: None, batch_size: 6
|                          Tasks                           |Version|     Filter      |n-shot|  Metric   |   | Value |   |Stderr|
|----------------------------------------------------------|------:|-----------------|-----:|-----------|---|------:|---|-----:|
|anli_r1                                                   |      1|none             |     0|acc        |â†‘  | 0.5500|Â±  |0.0157|
|anli_r2                                                   |      1|none             |     0|acc        |â†‘  | 0.4610|Â±  |0.0158|
|anli_r3                                                   |      1|none             |     0|acc        |â†‘  | 0.5133|Â±  |0.0144|
|arc_challenge                                             |      1|none             |     0|acc        |â†‘  | 0.5043|Â±  |0.0146|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.5392|Â±  |0.0146|
|bbh                                                       |      3|get-answer       |      |exact_match|â†‘  | 0.7523|Â±  |0.0047|
| - bbh_cot_fewshot_boolean_expressions                    |      4|get-answer       |     3|exact_match|â†‘  | 0.9640|Â±  |0.0118|
| - bbh_cot_fewshot_causal_judgement                       |      4|get-answer       |     3|exact_match|â†‘  | 0.3636|Â±  |0.0353|
| - bbh_cot_fewshot_date_understanding                     |      4|get-answer       |     3|exact_match|â†‘  | 0.7800|Â±  |0.0263|
| - bbh_cot_fewshot_disambiguation_qa                      |      4|get-answer       |     3|exact_match|â†‘  | 0.6120|Â±  |0.0309|
| - bbh_cot_fewshot_dyck_languages                         |      4|get-answer       |     3|exact_match|â†‘  | 0.3800|Â±  |0.0308|
| - bbh_cot_fewshot_formal_fallacies                       |      4|get-answer       |     3|exact_match|â†‘  | 0.6360|Â±  |0.0305|
| - bbh_cot_fewshot_geometric_shapes                       |      4|get-answer       |     3|exact_match|â†‘  | 0.5040|Â±  |0.0317|
| - bbh_cot_fewshot_hyperbaton                             |      4|get-answer       |     3|exact_match|â†‘  | 0.9560|Â±  |0.0130|
| - bbh_cot_fewshot_logical_deduction_five_objects         |      4|get-answer       |     3|exact_match|â†‘  | 0.5800|Â±  |0.0313|
| - bbh_cot_fewshot_logical_deduction_seven_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.2920|Â±  |0.0288|
| - bbh_cot_fewshot_logical_deduction_three_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.9080|Â±  |0.0183|
| - bbh_cot_fewshot_movie_recommendation                   |      4|get-answer       |     3|exact_match|â†‘  | 0.7040|Â±  |0.0289|
| - bbh_cot_fewshot_multistep_arithmetic_two               |      4|get-answer       |     3|exact_match|â†‘  | 0.9920|Â±  |0.0056|
| - bbh_cot_fewshot_navigate                               |      4|get-answer       |     3|exact_match|â†‘  | 0.9200|Â±  |0.0172|
| - bbh_cot_fewshot_object_counting                        |      4|get-answer       |     3|exact_match|â†‘  | 0.8480|Â±  |0.0228|
| - bbh_cot_fewshot_penguins_in_a_table                    |      4|get-answer       |     3|exact_match|â†‘  | 0.7740|Â±  |0.0347|
| - bbh_cot_fewshot_reasoning_about_colored_objects        |      4|get-answer       |     3|exact_match|â†‘  | 0.8600|Â±  |0.0220|
| - bbh_cot_fewshot_ruin_names                             |      4|get-answer       |     3|exact_match|â†‘  | 0.7600|Â±  |0.0271|
| - bbh_cot_fewshot_salient_translation_error_detection    |      4|get-answer       |     3|exact_match|â†‘  | 0.5880|Â±  |0.0312|
| - bbh_cot_fewshot_snarks                                 |      4|get-answer       |     3|exact_match|â†‘  | 0.6966|Â±  |0.0346|
| - bbh_cot_fewshot_sports_understanding                   |      4|get-answer       |     3|exact_match|â†‘  | 0.8280|Â±  |0.0239|
| - bbh_cot_fewshot_temporal_sequences                     |      4|get-answer       |     3|exact_match|â†‘  | 0.8840|Â±  |0.0203|
| - bbh_cot_fewshot_tracking_shuffled_objects_five_objects |      4|get-answer       |     3|exact_match|â†‘  | 0.9800|Â±  |0.0089|
| - bbh_cot_fewshot_tracking_shuffled_objects_seven_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.9080|Â±  |0.0183|
| - bbh_cot_fewshot_tracking_shuffled_objects_three_objects|      4|get-answer       |     3|exact_match|â†‘  | 0.9960|Â±  |0.0040|
| - bbh_cot_fewshot_web_of_lies                            |      4|get-answer       |     3|exact_match|â†‘  | 1.0000|Â±  |0.0000|
| - bbh_cot_fewshot_word_sorting                           |      4|get-answer       |     3|exact_match|â†‘  | 0.4920|Â±  |0.0317|
|boolq                                                     |      2|none             |     0|acc        |â†‘  | 0.8505|Â±  |0.0062|
|drop                                                      |      3|none             |     0|em         |â†‘  | 0.0060|Â±  |0.0008|
|                                                          |       |none             |     0|f1         |â†‘  | 0.0977|Â±  |0.0020|
|gpqa_diamond_cot_n_shot                                   |      2|flexible-extract |     0|exact_match|â†‘  | 0.1111|Â±  |0.0224|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_cot_zeroshot                                 |      1|flexible-extract |     0|exact_match|â†‘  | 0.0859|Â±  |0.0200|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_generative_n_shot                            |      2|flexible-extract |     0|exact_match|â†‘  | 0.1818|Â±  |0.0275|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_diamond_n_shot                                       |      2|none             |     0|acc        |â†‘  | 0.3939|Â±  |0.0348|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3939|Â±  |0.0348|
|gpqa_diamond_zeroshot                                     |      1|none             |     0|acc        |â†‘  | 0.3636|Â±  |0.0343|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3636|Â±  |0.0343|
|gpqa_extended_cot_n_shot                                  |      2|flexible-extract |     0|exact_match|â†‘  | 0.1136|Â±  |0.0136|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_cot_zeroshot                                |      1|flexible-extract |     0|exact_match|â†‘  | 0.0879|Â±  |0.0121|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0055|Â±  |0.0032|
|gpqa_extended_generative_n_shot                           |      2|flexible-extract |     0|exact_match|â†‘  | 0.2436|Â±  |0.0184|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_extended_n_shot                                      |      2|none             |     0|acc        |â†‘  | 0.3407|Â±  |0.0203|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3407|Â±  |0.0203|
|gpqa_extended_zeroshot                                    |      1|none             |     0|acc        |â†‘  | 0.3388|Â±  |0.0203|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3388|Â±  |0.0203|
|gpqa_main_cot_n_shot                                      |      2|flexible-extract |     0|exact_match|â†‘  | 0.0893|Â±  |0.0135|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0045|Â±  |0.0032|
|gpqa_main_cot_zeroshot                                    |      1|flexible-extract |     0|exact_match|â†‘  | 0.0647|Â±  |0.0116|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_generative_n_shot                               |      2|flexible-extract |     0|exact_match|â†‘  | 0.2455|Â±  |0.0204|
|                                                          |       |strict-match     |     0|exact_match|â†‘  | 0.0000|Â±  |0.0000|
|gpqa_main_n_shot                                          |      2|none             |     0|acc        |â†‘  | 0.3438|Â±  |0.0225|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3438|Â±  |0.0225|
|gpqa_main_zeroshot                                        |      1|none             |     0|acc        |â†‘  | 0.3259|Â±  |0.0222|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.3259|Â±  |0.0222|
|gsm8k                                                     |      3|flexible-extract |     5|exact_match|â†‘  | 0.8484|Â±  |0.0099|
|                                                          |       |strict-match     |     5|exact_match|â†‘  | 0.8567|Â±  |0.0097|
|hellaswag                                                 |      1|none             |     0|acc        |â†‘  | 0.5223|Â±  |0.0050|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.6833|Â±  |0.0046|
|mmlu                                                      |      2|none             |      |acc        |â†‘  | 0.6836|Â±  |0.0037|
| - humanities                                             |      2|none             |      |acc        |â†‘  | 0.5957|Â±  |0.0067|
|  - formal_logic                                          |      1|none             |     0|acc        |â†‘  | 0.6429|Â±  |0.0429|
|  - high_school_european_history                          |      1|none             |     0|acc        |â†‘  | 0.7939|Â±  |0.0316|
|  - high_school_us_history                                |      1|none             |     0|acc        |â†‘  | 0.8431|Â±  |0.0255|
|  - high_school_world_history                             |      1|none             |     0|acc        |â†‘  | 0.8397|Â±  |0.0239|
|  - international_law                                     |      1|none             |     0|acc        |â†‘  | 0.7355|Â±  |0.0403|
|  - jurisprudence                                         |      1|none             |     0|acc        |â†‘  | 0.7407|Â±  |0.0424|
|  - logical_fallacies                                     |      1|none             |     0|acc        |â†‘  | 0.8098|Â±  |0.0308|
|  - moral_disputes                                        |      1|none             |     0|acc        |â†‘  | 0.6965|Â±  |0.0248|
|  - moral_scenarios                                       |      1|none             |     0|acc        |â†‘  | 0.3799|Â±  |0.0162|
|  - philosophy                                            |      1|none             |     0|acc        |â†‘  | 0.7235|Â±  |0.0254|
|  - prehistory                                            |      1|none             |     0|acc        |â†‘  | 0.7438|Â±  |0.0243|
|  - professional_law                                      |      1|none             |     0|acc        |â†‘  | 0.4811|Â±  |0.0128|
|  - world_religions                                       |      1|none             |     0|acc        |â†‘  | 0.7836|Â±  |0.0316|
| - other                                                  |      2|none             |      |acc        |â†‘  | 0.7126|Â±  |0.0079|
|  - business_ethics                                       |      1|none             |     0|acc        |â†‘  | 0.7100|Â±  |0.0456|
|  - clinical_knowledge                                    |      1|none             |     0|acc        |â†‘  | 0.7396|Â±  |0.0270|
|  - college_medicine                                      |      1|none             |     0|acc        |â†‘  | 0.7052|Â±  |0.0348|
|  - global_facts                                          |      1|none             |     0|acc        |â†‘  | 0.3400|Â±  |0.0476|
|  - human_aging                                           |      1|none             |     0|acc        |â†‘  | 0.6771|Â±  |0.0314|
|  - management                                            |      1|none             |     0|acc        |â†‘  | 0.8155|Â±  |0.0384|
|  - marketing                                             |      1|none             |     0|acc        |â†‘  | 0.8675|Â±  |0.0222|
|  - medical_genetics                                      |      1|none             |     0|acc        |â†‘  | 0.7600|Â±  |0.0429|
|  - miscellaneous                                         |      1|none             |     0|acc        |â†‘  | 0.7969|Â±  |0.0144|
|  - nutrition                                             |      1|none             |     0|acc        |â†‘  | 0.7255|Â±  |0.0256|
|  - professional_accounting                               |      1|none             |     0|acc        |â†‘  | 0.5319|Â±  |0.0298|
|  - professional_medicine                                 |      1|none             |     0|acc        |â†‘  | 0.7243|Â±  |0.0271|
|  - virology                                              |      1|none             |     0|acc        |â†‘  | 0.5060|Â±  |0.0389|
| - social sciences                                        |      2|none             |      |acc        |â†‘  | 0.7803|Â±  |0.0074|
|  - econometrics                                          |      1|none             |     0|acc        |â†‘  | 0.6316|Â±  |0.0454|
|  - high_school_geography                                 |      1|none             |     0|acc        |â†‘  | 0.8283|Â±  |0.0269|
|  - high_school_government_and_politics                   |      1|none             |     0|acc        |â†‘  | 0.8756|Â±  |0.0238|
|  - high_school_macroeconomics                            |      1|none             |     0|acc        |â†‘  | 0.7462|Â±  |0.0221|
|  - high_school_microeconomics                            |      1|none             |     0|acc        |â†‘  | 0.8151|Â±  |0.0252|
|  - high_school_psychology                                |      1|none             |     0|acc        |â†‘  | 0.8716|Â±  |0.0143|
|  - human_sexuality                                       |      1|none             |     0|acc        |â†‘  | 0.7634|Â±  |0.0373|
|  - professional_psychology                               |      1|none             |     0|acc        |â†‘  | 0.7206|Â±  |0.0182|
|  - public_relations                                      |      1|none             |     0|acc        |â†‘  | 0.6727|Â±  |0.0449|
|  - security_studies                                      |      1|none             |     0|acc        |â†‘  | 0.7061|Â±  |0.0292|
|  - sociology                                             |      1|none             |     0|acc        |â†‘  | 0.8308|Â±  |0.0265|
|  - us_foreign_policy                                     |      1|none             |     0|acc        |â†‘  | 0.8100|Â±  |0.0394|
| - stem                                                   |      2|none             |      |acc        |â†‘  | 0.6917|Â±  |0.0080|
|  - abstract_algebra                                      |      1|none             |     0|acc        |â†‘  | 0.6000|Â±  |0.0492|
|  - anatomy                                               |      1|none             |     0|acc        |â†‘  | 0.6148|Â±  |0.0420|
|  - astronomy                                             |      1|none             |     0|acc        |â†‘  | 0.8026|Â±  |0.0324|
|  - college_biology                                       |      1|none             |     0|acc        |â†‘  | 0.8194|Â±  |0.0322|
|  - college_chemistry                                     |      1|none             |     0|acc        |â†‘  | 0.5400|Â±  |0.0501|
|  - college_computer_science                              |      1|none             |     0|acc        |â†‘  | 0.6700|Â±  |0.0473|
|  - college_mathematics                                   |      1|none             |     0|acc        |â†‘  | 0.5400|Â±  |0.0501|
|  - college_physics                                       |      1|none             |     0|acc        |â†‘  | 0.5882|Â±  |0.0490|
|  - computer_security                                     |      1|none             |     0|acc        |â†‘  | 0.7900|Â±  |0.0409|
|  - conceptual_physics                                    |      1|none             |     0|acc        |â†‘  | 0.7830|Â±  |0.0269|
|  - electrical_engineering                                |      1|none             |     0|acc        |â†‘  | 0.7310|Â±  |0.0370|
|  - elementary_mathematics                                |      1|none             |     0|acc        |â†‘  | 0.6799|Â±  |0.0240|
|  - high_school_biology                                   |      1|none             |     0|acc        |â†‘  | 0.8645|Â±  |0.0195|
|  - high_school_chemistry                                 |      1|none             |     0|acc        |â†‘  | 0.7094|Â±  |0.0319|
|  - high_school_computer_science                          |      1|none             |     0|acc        |â†‘  | 0.8500|Â±  |0.0359|
|  - high_school_mathematics                               |      1|none             |     0|acc        |â†‘  | 0.4815|Â±  |0.0305|
|  - high_school_physics                                   |      1|none             |     0|acc        |â†‘  | 0.6093|Â±  |0.0398|
|  - high_school_statistics                                |      1|none             |     0|acc        |â†‘  | 0.6944|Â±  |0.0314|
|  - machine_learning                                      |      1|none             |     0|acc        |â†‘  | 0.6071|Â±  |0.0464|
|nq_open                                                   |      4|remove_whitespace|     0|exact_match|â†‘  | 0.0147|Â±  |0.0020|
|openbookqa                                                |      1|none             |     0|acc        |â†‘  | 0.2960|Â±  |0.0204|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.4020|Â±  |0.0219|
|piqa                                                      |      1|none             |     0|acc        |â†‘  | 0.7514|Â±  |0.0101|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.7514|Â±  |0.0101|
|qnli                                                      |      1|none             |     0|acc        |â†‘  | 0.8087|Â±  |0.0053|
|sciq                                                      |      1|none             |     0|acc        |â†‘  | 0.9550|Â±  |0.0066|
|                                                          |       |none             |     0|acc_norm   |â†‘  | 0.9320|Â±  |0.0080|
|triviaqa                                                  |      3|remove_whitespace|     0|exact_match|â†‘  | 0.2250|Â±  |0.0031|
|truthfulqa_gen                                            |      3|none             |     0|bleu_acc   |â†‘  | 0.5838|Â±  |0.0173|
|                                                          |       |none             |     0|bleu_diff  |â†‘  |12.2904|Â±  |0.9730|
|                                                          |       |none             |     0|bleu_max   |â†‘  |29.1140|Â±  |0.8421|
|                                                          |       |none             |     0|rouge1_acc |â†‘  | 0.6095|Â±  |0.0171|
|                                                          |       |none             |     0|rouge1_diff|â†‘  |17.9082|Â±  |1.3731|
|                                                          |       |none             |     0|rouge1_max |â†‘  |54.7069|Â±  |0.9372|
|                                                          |       |none             |     0|rouge2_acc |â†‘  | 0.5520|Â±  |0.0174|
|                                                          |       |none             |     0|rouge2_diff|â†‘  |18.5593|Â±  |1.4928|
|                                                          |       |none             |     0|rouge2_max |â†‘  |42.6485|Â±  |1.1203|
|                                                          |       |none             |     0|rougeL_acc |â†‘  | 0.5961|Â±  |0.0172|
|                                                          |       |none             |     0|rougeL_diff|â†‘  |17.8681|Â±  |1.3823|
|                                                          |       |none             |     0|rougeL_max |â†‘  |52.3619|Â±  |0.9738|
|truthfulqa_mc1                                            |      2|none             |     0|acc        |â†‘  | 0.3672|Â±  |0.0169|
|truthfulqa_mc2                                            |      3|none             |     0|acc        |â†‘  | 0.5476|Â±  |0.0158|
|winogrande                                                |      1|none             |     0|acc        |â†‘  | 0.6582|Â±  |0.0133|

|      Groups      |Version|  Filter  |n-shot|  Metric   |   |Value |   |Stderr|
|------------------|------:|----------|------|-----------|---|-----:|---|-----:|
|bbh               |      3|get-answer|      |exact_match|â†‘  |0.7523|Â±  |0.0047|
|mmlu              |      2|none      |      |acc        |â†‘  |0.6836|Â±  |0.0037|
| - humanities     |      2|none      |      |acc        |â†‘  |0.5957|Â±  |0.0067|
| - other          |      2|none      |      |acc        |â†‘  |0.7126|Â±  |0.0079|
| - social sciences|      2|none      |      |acc        |â†‘  |0.7803|Â±  |0.0074|
| - stem           |      2|none      |      |acc        |â†‘  |0.6917|Â±  |0.0080|

Qwen_Qwen3-4B: 5h 51m 27s
âœ… Benchmark completed for Qwen_Qwen3-4B

ðŸ”¥ Starting benchmark for openai_gpt-oss-20b
openai_gpt-oss-20b: 0h 0m 6s
âœ… Benchmark completed for openai_gpt-oss-20b

ðŸ”¥ Starting benchmark for openai_gpt-oss-20b
openai_gpt-oss-20b: 0h 0m 4s
âœ… Benchmark completed for openai_gpt-oss-20b

ðŸ”¥ Starting benchmark for openai_gpt-oss-20b
openai_gpt-oss-20b: 0h 0m 4s
âœ… Benchmark completed for openai_gpt-oss-20b

ðŸ”¥ Starting benchmark for openai_gpt-oss-20b
openai_gpt-oss-20b: 0h 0m 4s
âœ… Benchmark completed for openai_gpt-oss-20b

ðŸ”¥ Starting benchmark for openai_gpt-oss-20b
openai_gpt-oss-20b: 0h 0m 0s
âœ… Benchmark completed for openai_gpt-oss-20b

ðŸ”¥ Starting benchmark for openai_gpt-oss-20b
openai_gpt-oss-20b: 0h 0m 4s
âœ… Benchmark completed for openai_gpt-oss-20b

ðŸ”¥ Starting benchmark for openai_gpt-oss-20b
openai_gpt-oss-20b: 0h 0m 17s
âœ… Benchmark completed for openai_gpt-oss-20b

